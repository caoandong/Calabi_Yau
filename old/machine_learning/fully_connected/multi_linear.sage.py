
# This file was *autogenerated* from the file multi_linear.sage
from sage.all_cmdline import *   # import sage library

_sage_const_2 = Integer(2); _sage_const_1 = Integer(1); _sage_const_0 = Integer(0); _sage_const_2000 = Integer(2000); _sage_const_1en3 = RealNumber('1e-3'); _sage_const_50000 = Integer(50000); _sage_const_10 = Integer(10); _sage_const_0p5 = RealNumber('0.5'); _sage_const_1000 = Integer(1000); _sage_const_0p4 = RealNumber('0.4')
import tensorflow as tf
import numpy as np
import math
import sys
import os

# Input: parametrized heights
# Output: volume

def placeholder_inputs(batch_size, num_param):
    param_pl = tf.placeholder(name="input_param", dtype=tf.float32, shape=(batch_size, num_param))
    vol_pl = tf.placeholder(name="input_vol", dtype=tf.float32, shape=(batch_size))
    
    return param_pl, vol_pl

def load_input(train_path):
    train = open(train_path, 'r')
    train_x = []
    train_y = []
    for line in train:
        data = eval(line)
        if data[_sage_const_1 ] > _sage_const_0 :
            train_x.append(data[_sage_const_0 ])
            inv_vol = _sage_const_1 /data[_sage_const_1 ]
            train_y.append(inv_vol)
    print ('Number of data: ', len(train_x))
    
    return train_x, train_y

def fl_linear(param, batch_size, num_param):
    with tf.variable_scope("fl_linear"):
        input_param = tf.placeholder(name="input_param", dtype=tf.float32, shape=(batch_size, num_param))
        input_vol = tf.placeholder(name="input_vol", dtype=tf.float32, shape=(batch_size))
        # Linear activation
        net = tf.fully_connected(_sage_const_1 , activation_fn=None, scope='fc1')
    
    return net

def mat_mul(param, batch_size, num_param):
    with tf.variable_scope("mat_mul"):
        input_parameter = tf.placeholder(name="input_parameter", dtype=tf.float32, shape=(batch_size, num_param))
        input_volume = tf.placeholder(name="input_volume", dtype=tf.float32, shape=(batch_size))
        W = tf.Variable(tf.random_normal([num_param, _sage_const_1 ], dtype=tf.float32), name="W")
        b = tf.Variable(tf.zeros([batch_size, _sage_const_1 ], dtype=tf.float32), name="b")
        y = tf.add(tf.matmul(input_parameter, W), b)
        cost_op = tf.reduce_mean(tf.pow(y-input_volume, _sage_const_2 ))

    return y, cost_op, input_parameter, input_volume

def train(data, vol):
    
    input_data = data[_sage_const_0 :_sage_const_1000 ]
    input_vol = vol[_sage_const_0 :_sage_const_1000 ]
    
    test_data = data[_sage_const_1000 :_sage_const_2000 ]
    test_vol = vol[_sage_const_1000 :_sage_const_2000 ]
    
    num_data = len(input_data)
    num_param = len(input_data[_sage_const_0 ])
    num_test = len(test_data)
    print('number of data: ', num_data)
    print('number of parameters: ', num_param)
    print('number of test: ', num_test)
    
    num_batches = int(num_data/batch_size)
    test_batches = int(num_test/batch_size)
    input_param = np.array(input_data)
    input_vol = np.array(input_vol)
    test_param = np.array(test_data)
    test_vol = np.array(test_vol)
    
    #y = fl_linear(input_param)
    y, cost_op, input_parameter, input_volume = mat_mul(input_param, batch_size, num_param)
    
    learning_rate = tf.Variable(_sage_const_0p5 , trainable=False)
    optimizer = tf.train.AdamOptimizer(learning_rate)
    train_op = optimizer.minimize(cost_op)
    
    sess = tf.Session() # Create TensorFlow session
    print ("Beginning Training")
    with sess.as_default():
        init = tf.global_variables_initializer()
        sess.run(init)
        sess.run(tf.assign(learning_rate, alpha))
        for epoch in range(_sage_const_1 , max_epochs):
            print('epoch: ', epoch)
            # Train one epoch
            train_one_epoch(sess, cost_op, train_op, num_batches, input_parameter, input_param, input_volume, input_vol)
            eval_one_epoch(sess, cost_op, test_batches, input_parameter, test_param, input_volume, test_vol)
            
        print('Done.')
        W = tf.get_variable("W")
        print("Weight: ", W.eval())
            

def train_one_epoch(sess, cost_op, train_op, num_batches, input_parameter, input_param, input_volume, input_vol):
     for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = (batch_idx+_sage_const_1 ) * batch_size
        cost,_ = sess.run([cost_op, train_op], feed_dict={input_parameter: input_param[start_idx:end_idx], input_volume: input_vol[start_idx:end_idx]})
        print("loss: ", cost)
        
def eval_one_epoch(sess, cost_op, test_batches, input_parameter, test_param, input_volume, test_vol):
    for batch_idx in range(test_batches):
        start_idx = batch_idx * batch_size
        end_idx = (batch_idx+_sage_const_1 ) * batch_size
        cost = sess.run(cost_op, feed_dict={input_parameter: test_param[start_idx:end_idx], input_volume: test_vol[start_idx:end_idx]})
        print("test loss: ", cost)

# Parameters:
last_cost = _sage_const_0 
alpha = _sage_const_0p4 
batch_size = _sage_const_10 
max_epochs = _sage_const_50000 
tolerance = _sage_const_1en3 

    
train_path = '/home/carnd/CYML/output/train/cylinder/tri_1_to_50_2.txt'
train_data, train_vol = load_input(train_path)
print ('sample data: ', train_data[_sage_const_0 ])
train(train_data, train_vol)

