{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input: parametrized heights\n",
    "# Output: volume\n",
    "\n",
    "def placeholder_inputs(batch_size, num_param):\n",
    "    param_pl = tf.placeholder(name=\"input_param\", dtype=tf.float32, shape=(batch_size, num_param))\n",
    "    vol_pl = tf.placeholder(name=\"input_vol\", dtype=tf.float32, shape=(batch_size))\n",
    "    \n",
    "    return param_pl, vol_pl\n",
    "\n",
    "def load_input(train_path):\n",
    "    train = open(train_path, 'r')\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    for line in train:\n",
    "        data = eval(line)\n",
    "        if data[1] > 0:\n",
    "            train_x.append(data[0])\n",
    "            inv_vol = 1/data[1]\n",
    "            train_y.append(inv_vol)\n",
    "    print ('Number of data: ', len(train_x))\n",
    "    \n",
    "    return train_x, train_y\n",
    "\n",
    "def fl_linear(param, batch_size, num_param):\n",
    "    with tf.variable_scope(\"fl_linear\", reuse=True):\n",
    "        input_param = tf.placeholder(name=\"input_param\", dtype=tf.float32, shape=(batch_size, num_param))\n",
    "        input_vol = tf.placeholder(name=\"input_vol\", dtype=tf.float32, shape=(batch_size))\n",
    "        # Linear activation\n",
    "        net = tf.fully_connected(1, activation_fn=None, scope='fc1')\n",
    "    \n",
    "    return net\n",
    "\n",
    "def mat_mul(param, batch_size, num_param):\n",
    "    with tf.variable_scope(\"mat_mul\", reuse=True):\n",
    "        input_parameter = tf.placeholder(name=\"input_parameter\", dtype=tf.float32, shape=(batch_size, num_param))\n",
    "        input_volume = tf.placeholder(name=\"input_volume\", dtype=tf.float32, shape=(batch_size))\n",
    "        W = tf.Variable(tf.random_normal([num_param, 1], dtype=tf.float32), name=\"W\")\n",
    "        b = tf.Variable(tf.zeros([batch_size, 1], dtype=tf.float32), name=\"b\")\n",
    "        y = tf.add(tf.matmul(input_parameter, W), b)\n",
    "        cost_op = tf.reduce_mean(tf.pow(y-input_volume, 2))\n",
    "\n",
    "    return y, cost_op, input_parameter, input_volume\n",
    "\n",
    "def train(data, vol):\n",
    "    \n",
    "    input_data = data[0:1000]\n",
    "    input_vol = vol[0:1000]\n",
    "    \n",
    "    test_data = data[1000:2000]\n",
    "    test_vol = vol[1000:2000]\n",
    "    \n",
    "    num_data = len(input_data)\n",
    "    num_param = len(input_data[0])\n",
    "    num_test = len(test_data)\n",
    "    print('number of data: ', num_data)\n",
    "    print('number of parameters: ', num_param)\n",
    "    print('number of test: ', num_test)\n",
    "    \n",
    "    num_batches = int(num_data/batch_size)\n",
    "    test_batches = int(num_test/batch_size)\n",
    "    input_param = np.array(input_data)\n",
    "    input_vol = np.array(input_vol)\n",
    "    test_param = np.array(test_data)\n",
    "    test_vol = np.array(test_vol)\n",
    "    \n",
    "    #y = fl_linear(input_param)\n",
    "    y, cost_op, input_parameter, input_volume = mat_mul(input_param, batch_size, num_param)\n",
    "    \n",
    "    learning_rate = tf.Variable(0.5, trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(cost_op)\n",
    "    \n",
    "    sess = tf.Session() # Create TensorFlow session\n",
    "    print (\"Beginning Training\")\n",
    "    with sess.as_default():\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        sess.run(tf.assign(learning_rate, alpha))\n",
    "        for epoch in range(1, max_epochs):\n",
    "            print('epoch: ', epoch)\n",
    "            # Train one epoch\n",
    "            train_one_epoch(sess, cost_op, train_op, num_batches, input_parameter, input_param, input_volume, input_vol)\n",
    "            eval_one_epoch(sess, cost_op, test_batches, input_parameter, test_param, input_volume, test_vol)\n",
    "            \n",
    "        print('Done.')\n",
    "        W = tf.get_variable(\"W\")\n",
    "        print(\"Weight: \", W.eval())\n",
    "            \n",
    "\n",
    "def train_one_epoch(sess, cost_op, train_op, num_batches, input_parameter, input_param, input_volume, input_vol):\n",
    "     for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx+1) * batch_size\n",
    "        cost,_ = sess.run([cost_op, train_op], feed_dict={input_parameter: input_param[start_idx:end_idx], input_volume: input_vol[start_idx:end_idx]})\n",
    "        print(\"loss: \", cost)\n",
    "        \n",
    "def eval_one_epoch(sess, cost_op, test_batches, input_parameter, test_param, input_volume, test_vol):\n",
    "    for batch_idx in range(test_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx+1) * batch_size\n",
    "        cost = sess.run(cost_op, feed_dict={input_parameter: test_param[start_idx:end_idx], input_volume: test_vol[start_idx:end_idx]})\n",
    "        print(\"test loss: \", cost)\n",
    "\n",
    "# Parameters:\n",
    "# last_cost = 0\n",
    "# alpha = 0.4\n",
    "# batch_size = 10\n",
    "# max_epochs = 100\n",
    "# tolerance = 1e-3\n",
    "\n",
    "    \n",
    "# train_path = '/home/carnd/CYML/output/train/cylinder/tri_1_to_50_2.txt'\n",
    "# train_data, train_vol = load_input(train_path)\n",
    "# print ('sample data: ', train_data[0])\n",
    "# train(train_data, train_vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice_5:0\", shape=(3,), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 1 but is rank 2 for 'concat_5' (op: 'ConcatV2') with input shapes: [3], [1,3], [].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/home/carnd/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1566\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1567\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1568\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shape must be rank 1 but is rank 2 for 'concat_5' (op: 'ConcatV2') with input shapes: [3], [1,3], [].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-bf4441b23628>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_param\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1187\u001b[0m               tensor_shape.scalar())\n\u001b[1;32m   1188\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mconcat_v2\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0m_attr_N\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m--> 953\u001b[0;31m         \"ConcatV2\", values=values, axis=axis, name=name)\n\u001b[0m\u001b[1;32m    954\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3390\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3391\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3392\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3394\u001b[0m       \u001b[0;31m# Note: shapes are lazily computed with the C API enabled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1732\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1733\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1734\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1735\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1736\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1568\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1570\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape must be rank 1 but is rank 2 for 'concat_5' (op: 'ConcatV2') with input shapes: [3], [1,3], []."
     ]
    }
   ],
   "source": [
    "data = np.arange(6).reshape(2,3)\n",
    "param = np.array([1,1,1])\n",
    "input_data = tf.placeholder(name=\"input_data\", dtype=tf.float32, shape=(2, 3))\n",
    "input_param = tf.placeholder(name=\"input_param\", dtype=tf.float32, shape=(1,3))\n",
    "for i in range(2):\n",
    "    row = input_data[i,]\n",
    "    print(row)\n",
    "    tmp = tf.concat([row, input_param], 1)\n",
    "    try:\n",
    "        out = tf.stack([input_param, tmp])\n",
    "    except:\n",
    "        out = tmp\n",
    "        \n",
    "sess = tf.Session() # Create TensorFlow session\n",
    "saver = tf.train.Saver()\n",
    "with sess.as_default():\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    print(\"slice: \", sess.run(row, feed_dict = {input_data:data, input_param:param}))\n",
    "    print(\"stack result: \", sess.run(out, feed_dict = {input_data:data, input_param:param}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_12:0\", shape=(2, 3), dtype=int32)\n",
      "Tensor(\"Reshape_14:0\", shape=(1, 3), dtype=int32)\n",
      "[[1, 2, 3]]\n",
      "Tensor(\"concat_12:0\", shape=(1, 6), dtype=int32)\n",
      "[[1, 2, 3, 8, 9, 10]]\n",
      "Tensor(\"MatMul_4:0\", shape=(3, 3), dtype=int32)\n",
      "[[1 2 3]\n",
      " [2 4 6]\n",
      " [3 6 9]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "x = tf.constant([[1,2,3],[8,9,10]])\n",
    "print(x)\n",
    "#y = tf.transpose(x[0,:])\n",
    "x_0 = x[0,:]\n",
    "x_0 = tf.reshape(x_0,[1,-1])\n",
    "x_1 = x[1,:]\n",
    "x_1 = tf.reshape(x_1,[1,-1])\n",
    "print(x_0)\n",
    "print(x_0.eval(session=sess).tolist())\n",
    "z = tf.concat([x_0,x_1],1)\n",
    "print(z)\n",
    "print(z.eval(session=sess).tolist())\n",
    "mul = tf.matmul(tf.transpose(x_0), x_0)\n",
    "print(mul)\n",
    "print(mul.eval(session=sess))\n",
    "for i in range(3):\n",
    "    concat = tf.reshape(mul[i,i:],[1,-1])\n",
    "    mul_tmp = tf.reshape(mul[i,i:],[1,-1])\n",
    "    concat = tf.concat([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_3:  [[1 2 3]\n",
      " [2 4 6]\n",
      " [3 6 9]]\n",
      "param_new reshaped 2:  Tensor(\"Reshape_1826:0\", shape=(6,), dtype=int32)\n",
      "param list:  Tensor(\"Reshape_1809:0\", shape=(1, 3), dtype=int32)\n",
      "x_3:  [[ 1  2  3]\n",
      " [ 2  4  6]\n",
      " [ 3  6  9]\n",
      " [ 4  8 12]\n",
      " [ 6 12 18]\n",
      " [ 9 18 27]]\n",
      "added row:  Tensor(\"Reshape_1842:0\", shape=(3,), dtype=int32)\n",
      "param_new:  Tensor(\"Reshape_1841:0\", shape=(1, 6), dtype=int32)\n",
      "param_new:  [[ 1  2  3  4  6  9  4  8 12]]\n",
      "param_new reshaped 2:  Tensor(\"Reshape_1844:0\", shape=(9,), dtype=int32)\n",
      "param list:  Tensor(\"concat_765:0\", shape=(1, 9), dtype=int32)\n",
      "param_fin:\n",
      "  [[ 1  2  3  1  2  3  4  6  9  1  2  3  4  6  9  4  8 12]]\n",
      "x_3:  [[49 56 63]\n",
      " [56 64 72]\n",
      " [63 72 81]]\n",
      "param_new reshaped 2:  Tensor(\"Reshape_1865:0\", shape=(6,), dtype=int32)\n",
      "param list:  Tensor(\"Reshape_1848:0\", shape=(1, 3), dtype=int32)\n",
      "x_3:  [[343 392 441]\n",
      " [392 448 504]\n",
      " [441 504 567]\n",
      " [448 512 576]\n",
      " [504 576 648]\n",
      " [567 648 729]]\n",
      "added row:  Tensor(\"Reshape_1881:0\", shape=(3,), dtype=int32)\n",
      "param_new:  Tensor(\"Reshape_1880:0\", shape=(1, 6), dtype=int32)\n",
      "param_new:  [[343 392 441 448 504 567 448 512 576]]\n",
      "param_new reshaped 2:  Tensor(\"Reshape_1883:0\", shape=(9,), dtype=int32)\n",
      "param list:  Tensor(\"concat_781:0\", shape=(1, 9), dtype=int32)\n",
      "param_fin:\n",
      "  [[  1   2   3   1   2   3   4   6   9   1   2   3   4   6   9   4   8  12]\n",
      " [  7   8   9  49  56  63  64  72  81 343 392 441 448 504 567 448 512 576]]\n"
     ]
    }
   ],
   "source": [
    "sess= tf.Session()\n",
    "degree = 2\n",
    "x = tf.constant([[1,2,3],[7,8,9]])\n",
    "num_batches = 2\n",
    "num_param = 3\n",
    "param_fin = tf.constant([])\n",
    "\n",
    "for i in range(num_batches):\n",
    "    num_row = num_param\n",
    "    x_0 = x[i, :]\n",
    "    param_list = tf.reshape(x_0, [1, -1])\n",
    "#     print ('param_list', param_list.eval(session=sess))\n",
    "    x_1 = tf.reshape(x_0, [-1, 1])\n",
    "#     print ('x_1: ', x_1.eval(session=sess))\n",
    "    \n",
    "    x_2 = tf.reshape(x_1, [1, -1])\n",
    "    \n",
    "    for deg in range(1, degree+1):\n",
    "        x_3 = tf.matmul(x_1, x_2)\n",
    "        print ('x_3: ', x_3.eval(session=sess))\n",
    "        param_new = tf.constant([])\n",
    "#         param_new[num_row] = x_3[num_row, num_row\n",
    "        counter = 0\n",
    "        for j in range(num_param):\n",
    "            for k in range(num_param):\n",
    "                if k >= j:\n",
    "                    diag = x_3[j,k]\n",
    "                    diag = tf.reshape(diag, [-1])\n",
    "                    counter += 1\n",
    "                    try:\n",
    "                        param_new = tf.concat([param_new, diag[None, :]], 0)\n",
    "                    except:\n",
    "                        param_new = tf.reshape(diag, [-1, 1])\n",
    "#                     print('counter: ', counter)\n",
    "#                     print('param_new: ', param_new.eval(session=sess))\n",
    "                    param_new = tf.reshape(param_new, [counter, -1])\n",
    "        #             print ('concating: ', param_new.eval(session=sess))\n",
    "#         diag = x_3[num_row-1,num_param-1]\n",
    "        param_new = tf.reshape(param_new, [1, -1])\n",
    "#         print ('param_new: ', param_new.eval(session=sess))\n",
    "        for j in range(num_param, num_row):\n",
    "            diag = x_3[j,:]\n",
    "            diag = tf.reshape(diag, [-1])\n",
    "            print('added row: ', diag)\n",
    "            print('param_new: ', param_new)\n",
    "            param_new = tf.concat([param_new, diag[None, :]], 1)\n",
    "            print('param_new: ', param_new.eval(session=sess))\n",
    "            counter += num_param\n",
    "            param_new = tf.reshape(param_new, [counter, -1])\n",
    "#             for k in range(num_param):\n",
    "#                 diag = x_3[j,k]\n",
    "#                 diag = tf.reshape(diag, [-1])\n",
    "#                 print('diag')\n",
    "#                 counter += 1\n",
    "#                 param_new = tf.concat([param_new, diag[None, :]], 0)\n",
    "#                 param_new = tf.reshape(param_new, [counter, -1])\n",
    "#         diag = tf.reshape(diag, [-1])\n",
    "#         print('last: ', diag.eval(session=sess))\n",
    "#         param_new = tf.concat([param_new, diag[None, :]], 0)\n",
    "#         print ('param_new: ', param_new.eval(session=sess))\n",
    "        param_new = tf.reshape(param_new, [-1])\n",
    "        print ('param_new reshaped 2: ', param_new)\n",
    "        print ('param list: ', param_list)\n",
    "        param_list = tf.concat([param_list, param_new[None,:]], 1)\n",
    "        x_1 = tf.reshape(param_new, [-1, 1])\n",
    "#         print ('x_1: ', x_1.eval(session=sess))\n",
    "        num_row += 1\n",
    "        \n",
    "    try:\n",
    "        param_list = tf.reshape(param_list, [-1])\n",
    "        param_fin = tf.concat([param_fin, param_list[None, :]], 0)\n",
    "    except:\n",
    "        param_fin = tf.reshape(param_list, [1, -1])\n",
    "    print ('param_fin:\\n ', param_fin.eval(session=sess))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "[[  0.   1.   2.   0.   0.   0.   1.   2.   4.]\n",
      " [  3.   4.   5.   9.  12.  15.  16.  20.  25.]\n",
      " [  6.   7.   8.  36.  42.  48.  49.  56.  64.]\n",
      " [  9.  10.  11.  81.  90.  99. 100. 110. 121.]\n",
      " [ 12.  13.  14. 144. 156. 168. 169. 182. 196.]\n",
      " [ 15.  16.  17. 225. 240. 255. 256. 272. 289.]]\n",
      "[6 9]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "print('Done.')\n",
    "\n",
    "def expand_2(input_param_1, batch_size, num_param):\n",
    "    for row in range(batch_size):\n",
    "        param_tmp = input_param_1[row,:]\n",
    "        param_tmp = tf.reshape(param_tmp, [1, -1])\n",
    "        param_mat = tf.matmul(tf.transpose(param_tmp),param_tmp)\n",
    "\n",
    "        for i in range(num_param):\n",
    "            param_tmp2 = tf.reshape(param_mat[i,i:], [1,-1])\n",
    "            #print(param_tmp2.eval(session=sess))\n",
    "            param_tmp = tf.concat([param_tmp,param_tmp2], 1)\n",
    "        param_tmp = tf.reshape(param_tmp, [-1])\n",
    "\n",
    "        try:\n",
    "            input_parameter = tf.concat([input_parameter,param_tmp[None, :]],0)\n",
    "            input_parameter = tf.reshape(input_parameter, [row+1, -1])\n",
    "        except:\n",
    "            input_parameter = param_tmp\n",
    "            input_parameter = tf.reshape(input_parameter, [row+1, -1])\n",
    "    \n",
    "    return input_parameter\n",
    "\n",
    "sess = tf.Session()\n",
    "data = np.arange(18)\n",
    "data = data.reshape([6,-1])\n",
    "batch_size = 6\n",
    "num_param = 3\n",
    "input_param_1 = tf.constant(data, dtype=tf.float32)\n",
    "input_parameter = expand_2(input_param_1, batch_size, num_param)\n",
    "\n",
    "# for row in range(batch_size):\n",
    "#     param_tmp = input_param_1[row,:]\n",
    "#     param_tmp = tf.reshape(param_tmp, [1, -1])\n",
    "#     param_mat = tf.matmul(tf.transpose(param_tmp),param_tmp)\n",
    "\n",
    "#     for i in range(num_param):\n",
    "#         param_tmp2 = tf.reshape(param_mat[i,i:], [1,-1])\n",
    "#         #print(param_tmp2.eval(session=sess))\n",
    "#         param_tmp = tf.concat([param_tmp,param_tmp2], 1)\n",
    "#     param_tmp = tf.reshape(param_tmp, [-1])\n",
    "\n",
    "#     try:\n",
    "#         input_parameter = tf.concat([input_parameter,param_tmp[None, :]],0)\n",
    "#         input_parameter = tf.reshape(input_parameter, [row+1, -1])\n",
    "#     except:\n",
    "#         input_parameter = param_tmp\n",
    "#         input_parameter = tf.reshape(input_parameter, [row+1, -1])\n",
    "\n",
    "print(input_parameter.eval(session=sess))\n",
    "print(tf.shape(input_parameter).eval(session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_input(train_path):\n",
    "    train = open(train_path, 'r')\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    counter = 0\n",
    "    for line in train:\n",
    "        if counter > 10:\n",
    "            break\n",
    "        data = eval(line)\n",
    "        if data[1] > 0:\n",
    "            train_x.append(data[0])\n",
    "#             inv_vol = 1/data[1]\n",
    "            train_y.append(data[1])\n",
    "        counter += 1\n",
    "    print ('Number of data: ', len(train_x))\n",
    "    \n",
    "    return train_x, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  -35.0043392\n",
      "Number of data:  11\n",
      "diff 0.5022808532687777\n",
      "diff 0.5022808532542258\n",
      "diff 0.327472965164726\n",
      "diff 0.5022808533524512\n",
      "diff 0.32747296516472557\n",
      "diff 0.2494767435816346\n",
      "diff 0.23791506801962586\n",
      "diff 0.5022808532396739\n",
      "diff 0.3274729651647258\n",
      "diff 0.24947674356162572\n",
      "diff 0.23791506801962442\n"
     ]
    }
   ],
   "source": [
    "W = np.array([-0.87437505,\n",
    "             6.952491,\n",
    "              4.5137253,\n",
    "              0.74184924,\n",
    "              2.0514772 ,\n",
    "             -0.14970087,\n",
    "             -0.13637641,\n",
    "             -0.20277256,\n",
    "             -0.05430971])\n",
    "b = np.array([-35.561787,\n",
    "             -35.39475 ,\n",
    "             -35.3224  ,\n",
    "             -34.635918,\n",
    "             -34.72557 ,\n",
    "             -34.767803,\n",
    "             -34.995155,\n",
    "             -34.83515 ,\n",
    "             -34.908066,\n",
    "             -34.896793])\n",
    "W = np.array([-1.32813477e+03,\n",
    "              2.83764465e+02,\n",
    "              5.81152466e+02,\n",
    "              1.65513077e+02,\n",
    "              1.64283314e+01,\n",
    "             -1.20188210e+02,\n",
    "              1.55702343e+01,\n",
    "              1.09391075e+02,\n",
    "             -1.57943008e+02,\n",
    "             -4.78923941e+00,\n",
    "             -2.92627001e+00,\n",
    "              1.90355957e+00,\n",
    "             -5.39966488e+00,\n",
    "             -4.67496252e+00,\n",
    "              1.12794566e+00,\n",
    "              7.61268473e+00,\n",
    "              1.16817160e+01,\n",
    "              6.08279085e+00,\n",
    "             -1.99237764e+00])\n",
    "b = np.array([-811.7567 \n",
    "             -751.0992 ,\n",
    "             -827.5288 ,\n",
    "             -879.7452 ,\n",
    "             -963.4682 ,\n",
    "             -740.0352 ,\n",
    "             -635.4922 ,\n",
    "             -712.0758 ,\n",
    "             -713.21295,\n",
    "             -757.816  ])\n",
    "\n",
    "# Mean:\n",
    "b_mean = np.mean(b)\n",
    "print('mean: ', b_mean)\n",
    "\n",
    "# Test if this model actually works\n",
    "train_path = '/home/carnd/CYML/output/train/cylinder/lift_1_to_50.txt'\n",
    "train_data, train_vol = load_input(train_path)\n",
    "num_data = len(train_data)\n",
    "num_param = len(train_data[0])\n",
    "# transform the num_data\n",
    "\n",
    "for i in range(num_data):\n",
    "    data = np.array(train_data[i])\n",
    "    data = np.expand_dims(data, axis=1)\n",
    "#     print('original: ', data)\n",
    "    data2 = data.reshape([num_param]).tolist()\n",
    "#     print('reshaped: ',data2)\n",
    "    data = np.matmul(data, np.transpose(data))\n",
    "#     print('matrix: ', data)\n",
    "    for j in range(num_param):\n",
    "        # for each row of this matrix\n",
    "        data2 += data[j,j:].tolist()\n",
    "        #data2.append(data[j,j:].tolist())\n",
    "#     print('extended data: ', data2)\n",
    "    pred = np.dot(data2, W)\n",
    "    diff = abs(1/float(pred) - train_vol[i])\n",
    "    print('diff', diff)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data:  10\n",
      "train_data:  [[1, 0, 0], [1, 1, 0], [2, 0, 0], [1, 1, 1], [2, 1, 0], [3, 0, 0], [2, 1, 1], [2, 2, 0], [3, 1, 0], [4, 0, 0]]\n",
      "train_vol:  [1.6874999999416787, 2.3703703703703702, 3.3749999998833573, 2.8854381997703142, 2.4816806884647713, 5.06249999966964, 4.417092671887906, 4.740740740300995, 2.5292143561705105, 6.749999999352326]\n",
      "y:  [[-2033.21429352]\n",
      " [-1724.64925205]\n",
      " [-2898.33450839]\n",
      " [-1291.62351806]\n",
      " [-2589.2702816 ]\n",
      " [-3489.89944218]\n",
      " [-2055.25805308]\n",
      " [-2260.39522943]\n",
      " [-3191.13535983]\n",
      " [-3836.64453135]]\n",
      "train_vol:  [1.6875     2.37037037 3.375      2.8854382  2.48168069 5.0625\n",
      " 4.41709267 4.74074074 2.52921436 6.75      ]\n",
      "cost:  7048172.7598907305\n",
      "W:  [[-1.32813477e+03]\n",
      " [ 2.83764465e+02]\n",
      " [ 5.81152466e+02]\n",
      " [ 1.65513077e+02]\n",
      " [ 1.64283314e+01]\n",
      " [-1.20188210e+02]\n",
      " [ 1.55702343e+01]\n",
      " [ 1.09391075e+02]\n",
      " [-1.57943008e+02]\n",
      " [-4.78923941e+00]\n",
      " [-2.92627001e+00]\n",
      " [ 1.90355957e+00]\n",
      " [-5.39966488e+00]\n",
      " [-4.67496252e+00]\n",
      " [ 1.12794566e+00]\n",
      " [ 7.61268473e+00]\n",
      " [ 1.16817160e+01]\n",
      " [ 6.08279085e+00]\n",
      " [-1.99237764e+00]]\n",
      "diff:  [0.59308442]\n",
      "diff:  [0.42245483]\n",
      "diff:  [0.29664132]\n",
      "diff:  [0.34734203]\n",
      "diff:  [0.40333894]\n",
      "diff:  [0.19781741]\n",
      "diff:  [0.22687981]\n",
      "diff:  [0.2113799]\n",
      "diff:  [0.39569306]\n",
      "diff:  [0.14840879]\n"
     ]
    }
   ],
   "source": [
    "def load_input(train_path):\n",
    "    train = open(train_path, 'r')\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    counter = 0\n",
    "    for line in train:\n",
    "        if counter >= 10:\n",
    "            break\n",
    "        data = eval(line)\n",
    "        if data[1] > 0:\n",
    "            train_x.append(data[0])\n",
    "            inv_vol = 1/data[1]\n",
    "            train_y.append(inv_vol)\n",
    "        counter += 1\n",
    "    print ('Number of data: ', len(train_x))\n",
    "    \n",
    "    return train_x, train_y\n",
    "\n",
    "def expand_3(input_param_1, W, b_m, train_vol):\n",
    "    # TODO: make this more general\n",
    "    num_param = 3\n",
    "    input_param = 10\n",
    "    for row in range(batch_size):\n",
    "        param_tmp = input_param_1[row,:]\n",
    "        param_tmp = tf.reshape(param_tmp, [1, -1])\n",
    "        p1 = param_tmp[0,0]\n",
    "        p2 = param_tmp[0,1]\n",
    "        p3 = param_tmp[0,2]\n",
    "        \n",
    "        p = tf.pow(p1,2)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.pow(p2,2)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.pow(p3,2)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        \n",
    "        p = tf.multiply(p1, p2)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.multiply(p1, p3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.multiply(p2, p3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        \n",
    "        p = tf.pow(p1,3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.pow(p2,3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.pow(p3,3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        \n",
    "        p = tf.multiply(tf.pow(p1,2), p2)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.multiply(tf.pow(p1,2), p3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.multiply(p1, tf.pow(p2,2))\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.multiply(p1, tf.pow(p3,2))\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        \n",
    "        p = tf.multiply(tf.pow(p2,2), p3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.multiply(tf.pow(p3,2), p2)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        \n",
    "        p = tf.multiply(tf.multiply(p1, p2),p3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        param_tmp = tf.cast(param_tmp, tf.float64)\n",
    "        \n",
    "        try:\n",
    "            param_tmp = tf.reshape(param_tmp, [-1])\n",
    "            input_param = tf.concat([input_param, param_tmp[None,:]], 0)\n",
    "        except:\n",
    "            input_param = tf.reshape(param_tmp, [1, -1])\n",
    "    \n",
    "    W = tf.constant(W)\n",
    "    W = tf.reshape(W, [-1,1])\n",
    "    W = tf.cast(W, tf.float64)\n",
    "    b_m = tf.constant(b_m)\n",
    "    b_m = tf.cast(b_m, tf.float64)\n",
    "    y = tf.add(tf.matmul(input_param, W), b_m)\n",
    "    train_vol = tf.constant(train_vol, dtype=tf.float64)\n",
    "    train_vol = tf.cast(train_vol, tf.float64)\n",
    "    print('y: ', y.eval(session=sess))\n",
    "    print('train_vol: ', train_vol.eval(session=sess))\n",
    "#     cost = tf.reduce_mean(tf.pow(y-tf.multiply(train_vol, y),2))\n",
    "    cost = tf.reduce_mean(tf.pow(y-train_vol,2))\n",
    "    num_param = num_param + 6+ 10\n",
    "    return input_param, cost\n",
    "\n",
    "W = np.array([-1.32813477e+03,\n",
    "              2.83764465e+02,\n",
    "              5.81152466e+02,\n",
    "              1.65513077e+02,\n",
    "              1.64283314e+01,\n",
    "             -1.20188210e+02,\n",
    "              1.55702343e+01,\n",
    "              1.09391075e+02,\n",
    "             -1.57943008e+02,\n",
    "             -4.78923941e+00,\n",
    "             -2.92627001e+00,\n",
    "              1.90355957e+00,\n",
    "             -5.39966488e+00,\n",
    "             -4.67496252e+00,\n",
    "              1.12794566e+00,\n",
    "              7.61268473e+00,\n",
    "              1.16817160e+01,\n",
    "              6.08279085e+00,\n",
    "             -1.99237764e+00])\n",
    "b = np.array([-811.7567 \n",
    "             -751.0992 ,\n",
    "             -827.5288 ,\n",
    "             -879.7452 ,\n",
    "             -963.4682 ,\n",
    "             -740.0352 ,\n",
    "             -635.4922 ,\n",
    "             -712.0758 ,\n",
    "             -713.21295,\n",
    "             -757.816  ])\n",
    "\n",
    "sess = tf.Session()\n",
    "train_path = '/home/carnd/CYML/output/train/cylinder/lift_1_to_50.txt'\n",
    "train_data, train_vol = load_input(train_path)\n",
    "print('train_data: ', train_data)\n",
    "print('train_vol: ', train_vol)\n",
    "input_data = tf.constant(train_data)\n",
    "b = b.reshape([-1,1])\n",
    "b_mean = np.mean(b)\n",
    "input_param, cost = expand_3(input_data, W, b_mean, train_vol)\n",
    "print('cost: ', cost.eval(session=sess))\n",
    "input_data = input_param.eval(session=sess)\n",
    "# print('input_param: ', input_data)\n",
    "W = W.reshape([-1,1])\n",
    "print('W: ', W)\n",
    "pred_list = np.dot(input_data, W)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for row in range(pred_list.shape[0]):\n",
    "    pred = 1/(pred_list[row]+b_mean)\n",
    "    diff = abs(1/train_vol[row] - pred)\n",
    "    print ('diff: ', diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mat_mul(param, batch_size, num_param, deg):\n",
    "    # Hyper-parmeter: how many layers we will need\n",
    "    with tf.variable_scope(\"mat_mul\", reuse=True):\n",
    "        input_param_1 = tf.placeholder(name=\"input_param_1\", dtype=tf.float32, shape=(batch_size, num_param))\n",
    "        input_volume = tf.placeholder(name=\"input_volume\", dtype=tf.float32, shape=(batch_size))\n",
    "        \n",
    "        for i in range(deg):\n",
    "            deg_n1 = tf.matmul(deg_n0, tf.transpose(deg_1))\n",
    "            deg_n2 = tf.concat([deg_n1, deg_1])\n",
    "        \n",
    "        \n",
    "        \n",
    "        input_parameter = expand_2(input_param_1, batch_size, num_param)\n",
    "        num_param = int(num_param + num_param*(num_param+1)/2)\n",
    "\n",
    "        W1 = tf.Variable(tf.random_normal([num_param, 1], dtype=tf.float32), name=\"W\")\n",
    "        b1 = tf.Variable(tf.zeros([batch_size, 1], dtype=tf.float32), name=\"b\")\n",
    "        y = tf.add(tf.matmul(input_parameter, W1), b1)\n",
    "        \n",
    "        cost_op = tf.reduce_mean(tf.pow(y-input_volume, 2))\n",
    "    \n",
    "    return y, cost_op, input_param_1, input_volume, input_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param_tmp: [[ 1  2  3  1  4  9  2  3  6  1  8 27  2  3  4  9 12 18  6]]\n",
      "init\n",
      "input_param: [[ 1  2  3  1  4  9  2  3  6  1  8 27  2  3  4  9 12 18  6]]\n",
      "param_tmp: [[   2    3   10    4    9  100    6   20   30    8   27 1000   12   40\n",
      "    18  200   90  300   60]]\n",
      "concat\n",
      "input_param: [[   1    2    3    1    4    9    2    3    6    1    8   27    2    3\n",
      "     4    9   12   18    6]\n",
      " [   2    3   10    4    9  100    6   20   30    8   27 1000   12   40\n",
      "    18  200   90  300   60]]\n",
      "num_param:  19\n"
     ]
    }
   ],
   "source": [
    "def expand_3(input_param_1, batch_size, num_param):\n",
    "    # TODO: make this more general\n",
    "    num_param = 3\n",
    "    input_param = 10\n",
    "    for row in range(batch_size):\n",
    "        param_tmp = input_param_1[row,:]\n",
    "        param_tmp = tf.reshape(param_tmp, [1, -1])\n",
    "        p1 = param_tmp[0,0]\n",
    "        p2 = param_tmp[0,1]\n",
    "        p3 = param_tmp[0,2]\n",
    "        \n",
    "        p = tf.pow(p1,2)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.pow(p2,2)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.pow(p3,2)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        \n",
    "        p = tf.multiply(p1, p2)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.multiply(p1, p3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.multiply(p2, p3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        \n",
    "        p = tf.pow(p1,3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.pow(p2,3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.pow(p3,3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        \n",
    "        p = tf.multiply(tf.pow(p1,2), p2)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.multiply(tf.pow(p1,2), p3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.multiply(p1, tf.pow(p2,2))\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.multiply(p1, tf.pow(p3,2))\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        \n",
    "        p = tf.multiply(tf.pow(p2,2), p3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.multiply(tf.pow(p3,2), p2)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        \n",
    "        p = tf.multiply(tf.multiply(p1, p2),p3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        \n",
    "        print('param_tmp:', param_tmp.eval(session=sess))\n",
    "        \n",
    "        try:\n",
    "            param_tmp = tf.reshape(param_tmp, [-1])\n",
    "            input_param = tf.concat([input_param, param_tmp[None,:]], 0)\n",
    "            print('concat')\n",
    "        except:\n",
    "            print('init')\n",
    "            input_param = tf.reshape(param_tmp, [1, -1])\n",
    "        print('input_param:',input_param.eval(session=sess))\n",
    "    \n",
    "    num_param = num_param + 6+ 10\n",
    "    return input_param, num_param\n",
    "\n",
    "sess = tf.Session()\n",
    "input_param = tf.constant([[1,2,3],[2,3,10]])\n",
    "input_param, num_param = expand_3(input_param, 2, 3)\n",
    "print ('num_param: ', num_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data:  50\n",
      "sample data:  [1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "number of train:  40\n",
      "number of test:  10\n",
      "number of parameters:  19\n",
      "number of test:  10\n",
      "batch_size:  5\n",
      "number of batches:  8\n",
      "Beginning Training\n",
      "epoch:  1\n",
      "batches:  8\n",
      "loss:  29.978914\n",
      "layer_1:  [[  0.34704268   2.223515    -1.0253036    0.0629617   -2.4453444\n",
      "    1.0642825   -1.7208438   -4.4426785    0.92104733  -1.8762028 ]\n",
      " [  1.3546227    2.6832635   -1.393161     2.390409    -1.0097661\n",
      "   -0.9549737   -1.8105437    0.6898159    0.07992053  -2.0335133 ]\n",
      " [ -5.879792    14.785764    -1.1561043   -0.22125626 -18.5046\n",
      "   -4.101163    -8.089755   -22.110945     8.773116    -5.6068845 ]\n",
      " [  4.0933013   -0.27020052  -0.55880696   4.1185055    3.4667315\n",
      "   -1.9713554   -2.0433202    0.68261653   0.45044565  -2.4501152 ]\n",
      " [ -1.863615    15.453334    -4.269288     3.596027   -21.272163\n",
      "   -7.8685412   -8.341673   -10.602402     4.489821    -8.241892  ]]\n",
      "layer_2:  [[ -2.3795393   6.2310963   5.031282   -2.233979    8.262679 ]\n",
      " [-11.452366   -3.857435   -0.7700499  -9.971713    3.2545621]\n",
      " [ -7.769945   40.86543    28.292162   -3.6150131  61.010086 ]\n",
      " [-10.062828  -12.002291    4.5779033 -18.109163   -1.9318999]\n",
      " [-34.199993   24.897812    7.8196096 -21.019156   48.443012 ]]\n",
      "layer_3:  [[ 2.2055926]\n",
      " [10.618729 ]\n",
      " [22.91346  ]\n",
      " [ 7.087777 ]\n",
      " [41.960327 ]]\n",
      "loss:  nan\n",
      "layer_1:  [[  -9.872473    30.888618    17.771967   -15.8659315  -49.24186\n",
      "    -9.102882    -6.2010736  -76.79456     42.167267     4.331983 ]\n",
      " [  23.201347    -1.5732956   12.627986   -12.393734     3.1894882\n",
      "     3.0430634    5.7538385  -26.087969    20.714611     8.207715 ]\n",
      " [  17.20269     -3.6168466   15.950061   -10.833149     1.0563898\n",
      "    11.014374    -7.220765    -3.7306385   17.302525     5.964096 ]\n",
      " [   6.8420334   25.79264     18.658995   -18.290628   -51.331852\n",
      "    -9.013893     1.1936188  -62.860844    39.118        4.666463 ]\n",
      " [ -31.622967    72.53395     45.123276   -33.084343  -124.52123\n",
      "   -36.34742    -11.949136  -163.48329     90.914566    14.66443  ]]\n",
      "layer_2:  [[162.77946   241.81555   198.95227   145.25113    74.33087  ]\n",
      " [ 63.942524   61.457436   52.919407   31.428549    3.264782 ]\n",
      " [ 39.48559    19.18786    21.014355   36.623672   -2.3784513]\n",
      " [143.7822    223.1391    156.51642   129.7472     46.685257 ]\n",
      " [364.45554   548.7718    439.19852   332.98914   174.0258   ]]\n",
      "layer_3:  [[-360.83572 ]\n",
      " [-105.37035 ]\n",
      " [ -28.415213]\n",
      " [-323.42487 ]\n",
      " [-802.41455 ]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "test loss:  nan\n",
      "test loss:  nan\n",
      "epoch:  2\n",
      "batches:  8\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "test loss:  nan\n",
      "test loss:  nan\n",
      "epoch:  3\n",
      "batches:  8\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "test loss:  nan\n",
      "test loss:  nan\n",
      "epoch:  4\n",
      "batches:  8\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "test loss:  nan\n",
      "test loss:  nan\n",
      "epoch:  5\n",
      "batches:  8\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "test loss:  nan\n",
      "test loss:  nan\n",
      "epoch:  6\n",
      "batches:  8\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "test loss:  nan\n",
      "test loss:  nan\n",
      "epoch:  7\n",
      "batches:  8\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "test loss:  nan\n",
      "test loss:  nan\n",
      "epoch:  8\n",
      "batches:  8\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "test loss:  nan\n",
      "test loss:  nan\n",
      "epoch:  9\n",
      "batches:  8\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "loss:  nan\n",
      "layer_1:  [[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "layer_2:  [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "layer_3:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "test loss:  nan\n",
      "test loss:  nan\n",
      "Done.\n",
      "name:  mat_mul_3/W1:0\n",
      "variables:  [[ 0.01889279  1.1731416  -0.69022787  1.2698584  -1.1526166  -0.18205802\n",
      "   0.38871107 -0.05269752 -2.7975726   1.3411752 ]\n",
      " [-0.42031848 -0.8382207   0.54628074 -2.1214693  -1.5459927   0.7632903\n",
      "   0.8335535  -0.751637   -0.45606586 -0.521169  ]\n",
      " [-1.2899598  -0.6048026  -0.5172644  -1.0212617  -1.1072863  -0.24764095\n",
      "   0.2655982  -0.98136765  0.19952951  1.4905736 ]\n",
      " [ 0.90545034  0.39113307  2.5136666   0.6911476  -1.1909834  -0.7822424\n",
      "   0.3258385  -0.29613015 -1.3518872   1.5454707 ]\n",
      " [-0.57240576  0.97887367 -0.64365125  2.1068149   0.7835439   0.42370003\n",
      "   0.5933904   0.7272995   0.5989841   0.40562612]\n",
      " [ 0.3220456   0.63559157  0.41091153  2.8018124   0.80523515 -0.02561277\n",
      "  -0.07991271  0.02084642  0.7068095   1.4766989 ]\n",
      " [ 2.215357   -0.46536562 -0.30615455 -0.10457885  1.1690133   1.6452374\n",
      "   0.83625656 -0.34333342  1.2136581   1.1167208 ]\n",
      " [-1.1271625   0.68536     0.9052691   1.1208858  -0.69850487  0.5861502\n",
      "  -0.13761777  0.99174327 -2.3640416  -0.4102605 ]\n",
      " [-1.278484   -0.1217535  -2.780473    1.2160932   0.1786375   1.0492665\n",
      "   0.47258264  1.434958    1.7158726   2.0079823 ]\n",
      " [-1.0051503   0.5032449  -1.3198583   0.9084678   0.73055714  0.73841435\n",
      "  -0.01517805  1.0094396  -0.77081746  2.4922094 ]\n",
      " [-0.7023448   0.914443    1.1024433  -0.5838014   0.46506977  0.02614978\n",
      "  -1.3287312   0.39746827  0.258669    0.6823163 ]\n",
      " [ 1.0293866   1.5207082  -1.229238    1.7494309  -1.3967725   0.74437916\n",
      "  -0.4250992  -0.392578    0.07874372  0.9180635 ]\n",
      " [-1.2358423  -0.58034265  0.47833738 -0.6628338   0.42828274  1.0056077\n",
      "   2.2486312   1.8557547  -0.07551771 -0.3434826 ]\n",
      " [ 0.2824777  -0.94457227  1.510344    0.03361775 -1.918955    0.38440725\n",
      "   0.14587803  2.8171513  -0.31635106  1.0535243 ]\n",
      " [ 1.8315792   0.6265764   0.45011595  0.9939215   1.3612897   2.5672944\n",
      "  -0.4218812  -1.8169458  -0.28974855  0.27243274]\n",
      " [-1.6573019  -0.41179606 -1.180244   -1.0155393  -1.4155755   0.9831642\n",
      "   0.08137202  0.6965649  -0.684644   -1.6829432 ]\n",
      " [-1.2967712  -0.2431319   0.37260506  1.4704688   1.8268503   0.20976673\n",
      "  -0.8610864  -0.21986534  1.4790355   0.09925737]\n",
      " [ 1.108078   -0.14829034  1.2126023   0.41379523  1.3712088   0.89050156\n",
      "  -0.31350714 -1.2864527  -0.18402004 -0.05204884]\n",
      " [ 0.18881531  0.5421807   0.04353924 -0.08268093 -1.3864217   0.8784776\n",
      "   0.69859374 -0.29870862  1.4277818  -0.0032629 ]]\n",
      "name:  mat_mul_3/W2:0\n",
      "variables:  [[ 0.72017175 -0.4603827   2.2180524   0.16607572  1.0467061 ]\n",
      " [-1.3015352   1.2260387  -0.4121747  -1.3153825  -0.5513834 ]\n",
      " [ 0.80568546 -0.90809834 -0.21210037 -1.0257756   1.5743352 ]\n",
      " [-0.2018168   1.1093336  -0.23534606  2.221409    0.5534677 ]\n",
      " [-0.43587983  0.07592291 -0.08631987  0.85178345  2.244951  ]\n",
      " [-0.36466488  0.4907734  -1.734786    0.25479868 -0.27476364]\n",
      " [-0.8548157   0.18232709 -0.35608152 -0.5883759  -0.7672242 ]\n",
      " [-0.2442467   1.6195711  -1.0209025  -0.4126497   0.5539556 ]\n",
      " [ 0.72108704  0.5412746  -0.5512062   0.20223533 -0.04588975]\n",
      " [-2.3129559  -0.5413469  -1.1677616   1.5958737   0.91315883]]\n",
      "name:  mat_mul_3/W3:0\n",
      "variables:  [[ 0.49973905]\n",
      " [-1.0567588 ]\n",
      " [ 0.5859393 ]\n",
      " [-0.23184028]\n",
      " [ 0.24160631]]\n",
      "name:  mat_mul_3/b3:0\n",
      "variables:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# Game plan:\n",
    "# Method one: extend input paramters\n",
    "# Method two: multiply variables together to obtain non-linearity\n",
    "\n",
    "def load_input_extend(train_path):\n",
    "    train = open(train_path, 'r')\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    counter = 1\n",
    "    for line in train:\n",
    "        if counter > 50:\n",
    "            break\n",
    "        data = eval(line)\n",
    "        counter += 1\n",
    "        if data[1] > 0:\n",
    "            p1 = data[0][0]\n",
    "            p2 = data[0][1]\n",
    "            p3 = data[0][2]\n",
    "            p = p1**2\n",
    "            data[0].append(p)\n",
    "            p = p2**2\n",
    "            data[0].append(p)\n",
    "            p = p3**2\n",
    "            data[0].append(p)\n",
    "            p = p1*p2\n",
    "            data[0].append(p)\n",
    "            p = p1*p3\n",
    "            data[0].append(p)\n",
    "            p = p2*p3\n",
    "            data[0].append(p)\n",
    "            p = p1**3\n",
    "            data[0].append(p)\n",
    "            p = p2**3\n",
    "            data[0].append(p)\n",
    "            p = p3**3\n",
    "            data[0].append(p)\n",
    "            p = (p1**2)*p2\n",
    "            data[0].append(p)\n",
    "            p = (p1**2)*p3\n",
    "            data[0].append(p)\n",
    "            p = p1*(p2**2)\n",
    "            data[0].append(p)\n",
    "            p = p1*(p3**2)\n",
    "            data[0].append(p)\n",
    "            p = (p2**2)*p3\n",
    "            data[0].append(p)\n",
    "            p = (p3**2)*p2\n",
    "            data[0].append(p)\n",
    "            p = p1*p2*p3 \n",
    "            data[0].append(p)\n",
    "            train_x.append(data[0])\n",
    "            inv_vol = 1/data[1]\n",
    "            train_y.append(inv_vol)\n",
    "    print ('Number of data: ', len(train_x))\n",
    "    \n",
    "    return train_x, train_y\n",
    "\n",
    "def mat_mul_3(param, batch_size, num_param):\n",
    "    # TODO: make this more general\n",
    "    with tf.variable_scope(\"mat_mul_3\", reuse=True):\n",
    "        input_parameter = tf.placeholder(name=\"input_parameter\", dtype=tf.float32, shape=(batch_size, num_param))\n",
    "        input_volume = tf.placeholder(name=\"input_volume\", dtype=tf.float32, shape=(batch_size))\n",
    "        \n",
    "        # Layer 1:\n",
    "        # Input: (batch_size, num_param)\n",
    "        # W1: (num_param, 10), b: (batch_size, 10)\n",
    "        # Output: (batch_size, 10)\n",
    "        W1 = tf.Variable(tf.random_normal([num_param, 10], dtype=tf.float32), name=\"W1\")\n",
    "        y = tf.matmul(input_parameter, W1)\n",
    "        layer_1 = y\n",
    "        \n",
    "        # Layer 2:\n",
    "        # Input: (batch_size, 10)\n",
    "        # W1: (10, 5), b: (batch_size, 5)\n",
    "        # Output: (batch_size, 5)\n",
    "        W2 = tf.Variable(tf.random_normal([10, 5], dtype=tf.float32), name=\"W2\")\n",
    "        y = tf.matmul(y, W2)\n",
    "        layer_2 = y\n",
    "        \n",
    "        # Layer 3:\n",
    "        # Input: (batch_size, 5)\n",
    "        # W1: (5, 1), b: (batch_size, 1)\n",
    "        # Output: (batch_size, 1)\n",
    "        W3 = tf.Variable(tf.random_normal([5, 1], dtype=tf.float32), name=\"W3\")\n",
    "        b3 = tf.Variable(tf.zeros([batch_size, 1], dtype=tf.float32), name=\"b3\")\n",
    "        y = tf.add(tf.matmul(y, W3), b3)\n",
    "        layer_3 = y\n",
    "        \n",
    "        #cost_op = tf.reduce_mean(tf.pow(tf.pow(y,-1)-tf.multiply(tf.pow(input_volume,-1), y), 2))\n",
    "        cost_op = tf.reduce_mean(tf.pow(y-input_volume, 2))\n",
    "        #cost_op = tf.reduce_mean(tf.multiply(tf.pow(y-input_volume, 2), tf.pow(y, -3/4)))\n",
    "\n",
    "    return y, cost_op, input_parameter, input_volume, layer_1, layer_2, layer_3\n",
    "\n",
    "def train(data, vol):\n",
    "    num_data = len(data)\n",
    "    split_idx = int(num_data*0.8)\n",
    "    \n",
    "    input_data = data[0:split_idx]\n",
    "    input_vol = vol[0:split_idx]\n",
    "    \n",
    "    test_data = data[split_idx:]\n",
    "    test_vol = vol[split_idx:]\n",
    "    \n",
    "    num_train = split_idx\n",
    "    num_test = num_data - split_idx\n",
    "    num_param = len(input_data[0])\n",
    "    num_test = len(test_data)\n",
    "    print('number of train: ', num_train)\n",
    "    print('number of test: ', num_test)\n",
    "    print('number of parameters: ', num_param)\n",
    "    print('number of test: ', num_test)\n",
    "    \n",
    "    num_batches = int(num_train/batch_size)\n",
    "    print('batch_size: ', batch_size)\n",
    "    print('number of batches: ', num_batches)\n",
    "    test_batches = int(num_test/batch_size)\n",
    "    input_param = np.array(input_data)\n",
    "    input_vol = np.array(input_vol)\n",
    "    test_param = np.array(test_data)\n",
    "    test_vol = np.array(test_vol)\n",
    "    \n",
    "    #y = fl_linear(input_param)\n",
    "    y, cost_op, input_parameter, input_volume, layer_1, layer_2, layer_3 = mat_mul_3(input_param, batch_size, num_param)\n",
    "    \n",
    "    learning_rate = tf.Variable(0.5, trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(cost_op)\n",
    "    \n",
    "    sess = tf.Session() # Create TensorFlow session\n",
    "    saver = tf.train.Saver() # Save Tensorflow graph\n",
    "    print (\"Beginning Training\")\n",
    "    with sess.as_default():\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        sess.run(tf.assign(learning_rate, alpha))\n",
    "        for epoch in range(1, max_epochs):\n",
    "            print('epoch: ', epoch)\n",
    "            train_one_epoch(sess, cost_op, train_op, num_batches, input_parameter, input_param, input_volume, input_vol, y, layer_1, layer_2, layer_3)\n",
    "            eval_one_epoch(sess, cost_op, test_batches, input_parameter, test_param, input_volume, test_vol)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                path = \"log/model_3.ckpt\"\n",
    "                save_path = saver.save(sess, path)\n",
    "                print(\"Saved.\")\n",
    "            \n",
    "        print('Done.')\n",
    "        \n",
    "        collection = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='mat_mul_3')\n",
    "        for x in collection:\n",
    "            if x.name == 'mat_mul_3/W1:0' or x.name == 'mat_mul_3/W2:0'or x.name == 'mat_mul_3/W3:0' or x.name == 'mat_mul_3/b3:0':\n",
    "                print(\"name: \", x.name)\n",
    "                print(\"variables: \", sess.run(x))\n",
    "                \n",
    "def train_one_epoch(sess, cost_op, train_op, num_batches, input_parameter, input_param, input_volume, input_vol, y, layer_1, layer_2, layer_3):\n",
    "    print('batches: ', num_batches)\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx+1) * batch_size\n",
    "\n",
    "        batch_cost,_, batch_layer_1, batch_layer_2, batch_layer_3 = sess.run([cost_op, train_op, layer_1, layer_2, layer_3], feed_dict={input_parameter: input_param[start_idx:end_idx], input_volume: input_vol[start_idx:end_idx]})\n",
    "        print(\"loss: \", batch_cost)\n",
    "        print(\"layer_1: \", batch_layer_1)\n",
    "        print(\"layer_2: \", batch_layer_2)\n",
    "        print(\"layer_3: \", batch_layer_3)\n",
    "\n",
    "def eval_one_epoch(sess, cost_op, test_batches, input_parameter, test_param, input_volume, test_vol):\n",
    "    for batch_idx in range(test_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx+1) * batch_size\n",
    "        cost = sess.run(cost_op, feed_dict={input_parameter: test_param[start_idx:end_idx], input_volume: test_vol[start_idx:end_idx]})\n",
    "        print(\"test loss: \", cost)\n",
    "\n",
    "# Parameters:\n",
    "last_cost = 0\n",
    "alpha = 0.4\n",
    "batch_size = 5\n",
    "max_epochs = 10        \n",
    "\n",
    "train_path = '/home/carnd/CYML/output/train/cylinder/lift_1_to_50.txt'\n",
    "train_data, train_vol = load_input_extend(train_path)\n",
    "print ('sample data: ', train_data[0])\n",
    "train(train_data, train_vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invert:  0.5925925926130731\n",
      "invert:  1.6874999999416787\n"
     ]
    }
   ],
   "source": [
    "print ('invert: ', train_y[0]**(-1))\n",
    "print ('invert: ', (train_y[0]**(-1))**(-1))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
