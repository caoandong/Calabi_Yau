{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input: parametrized heights\n",
    "# Output: volume\n",
    "\n",
    "def placeholder_inputs(batch_size, num_param):\n",
    "    param_pl = tf.placeholder(name=\"input_param\", dtype=tf.float32, shape=(batch_size, num_param))\n",
    "    vol_pl = tf.placeholder(name=\"input_vol\", dtype=tf.float32, shape=(batch_size))\n",
    "    \n",
    "    return param_pl, vol_pl\n",
    "\n",
    "def load_input(train_path):\n",
    "    train = open(train_path, 'r')\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    for line in train:\n",
    "        data = eval(line)\n",
    "        if data[1] > 0:\n",
    "            train_x.append(data[0])\n",
    "            inv_vol = 1/data[1]\n",
    "            train_y.append(inv_vol)\n",
    "    print ('Number of data: ', len(train_x))\n",
    "    \n",
    "    return train_x, train_y\n",
    "\n",
    "def fl_linear(param, batch_size, num_param):\n",
    "    with tf.variable_scope(\"fl_linear\", reuse=True):\n",
    "        input_param = tf.placeholder(name=\"input_param\", dtype=tf.float32, shape=(batch_size, num_param))\n",
    "        input_vol = tf.placeholder(name=\"input_vol\", dtype=tf.float32, shape=(batch_size))\n",
    "        # Linear activation\n",
    "        net = tf.fully_connected(1, activation_fn=None, scope='fc1')\n",
    "    \n",
    "    return net\n",
    "\n",
    "def mat_mul(param, batch_size, num_param):\n",
    "    with tf.variable_scope(\"mat_mul\", reuse=True):\n",
    "        input_parameter = tf.placeholder(name=\"input_parameter\", dtype=tf.float32, shape=(batch_size, num_param))\n",
    "        input_volume = tf.placeholder(name=\"input_volume\", dtype=tf.float32, shape=(batch_size))\n",
    "        W = tf.Variable(tf.random_normal([num_param, 1], dtype=tf.float32), name=\"W\")\n",
    "        b = tf.Variable(tf.zeros([batch_size, 1], dtype=tf.float32), name=\"b\")\n",
    "        y = tf.add(tf.matmul(input_parameter, W), b)\n",
    "        cost_op = tf.reduce_mean(tf.pow(y-input_volume, 2))\n",
    "\n",
    "    return y, cost_op, input_parameter, input_volume\n",
    "\n",
    "def train(data, vol):\n",
    "    \n",
    "    input_data = data[0:1000]\n",
    "    input_vol = vol[0:1000]\n",
    "    \n",
    "    test_data = data[1000:2000]\n",
    "    test_vol = vol[1000:2000]\n",
    "    \n",
    "    num_data = len(input_data)\n",
    "    num_param = len(input_data[0])\n",
    "    num_test = len(test_data)\n",
    "    print('number of data: ', num_data)\n",
    "    print('number of parameters: ', num_param)\n",
    "    print('number of test: ', num_test)\n",
    "    \n",
    "    num_batches = int(num_data/batch_size)\n",
    "    test_batches = int(num_test/batch_size)\n",
    "    input_param = np.array(input_data)\n",
    "    input_vol = np.array(input_vol)\n",
    "    test_param = np.array(test_data)\n",
    "    test_vol = np.array(test_vol)\n",
    "    \n",
    "    #y = fl_linear(input_param)\n",
    "    y, cost_op, input_parameter, input_volume = mat_mul(input_param, batch_size, num_param)\n",
    "    \n",
    "    learning_rate = tf.Variable(0.5, trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(cost_op)\n",
    "    \n",
    "    sess = tf.Session() # Create TensorFlow session\n",
    "    print (\"Beginning Training\")\n",
    "    with sess.as_default():\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        sess.run(tf.assign(learning_rate, alpha))\n",
    "        for epoch in range(1, max_epochs):\n",
    "            print('epoch: ', epoch)\n",
    "            # Train one epoch\n",
    "            train_one_epoch(sess, cost_op, train_op, num_batches, input_parameter, input_param, input_volume, input_vol)\n",
    "            eval_one_epoch(sess, cost_op, test_batches, input_parameter, test_param, input_volume, test_vol)\n",
    "            \n",
    "        print('Done.')\n",
    "        W = tf.get_variable(\"W\")\n",
    "        print(\"Weight: \", W.eval())\n",
    "            \n",
    "\n",
    "def train_one_epoch(sess, cost_op, train_op, num_batches, input_parameter, input_param, input_volume, input_vol):\n",
    "     for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx+1) * batch_size\n",
    "        cost,_ = sess.run([cost_op, train_op], feed_dict={input_parameter: input_param[start_idx:end_idx], input_volume: input_vol[start_idx:end_idx]})\n",
    "        print(\"loss: \", cost)\n",
    "        \n",
    "def eval_one_epoch(sess, cost_op, test_batches, input_parameter, test_param, input_volume, test_vol):\n",
    "    for batch_idx in range(test_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx+1) * batch_size\n",
    "        cost = sess.run(cost_op, feed_dict={input_parameter: test_param[start_idx:end_idx], input_volume: test_vol[start_idx:end_idx]})\n",
    "        print(\"test loss: \", cost)\n",
    "\n",
    "# Parameters:\n",
    "# last_cost = 0\n",
    "# alpha = 0.4\n",
    "# batch_size = 10\n",
    "# max_epochs = 100\n",
    "# tolerance = 1e-3\n",
    "\n",
    "    \n",
    "# train_path = '/home/carnd/CYML/output/train/cylinder/tri_1_to_50_2.txt'\n",
    "# train_data, train_vol = load_input(train_path)\n",
    "# print ('sample data: ', train_data[0])\n",
    "# train(train_data, train_vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice_5:0\", shape=(3,), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 1 but is rank 2 for 'concat_5' (op: 'ConcatV2') with input shapes: [3], [1,3], [].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/home/carnd/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1566\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1567\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1568\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shape must be rank 1 but is rank 2 for 'concat_5' (op: 'ConcatV2') with input shapes: [3], [1,3], [].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-bf4441b23628>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_param\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1187\u001b[0m               tensor_shape.scalar())\n\u001b[1;32m   1188\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mconcat_v2\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0m_attr_N\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m--> 953\u001b[0;31m         \"ConcatV2\", values=values, axis=axis, name=name)\n\u001b[0m\u001b[1;32m    954\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3390\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3391\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3392\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3394\u001b[0m       \u001b[0;31m# Note: shapes are lazily computed with the C API enabled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1732\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1733\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1734\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1735\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1736\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1568\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1570\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape must be rank 1 but is rank 2 for 'concat_5' (op: 'ConcatV2') with input shapes: [3], [1,3], []."
     ]
    }
   ],
   "source": [
    "data = np.arange(6).reshape(2,3)\n",
    "param = np.array([1,1,1])\n",
    "input_data = tf.placeholder(name=\"input_data\", dtype=tf.float32, shape=(2, 3))\n",
    "input_param = tf.placeholder(name=\"input_param\", dtype=tf.float32, shape=(1,3))\n",
    "for i in range(2):\n",
    "    row = input_data[i,]\n",
    "    print(row)\n",
    "    tmp = tf.concat([row, input_param], 1)\n",
    "    try:\n",
    "        out = tf.stack([input_param, tmp])\n",
    "    except:\n",
    "        out = tmp\n",
    "        \n",
    "sess = tf.Session() # Create TensorFlow session\n",
    "saver = tf.train.Saver()\n",
    "with sess.as_default():\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    print(\"slice: \", sess.run(row, feed_dict = {input_data:data, input_param:param}))\n",
    "    print(\"stack result: \", sess.run(out, feed_dict = {input_data:data, input_param:param}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_12:0\", shape=(2, 3), dtype=int32)\n",
      "Tensor(\"Reshape_14:0\", shape=(1, 3), dtype=int32)\n",
      "[[1, 2, 3]]\n",
      "Tensor(\"concat_12:0\", shape=(1, 6), dtype=int32)\n",
      "[[1, 2, 3, 8, 9, 10]]\n",
      "Tensor(\"MatMul_4:0\", shape=(3, 3), dtype=int32)\n",
      "[[1 2 3]\n",
      " [2 4 6]\n",
      " [3 6 9]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "x = tf.constant([[1,2,3],[8,9,10]])\n",
    "print(x)\n",
    "#y = tf.transpose(x[0,:])\n",
    "x_0 = x[0,:]\n",
    "x_0 = tf.reshape(x_0,[1,-1])\n",
    "x_1 = x[1,:]\n",
    "x_1 = tf.reshape(x_1,[1,-1])\n",
    "print(x_0)\n",
    "print(x_0.eval(session=sess).tolist())\n",
    "z = tf.concat([x_0,x_1],1)\n",
    "print(z)\n",
    "print(z.eval(session=sess).tolist())\n",
    "mul = tf.matmul(tf.transpose(x_0), x_0)\n",
    "print(mul)\n",
    "print(mul.eval(session=sess))\n",
    "for i in range(3):\n",
    "    concat = tf.reshape(mul[i,i:],[1,-1])\n",
    "    mul_tmp = tf.reshape(mul[i,i:],[1,-1])\n",
    "    concat = tf.concat([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_3:  [[1 2 3]\n",
      " [2 4 6]\n",
      " [3 6 9]]\n",
      "param_new reshaped 2:  Tensor(\"Reshape_1826:0\", shape=(6,), dtype=int32)\n",
      "param list:  Tensor(\"Reshape_1809:0\", shape=(1, 3), dtype=int32)\n",
      "x_3:  [[ 1  2  3]\n",
      " [ 2  4  6]\n",
      " [ 3  6  9]\n",
      " [ 4  8 12]\n",
      " [ 6 12 18]\n",
      " [ 9 18 27]]\n",
      "added row:  Tensor(\"Reshape_1842:0\", shape=(3,), dtype=int32)\n",
      "param_new:  Tensor(\"Reshape_1841:0\", shape=(1, 6), dtype=int32)\n",
      "param_new:  [[ 1  2  3  4  6  9  4  8 12]]\n",
      "param_new reshaped 2:  Tensor(\"Reshape_1844:0\", shape=(9,), dtype=int32)\n",
      "param list:  Tensor(\"concat_765:0\", shape=(1, 9), dtype=int32)\n",
      "param_fin:\n",
      "  [[ 1  2  3  1  2  3  4  6  9  1  2  3  4  6  9  4  8 12]]\n",
      "x_3:  [[49 56 63]\n",
      " [56 64 72]\n",
      " [63 72 81]]\n",
      "param_new reshaped 2:  Tensor(\"Reshape_1865:0\", shape=(6,), dtype=int32)\n",
      "param list:  Tensor(\"Reshape_1848:0\", shape=(1, 3), dtype=int32)\n",
      "x_3:  [[343 392 441]\n",
      " [392 448 504]\n",
      " [441 504 567]\n",
      " [448 512 576]\n",
      " [504 576 648]\n",
      " [567 648 729]]\n",
      "added row:  Tensor(\"Reshape_1881:0\", shape=(3,), dtype=int32)\n",
      "param_new:  Tensor(\"Reshape_1880:0\", shape=(1, 6), dtype=int32)\n",
      "param_new:  [[343 392 441 448 504 567 448 512 576]]\n",
      "param_new reshaped 2:  Tensor(\"Reshape_1883:0\", shape=(9,), dtype=int32)\n",
      "param list:  Tensor(\"concat_781:0\", shape=(1, 9), dtype=int32)\n",
      "param_fin:\n",
      "  [[  1   2   3   1   2   3   4   6   9   1   2   3   4   6   9   4   8  12]\n",
      " [  7   8   9  49  56  63  64  72  81 343 392 441 448 504 567 448 512 576]]\n"
     ]
    }
   ],
   "source": [
    "sess= tf.Session()\n",
    "degree = 2\n",
    "x = tf.constant([[1,2,3],[7,8,9]])\n",
    "num_batches = 2\n",
    "num_param = 3\n",
    "param_fin = tf.constant([])\n",
    "\n",
    "for i in range(num_batches):\n",
    "    num_row = num_param\n",
    "    x_0 = x[i, :]\n",
    "    param_list = tf.reshape(x_0, [1, -1])\n",
    "#     print ('param_list', param_list.eval(session=sess))\n",
    "    x_1 = tf.reshape(x_0, [-1, 1])\n",
    "#     print ('x_1: ', x_1.eval(session=sess))\n",
    "    \n",
    "    x_2 = tf.reshape(x_1, [1, -1])\n",
    "    \n",
    "    for deg in range(1, degree+1):\n",
    "        x_3 = tf.matmul(x_1, x_2)\n",
    "        print ('x_3: ', x_3.eval(session=sess))\n",
    "        param_new = tf.constant([])\n",
    "#         param_new[num_row] = x_3[num_row, num_row\n",
    "        counter = 0\n",
    "        for j in range(num_param):\n",
    "            for k in range(num_param):\n",
    "                if k >= j:\n",
    "                    diag = x_3[j,k]\n",
    "                    diag = tf.reshape(diag, [-1])\n",
    "                    counter += 1\n",
    "                    try:\n",
    "                        param_new = tf.concat([param_new, diag[None, :]], 0)\n",
    "                    except:\n",
    "                        param_new = tf.reshape(diag, [-1, 1])\n",
    "#                     print('counter: ', counter)\n",
    "#                     print('param_new: ', param_new.eval(session=sess))\n",
    "                    param_new = tf.reshape(param_new, [counter, -1])\n",
    "        #             print ('concating: ', param_new.eval(session=sess))\n",
    "#         diag = x_3[num_row-1,num_param-1]\n",
    "        param_new = tf.reshape(param_new, [1, -1])\n",
    "#         print ('param_new: ', param_new.eval(session=sess))\n",
    "        for j in range(num_param, num_row):\n",
    "            diag = x_3[j,:]\n",
    "            diag = tf.reshape(diag, [-1])\n",
    "            print('added row: ', diag)\n",
    "            print('param_new: ', param_new)\n",
    "            param_new = tf.concat([param_new, diag[None, :]], 1)\n",
    "            print('param_new: ', param_new.eval(session=sess))\n",
    "            counter += num_param\n",
    "            param_new = tf.reshape(param_new, [counter, -1])\n",
    "#             for k in range(num_param):\n",
    "#                 diag = x_3[j,k]\n",
    "#                 diag = tf.reshape(diag, [-1])\n",
    "#                 print('diag')\n",
    "#                 counter += 1\n",
    "#                 param_new = tf.concat([param_new, diag[None, :]], 0)\n",
    "#                 param_new = tf.reshape(param_new, [counter, -1])\n",
    "#         diag = tf.reshape(diag, [-1])\n",
    "#         print('last: ', diag.eval(session=sess))\n",
    "#         param_new = tf.concat([param_new, diag[None, :]], 0)\n",
    "#         print ('param_new: ', param_new.eval(session=sess))\n",
    "        param_new = tf.reshape(param_new, [-1])\n",
    "        print ('param_new reshaped 2: ', param_new)\n",
    "        print ('param list: ', param_list)\n",
    "        param_list = tf.concat([param_list, param_new[None,:]], 1)\n",
    "        x_1 = tf.reshape(param_new, [-1, 1])\n",
    "#         print ('x_1: ', x_1.eval(session=sess))\n",
    "        num_row += 1\n",
    "        \n",
    "    try:\n",
    "        param_list = tf.reshape(param_list, [-1])\n",
    "        param_fin = tf.concat([param_fin, param_list[None, :]], 0)\n",
    "    except:\n",
    "        param_fin = tf.reshape(param_list, [1, -1])\n",
    "    print ('param_fin:\\n ', param_fin.eval(session=sess))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "[[  0.   1.   2.   0.   0.   0.   1.   2.   4.]\n",
      " [  3.   4.   5.   9.  12.  15.  16.  20.  25.]\n",
      " [  6.   7.   8.  36.  42.  48.  49.  56.  64.]\n",
      " [  9.  10.  11.  81.  90.  99. 100. 110. 121.]\n",
      " [ 12.  13.  14. 144. 156. 168. 169. 182. 196.]\n",
      " [ 15.  16.  17. 225. 240. 255. 256. 272. 289.]]\n",
      "[6 9]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "print('Done.')\n",
    "\n",
    "def expand_2(input_param_1, batch_size, num_param):\n",
    "    for row in range(batch_size):\n",
    "        param_tmp = input_param_1[row,:]\n",
    "        param_tmp = tf.reshape(param_tmp, [1, -1])\n",
    "        param_mat = tf.matmul(tf.transpose(param_tmp),param_tmp)\n",
    "\n",
    "        for i in range(num_param):\n",
    "            param_tmp2 = tf.reshape(param_mat[i,i:], [1,-1])\n",
    "            #print(param_tmp2.eval(session=sess))\n",
    "            param_tmp = tf.concat([param_tmp,param_tmp2], 1)\n",
    "        param_tmp = tf.reshape(param_tmp, [-1])\n",
    "\n",
    "        try:\n",
    "            input_parameter = tf.concat([input_parameter,param_tmp[None, :]],0)\n",
    "            input_parameter = tf.reshape(input_parameter, [row+1, -1])\n",
    "        except:\n",
    "            input_parameter = param_tmp\n",
    "            input_parameter = tf.reshape(input_parameter, [row+1, -1])\n",
    "    \n",
    "    return input_parameter\n",
    "\n",
    "sess = tf.Session()\n",
    "data = np.arange(18)\n",
    "data = data.reshape([6,-1])\n",
    "batch_size = 6\n",
    "num_param = 3\n",
    "input_param_1 = tf.constant(data, dtype=tf.float32)\n",
    "input_parameter = expand_2(input_param_1, batch_size, num_param)\n",
    "\n",
    "# for row in range(batch_size):\n",
    "#     param_tmp = input_param_1[row,:]\n",
    "#     param_tmp = tf.reshape(param_tmp, [1, -1])\n",
    "#     param_mat = tf.matmul(tf.transpose(param_tmp),param_tmp)\n",
    "\n",
    "#     for i in range(num_param):\n",
    "#         param_tmp2 = tf.reshape(param_mat[i,i:], [1,-1])\n",
    "#         #print(param_tmp2.eval(session=sess))\n",
    "#         param_tmp = tf.concat([param_tmp,param_tmp2], 1)\n",
    "#     param_tmp = tf.reshape(param_tmp, [-1])\n",
    "\n",
    "#     try:\n",
    "#         input_parameter = tf.concat([input_parameter,param_tmp[None, :]],0)\n",
    "#         input_parameter = tf.reshape(input_parameter, [row+1, -1])\n",
    "#     except:\n",
    "#         input_parameter = param_tmp\n",
    "#         input_parameter = tf.reshape(input_parameter, [row+1, -1])\n",
    "\n",
    "print(input_parameter.eval(session=sess))\n",
    "print(tf.shape(input_parameter).eval(session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_input(train_path):\n",
    "    train = open(train_path, 'r')\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    counter = 0\n",
    "    for line in train:\n",
    "        if counter > 10:\n",
    "            break\n",
    "        data = eval(line)\n",
    "        if data[1] > 0:\n",
    "            train_x.append(data[0])\n",
    "#             inv_vol = 1/data[1]\n",
    "            train_y.append(data[1])\n",
    "        counter += 1\n",
    "    print ('Number of data: ', len(train_x))\n",
    "    \n",
    "    return train_x, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  -35.0043392\n",
      "Number of data:  11\n",
      "diff 0.5022808532687777\n",
      "diff 0.5022808532542258\n",
      "diff 0.327472965164726\n",
      "diff 0.5022808533524512\n",
      "diff 0.32747296516472557\n",
      "diff 0.2494767435816346\n",
      "diff 0.23791506801962586\n",
      "diff 0.5022808532396739\n",
      "diff 0.3274729651647258\n",
      "diff 0.24947674356162572\n",
      "diff 0.23791506801962442\n"
     ]
    }
   ],
   "source": [
    "W = np.array([-0.87437505,\n",
    "             6.952491,\n",
    "              4.5137253,\n",
    "              0.74184924,\n",
    "              2.0514772 ,\n",
    "             -0.14970087,\n",
    "             -0.13637641,\n",
    "             -0.20277256,\n",
    "             -0.05430971])\n",
    "b = np.array([-35.561787,\n",
    "             -35.39475 ,\n",
    "             -35.3224  ,\n",
    "             -34.635918,\n",
    "             -34.72557 ,\n",
    "             -34.767803,\n",
    "             -34.995155,\n",
    "             -34.83515 ,\n",
    "             -34.908066,\n",
    "             -34.896793])\n",
    "W = np.array([-1.32813477e+03,\n",
    "              2.83764465e+02,\n",
    "              5.81152466e+02,\n",
    "              1.65513077e+02,\n",
    "              1.64283314e+01,\n",
    "             -1.20188210e+02,\n",
    "              1.55702343e+01,\n",
    "              1.09391075e+02,\n",
    "             -1.57943008e+02,\n",
    "             -4.78923941e+00,\n",
    "             -2.92627001e+00,\n",
    "              1.90355957e+00,\n",
    "             -5.39966488e+00,\n",
    "             -4.67496252e+00,\n",
    "              1.12794566e+00,\n",
    "              7.61268473e+00,\n",
    "              1.16817160e+01,\n",
    "              6.08279085e+00,\n",
    "             -1.99237764e+00])\n",
    "b = np.array([-811.7567 \n",
    "             -751.0992 ,\n",
    "             -827.5288 ,\n",
    "             -879.7452 ,\n",
    "             -963.4682 ,\n",
    "             -740.0352 ,\n",
    "             -635.4922 ,\n",
    "             -712.0758 ,\n",
    "             -713.21295,\n",
    "             -757.816  ])\n",
    "\n",
    "# Mean:\n",
    "b_mean = np.mean(b)\n",
    "print('mean: ', b_mean)\n",
    "\n",
    "# Test if this model actually works\n",
    "train_path = '/home/carnd/CYML/output/train/cylinder/lift_1_to_50.txt'\n",
    "train_data, train_vol = load_input(train_path)\n",
    "num_data = len(train_data)\n",
    "num_param = len(train_data[0])\n",
    "# transform the num_data\n",
    "\n",
    "for i in range(num_data):\n",
    "    data = np.array(train_data[i])\n",
    "    data = np.expand_dims(data, axis=1)\n",
    "#     print('original: ', data)\n",
    "    data2 = data.reshape([num_param]).tolist()\n",
    "#     print('reshaped: ',data2)\n",
    "    data = np.matmul(data, np.transpose(data))\n",
    "#     print('matrix: ', data)\n",
    "    for j in range(num_param):\n",
    "        # for each row of this matrix\n",
    "        data2 += data[j,j:].tolist()\n",
    "        #data2.append(data[j,j:].tolist())\n",
    "#     print('extended data: ', data2)\n",
    "    pred = np.dot(data2, W)\n",
    "    diff = abs(1/float(pred) - train_vol[i])\n",
    "    print('diff', diff)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data:  10\n",
      "train_data:  [[1, 0, 0], [1, 1, 0], [2, 0, 0], [1, 1, 1], [2, 1, 0], [3, 0, 0], [2, 1, 1], [2, 2, 0], [3, 1, 0], [4, 0, 0]]\n",
      "train_vol:  [1.6874999999416787, 2.3703703703703702, 3.3749999998833573, 2.8854381997703142, 2.4816806884647713, 5.06249999966964, 4.417092671887906, 4.740740740300995, 2.5292143561705105, 6.749999999352326]\n",
      "y:  [[-2033.21429352]\n",
      " [-1724.64925205]\n",
      " [-2898.33450839]\n",
      " [-1291.62351806]\n",
      " [-2589.2702816 ]\n",
      " [-3489.89944218]\n",
      " [-2055.25805308]\n",
      " [-2260.39522943]\n",
      " [-3191.13535983]\n",
      " [-3836.64453135]]\n",
      "train_vol:  [1.6875     2.37037037 3.375      2.8854382  2.48168069 5.0625\n",
      " 4.41709267 4.74074074 2.52921436 6.75      ]\n",
      "cost:  7048172.7598907305\n",
      "W:  [[-1.32813477e+03]\n",
      " [ 2.83764465e+02]\n",
      " [ 5.81152466e+02]\n",
      " [ 1.65513077e+02]\n",
      " [ 1.64283314e+01]\n",
      " [-1.20188210e+02]\n",
      " [ 1.55702343e+01]\n",
      " [ 1.09391075e+02]\n",
      " [-1.57943008e+02]\n",
      " [-4.78923941e+00]\n",
      " [-2.92627001e+00]\n",
      " [ 1.90355957e+00]\n",
      " [-5.39966488e+00]\n",
      " [-4.67496252e+00]\n",
      " [ 1.12794566e+00]\n",
      " [ 7.61268473e+00]\n",
      " [ 1.16817160e+01]\n",
      " [ 6.08279085e+00]\n",
      " [-1.99237764e+00]]\n",
      "diff:  [0.59308442]\n",
      "diff:  [0.42245483]\n",
      "diff:  [0.29664132]\n",
      "diff:  [0.34734203]\n",
      "diff:  [0.40333894]\n",
      "diff:  [0.19781741]\n",
      "diff:  [0.22687981]\n",
      "diff:  [0.2113799]\n",
      "diff:  [0.39569306]\n",
      "diff:  [0.14840879]\n"
     ]
    }
   ],
   "source": [
    "def load_input(train_path):\n",
    "    train = open(train_path, 'r')\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    counter = 0\n",
    "    for line in train:\n",
    "        if counter >= 10:\n",
    "            break\n",
    "        data = eval(line)\n",
    "        if data[1] > 0:\n",
    "            train_x.append(data[0])\n",
    "            inv_vol = 1/data[1]\n",
    "            train_y.append(inv_vol)\n",
    "        counter += 1\n",
    "    print ('Number of data: ', len(train_x))\n",
    "    \n",
    "    return train_x, train_y\n",
    "\n",
    "def expand_3(input_param_1, W, b_m, train_vol):\n",
    "    # TODO: make this more general\n",
    "    num_param = 3\n",
    "    input_param = 10\n",
    "    for row in range(batch_size):\n",
    "        param_tmp = input_param_1[row,:]\n",
    "        param_tmp = tf.reshape(param_tmp, [1, -1])\n",
    "        p1 = param_tmp[0,0]\n",
    "        p2 = param_tmp[0,1]\n",
    "        p3 = param_tmp[0,2]\n",
    "        \n",
    "        p = tf.pow(p1,2)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.pow(p2,2)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.pow(p3,2)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        \n",
    "        p = tf.multiply(p1, p2)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.multiply(p1, p3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.multiply(p2, p3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        \n",
    "        p = tf.pow(p1,3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.pow(p2,3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.pow(p3,3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        \n",
    "        p = tf.multiply(tf.pow(p1,2), p2)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.multiply(tf.pow(p1,2), p3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.multiply(p1, tf.pow(p2,2))\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.multiply(p1, tf.pow(p3,2))\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        \n",
    "        p = tf.multiply(tf.pow(p2,2), p3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.multiply(tf.pow(p3,2), p2)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        \n",
    "        p = tf.multiply(tf.multiply(p1, p2),p3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        param_tmp = tf.cast(param_tmp, tf.float64)\n",
    "        \n",
    "        try:\n",
    "            param_tmp = tf.reshape(param_tmp, [-1])\n",
    "            input_param = tf.concat([input_param, param_tmp[None,:]], 0)\n",
    "        except:\n",
    "            input_param = tf.reshape(param_tmp, [1, -1])\n",
    "    \n",
    "    W = tf.constant(W)\n",
    "    W = tf.reshape(W, [-1,1])\n",
    "    W = tf.cast(W, tf.float64)\n",
    "    b_m = tf.constant(b_m)\n",
    "    b_m = tf.cast(b_m, tf.float64)\n",
    "    y = tf.add(tf.matmul(input_param, W), b_m)\n",
    "    train_vol = tf.constant(train_vol, dtype=tf.float64)\n",
    "    train_vol = tf.cast(train_vol, tf.float64)\n",
    "    print('y: ', y.eval(session=sess))\n",
    "    print('train_vol: ', train_vol.eval(session=sess))\n",
    "#     cost = tf.reduce_mean(tf.pow(y-tf.multiply(train_vol, y),2))\n",
    "    cost = tf.reduce_mean(tf.pow(y-train_vol,2))\n",
    "    num_param = num_param + 6+ 10\n",
    "    return input_param, cost\n",
    "\n",
    "W = np.array([-1.32813477e+03,\n",
    "              2.83764465e+02,\n",
    "              5.81152466e+02,\n",
    "              1.65513077e+02,\n",
    "              1.64283314e+01,\n",
    "             -1.20188210e+02,\n",
    "              1.55702343e+01,\n",
    "              1.09391075e+02,\n",
    "             -1.57943008e+02,\n",
    "             -4.78923941e+00,\n",
    "             -2.92627001e+00,\n",
    "              1.90355957e+00,\n",
    "             -5.39966488e+00,\n",
    "             -4.67496252e+00,\n",
    "              1.12794566e+00,\n",
    "              7.61268473e+00,\n",
    "              1.16817160e+01,\n",
    "              6.08279085e+00,\n",
    "             -1.99237764e+00])\n",
    "b = np.array([-811.7567 \n",
    "             -751.0992 ,\n",
    "             -827.5288 ,\n",
    "             -879.7452 ,\n",
    "             -963.4682 ,\n",
    "             -740.0352 ,\n",
    "             -635.4922 ,\n",
    "             -712.0758 ,\n",
    "             -713.21295,\n",
    "             -757.816  ])\n",
    "\n",
    "sess = tf.Session()\n",
    "train_path = '/home/carnd/CYML/output/train/cylinder/lift_1_to_50.txt'\n",
    "train_data, train_vol = load_input(train_path)\n",
    "print('train_data: ', train_data)\n",
    "print('train_vol: ', train_vol)\n",
    "input_data = tf.constant(train_data)\n",
    "b = b.reshape([-1,1])\n",
    "b_mean = np.mean(b)\n",
    "input_param, cost = expand_3(input_data, W, b_mean, train_vol)\n",
    "print('cost: ', cost.eval(session=sess))\n",
    "input_data = input_param.eval(session=sess)\n",
    "# print('input_param: ', input_data)\n",
    "W = W.reshape([-1,1])\n",
    "print('W: ', W)\n",
    "pred_list = np.dot(input_data, W)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for row in range(pred_list.shape[0]):\n",
    "    pred = 1/(pred_list[row]+b_mean)\n",
    "    diff = abs(1/train_vol[row] - pred)\n",
    "    print ('diff: ', diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mat_mul(param, batch_size, num_param, deg):\n",
    "    # Hyper-parmeter: how many layers we will need\n",
    "    with tf.variable_scope(\"mat_mul\", reuse=True):\n",
    "        input_param_1 = tf.placeholder(name=\"input_param_1\", dtype=tf.float32, shape=(batch_size, num_param))\n",
    "        input_volume = tf.placeholder(name=\"input_volume\", dtype=tf.float32, shape=(batch_size))\n",
    "        \n",
    "        for i in range(deg):\n",
    "            deg_n1 = tf.matmul(deg_n0, tf.transpose(deg_1))\n",
    "            deg_n2 = tf.concat([deg_n1, deg_1])\n",
    "        \n",
    "        \n",
    "        \n",
    "        input_parameter = expand_2(input_param_1, batch_size, num_param)\n",
    "        num_param = int(num_param + num_param*(num_param+1)/2)\n",
    "\n",
    "        W1 = tf.Variable(tf.random_normal([num_param, 1], dtype=tf.float32), name=\"W\")\n",
    "        b1 = tf.Variable(tf.zeros([batch_size, 1], dtype=tf.float32), name=\"b\")\n",
    "        y = tf.add(tf.matmul(input_parameter, W1), b1)\n",
    "        \n",
    "        cost_op = tf.reduce_mean(tf.pow(y-input_volume, 2))\n",
    "    \n",
    "    return y, cost_op, input_param_1, input_volume, input_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param_tmp: [[ 1  2  3  1  4  9  2  3  6  1  8 27  2  3  4  9 12 18  6]]\n",
      "init\n",
      "input_param: [[ 1  2  3  1  4  9  2  3  6  1  8 27  2  3  4  9 12 18  6]]\n",
      "param_tmp: [[   2    3   10    4    9  100    6   20   30    8   27 1000   12   40\n",
      "    18  200   90  300   60]]\n",
      "concat\n",
      "input_param: [[   1    2    3    1    4    9    2    3    6    1    8   27    2    3\n",
      "     4    9   12   18    6]\n",
      " [   2    3   10    4    9  100    6   20   30    8   27 1000   12   40\n",
      "    18  200   90  300   60]]\n",
      "num_param:  19\n"
     ]
    }
   ],
   "source": [
    "def expand_3(input_param_1, batch_size, num_param):\n",
    "    # TODO: make this more general\n",
    "    num_param = 3\n",
    "    input_param = 10\n",
    "    for row in range(batch_size):\n",
    "        param_tmp = input_param_1[row,:]\n",
    "        param_tmp = tf.reshape(param_tmp, [1, -1])\n",
    "        p1 = param_tmp[0,0]\n",
    "        p2 = param_tmp[0,1]\n",
    "        p3 = param_tmp[0,2]\n",
    "        \n",
    "        p = tf.pow(p1,2)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.pow(p2,2)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.pow(p3,2)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        \n",
    "        p = tf.multiply(p1, p2)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.multiply(p1, p3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.multiply(p2, p3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        \n",
    "        p = tf.pow(p1,3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.pow(p2,3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.pow(p3,3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        \n",
    "        p = tf.multiply(tf.pow(p1,2), p2)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.multiply(tf.pow(p1,2), p3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.multiply(p1, tf.pow(p2,2))\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.multiply(p1, tf.pow(p3,2))\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        \n",
    "        p = tf.multiply(tf.pow(p2,2), p3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        p = tf.multiply(tf.pow(p3,2), p2)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        \n",
    "        p = tf.multiply(tf.multiply(p1, p2),p3)\n",
    "        p = tf.reshape(p, [-1])\n",
    "        param_tmp = tf.concat([param_tmp, p[None,:]], 1)\n",
    "        \n",
    "        print('param_tmp:', param_tmp.eval(session=sess))\n",
    "        \n",
    "        try:\n",
    "            param_tmp = tf.reshape(param_tmp, [-1])\n",
    "            input_param = tf.concat([input_param, param_tmp[None,:]], 0)\n",
    "            print('concat')\n",
    "        except:\n",
    "            print('init')\n",
    "            input_param = tf.reshape(param_tmp, [1, -1])\n",
    "        print('input_param:',input_param.eval(session=sess))\n",
    "    \n",
    "    num_param = num_param + 6+ 10\n",
    "    return input_param, num_param\n",
    "\n",
    "sess = tf.Session()\n",
    "input_param = tf.constant([[1,2,3],[2,3,10]])\n",
    "input_param, num_param = expand_3(input_param, 2, 3)\n",
    "print ('num_param: ', num_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data:  50\n",
      "sample data:  [1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "number of train:  40\n",
      "number of test:  10\n",
      "number of parameters:  19\n",
      "number of test:  10\n",
      "batch_size:  5\n",
      "number of batches:  8\n",
      "Beginning Training\n",
      "epoch:  1\n",
      "batches:  8\n",
      "loss:  227.53397\n",
      "layer_1:  [[ -0.08400881   0.7285836   -1.4688802    1.7337687    2.4287684\n",
      "   -1.1019212    0.17918992   0.6151695    1.6486315    0.39970595]\n",
      " [  0.9857779    0.8922627   -5.389453     5.3263       2.1905851\n",
      "    1.7714543   -1.3625824    1.7792897    2.3743463    2.0419292 ]\n",
      " [  2.3964784    5.4760523  -13.864322     9.682614    14.221719\n",
      "  -10.777562    -2.7799118    2.513164     7.819329    -2.080552  ]\n",
      " [ -0.8367122    1.412885    -7.1304455   -1.1579419    4.2312465\n",
      "    1.4391825    4.9699774   -2.546794    -0.02583212  -5.5218344 ]\n",
      " [  5.750342     4.0370774  -21.900763    13.528143    12.8580065\n",
      "   -3.6056557   -5.769534     7.086888    10.055863     0.91088074]]\n",
      "layer_2:  [[  1.4889588    4.190979    -6.6100264   -1.9708054   -2.6953015 ]\n",
      " [  3.2187376   11.191401   -16.80953     -3.8479648   -4.125348  ]\n",
      " [ 18.621803    30.832762   -20.122482    -0.8718753  -16.675648  ]\n",
      " [ -1.9660788   -7.9420757    3.1094017   21.599293    11.567712  ]\n",
      " [ 26.096478    40.77195    -30.200724     0.63788795  -6.618007  ]]\n",
      "layer_3:  [[  9.431378]\n",
      " [ 20.244501]\n",
      " [ 11.720919]\n",
      " [-22.82349 ]\n",
      " [ -4.360613]]\n",
      "loss:  596504.8\n",
      "layer_1:  [[  26.838703      5.408552    -62.756546     15.050286     25.797857\n",
      "   -52.30441     -28.477308     22.461575     37.767815    -26.72544   ]\n",
      " [  -2.2629747    14.900139    -22.150557     11.92269      23.531784\n",
      "     2.0815985    11.347521    -14.530227      0.48973846   -5.826517  ]\n",
      " [  14.688158     15.935755    -35.64872      39.171078     22.67702\n",
      "    15.665224     -5.914523     -0.59729266   -5.211485     14.034927  ]\n",
      " [  27.892574      9.466895    -69.68957      24.919052     28.852705\n",
      "   -31.495497    -26.31674      25.15517      35.617947    -14.303459  ]\n",
      " [  63.839905     20.492228   -144.91577      37.84054      56.37619\n",
      "  -120.15994     -67.713005     48.427994     81.94981     -64.01962   ]]\n",
      "layer_2:  [[ 212.65689    123.70857    129.69493     76.39549     45.65593  ]\n",
      " [ -47.491596     4.002083   -85.61577     -2.9437847  -47.90842  ]\n",
      " [ -17.273834    91.19383   -135.05905     -9.113106   -67.234344 ]\n",
      " [ 180.00835    139.44295     57.109734    60.188843    40.937607 ]\n",
      " [ 491.95642    293.00235    301.494      173.92743     94.78086  ]]\n",
      "layer_3:  [[ -644.0093 ]\n",
      " [  207.50157]\n",
      " [  147.21953]\n",
      " [ -527.1056 ]\n",
      " [-1484.4053 ]]\n",
      "loss:  208203.55\n",
      "layer_1:  [[  -0.6410947   26.322102   -24.08135     44.556755    41.705887\n",
      "    17.045815    20.648201   -23.094906   -17.265299     8.764027 ]\n",
      " [   6.84965     25.305763   -79.48566     40.170647    56.221146\n",
      "   -10.283997     9.814006   -28.961908     9.267942   -13.592281 ]\n",
      " [  16.450157     6.5084195 -100.60174     80.28704     62.92749\n",
      "    16.978771    -5.5430565   -2.1997728   -4.4484386   28.072994 ]\n",
      " [  36.60779     -4.9030757 -186.50616     81.40795     90.83889\n",
      "   -55.212475   -32.661293    22.443077    48.15419    -12.257507 ]\n",
      " [  79.065216     3.3513145 -324.46536    122.96485    150.10718\n",
      "  -183.88655    -86.35167     43.449627   105.88836    -79.52901  ]]\n",
      "layer_2:  [[-140.312       13.487141  -250.77463    -13.5870285 -158.66359  ]\n",
      " [ -99.286026     8.751995  -227.52075     83.79822   -181.67444  ]\n",
      " [-143.50299     81.17932   -379.93005     93.13794   -225.85959  ]\n",
      " [  -2.280449    65.49305   -290.76996    321.44562   -248.23538  ]\n",
      " [ 182.50533     97.18909   -241.33682    687.5707    -440.26562  ]]\n",
      "layer_3:  [[464.29507]\n",
      " [437.37457]\n",
      " [593.5766 ]\n",
      " [424.82605]\n",
      " [353.63626]]\n",
      "loss:  58426.477\n",
      "layer_1:  [[  14.334652    82.50759     26.245518    11.689468    41.212627\n",
      "    -6.7329245   14.359298   -22.90182    -13.424528   -44.136765 ]\n",
      " [  24.458328    50.35742    -58.82836     55.194206    59.15655\n",
      "    -3.0194368    6.21984    -19.09311      5.834995    -4.9659214]\n",
      " [  61.0292      55.29724    -98.522766   118.20182     65.08208\n",
      "    48.535896   -21.839096    -9.314768   -32.838894    45.324738 ]\n",
      " [  40.61954     44.298077  -158.23718     65.06031     91.270744\n",
      "   -47.809177   -11.123252   -38.309395    41.67648    -37.30001  ]\n",
      " [  48.63981      2.7425241 -180.54369    109.69046     98.65637\n",
      "    -9.192145   -29.40216     13.490846    27.481909    27.433868 ]]\n",
      "layer_2:  [[ -51.740665   -9.757656  -63.376762  -62.451767  -87.775566]\n",
      " [ -56.76721    63.01656  -251.91888   -11.823746 -195.99304 ]\n",
      " [-108.85086   227.13394  -459.93356   -19.900757 -302.7353  ]\n",
      " [ -33.9589     43.49874  -295.83392    82.479164 -322.15356 ]\n",
      " [ -77.126785  136.22736  -471.0956    119.85414  -320.0728  ]]\n",
      "layer_3:  [[103.06269]\n",
      " [188.49773]\n",
      " [292.22302]\n",
      " [253.01407]\n",
      " [329.57523]]\n",
      "loss:  46035.105\n",
      "layer_1:  [[ 7.54310837e+01 -3.76756172e+01 -2.79132996e+02  1.12592987e+02\n",
      "   1.39173965e+02 -9.82265930e+01 -5.63689194e+01  1.62017365e+01\n",
      "   9.71790619e+01  4.69483757e+00]\n",
      " [ 1.02420532e+02 -6.81797256e+01 -4.69672241e+02  1.88983917e+02\n",
      "   2.45484451e+02 -2.42949997e+02 -9.76167755e+01  3.01473355e+00\n",
      "   1.49085876e+02 -5.41956406e+01]\n",
      " [ 6.85182266e+01  1.51108597e+02  2.69242821e+01 -9.15508270e-02\n",
      "   3.64525757e+01 -5.08091164e+01 -1.54380035e+01 -2.17293816e+01\n",
      "   3.50287399e+01 -6.45673599e+01]\n",
      " [ 1.01742905e+02  1.36644623e+02 -6.29654312e+00  6.71439438e+01\n",
      "   2.91986599e+01 -2.18832016e+01 -3.80467377e+01  7.34445286e+00\n",
      "   1.89202480e+01  4.61240470e-01]\n",
      " [ 8.16344223e+01  8.73752289e+01 -1.07039238e+02  6.55848083e+01\n",
      "   7.17671432e+01 -4.76413727e+01 -2.94633198e+01 -1.44728928e+01\n",
      "   5.91122780e+01 -9.84162998e+00]]\n",
      "layer_2:  [[  58.92369   107.56686  -477.41907    52.26654  -473.10406 ]\n",
      " [  52.03253    36.633514 -721.79297   143.27716  -928.6001  ]\n",
      " [ 174.8677     83.90009    44.454    -197.33398  -141.39261 ]\n",
      " [ 231.37918   252.96968  -115.1479   -193.34595  -224.15613 ]\n",
      " [ 156.22566   182.47711  -235.8859   -147.80057  -298.43747 ]]\n",
      "layer_3:  [[ -94.96447]\n",
      " [ -54.66304]\n",
      " [-219.05637]\n",
      " [-324.91095]\n",
      " [-231.20154]]\n",
      "loss:  159853.16\n",
      "layer_1:  [[ 137.03467     53.585136  -108.93172     89.705154    48.812992\n",
      "     4.058056   -86.90452     47.89067     37.42742     49.686127 ]\n",
      " [ 103.36037     37.83497   -227.1007      57.542606   115.29155\n",
      "  -104.116684   -54.195225   -43.651928   105.65558    -56.885414 ]\n",
      " [ 113.745895   -32.3641    -244.4369     103.01752    118.95269\n",
      "   -53.32972    -75.77778     37.6904      96.787186    41.97343  ]\n",
      " [ 122.40973    -96.474525  -437.69266    152.79497    220.90237\n",
      "  -160.4238     -89.17603     13.470481   156.15128      6.2721214]\n",
      " [ 142.86993   -150.06303   -693.8141     265.4761     376.50635\n",
      "  -340.57672   -128.73239    -28.811874   211.80313    -54.937233 ]]\n",
      "layer_2:  [[  203.64525    319.62323   -250.98923   -158.19135   -246.16904 ]\n",
      " [  125.94589     56.833717  -268.7233    -193.54922   -466.75192 ]\n",
      " [  125.479034   207.11653   -447.23608   -104.348694  -394.64456 ]\n",
      " [   63.24707    101.4203    -716.0104     -88.170906  -755.4329  ]\n",
      " [  -30.85457     14.223984 -1144.322     -134.2252   -1413.7949  ]]\n",
      "layer_3:  [[-323.55804]\n",
      " [-270.3002 ]\n",
      " [-334.66876]\n",
      " [-409.9758 ]\n",
      " [-562.3971 ]]\n",
      "loss:  2296.255\n",
      "layer_1:  [[ 150.54219    165.95424    130.82121   -108.96993    -21.181194\n",
      "   -87.129     -108.58168    118.67555     59.90895   -173.16759  ]\n",
      " [ 122.458626   129.03546     13.277958  -105.52855     31.673786\n",
      "   -78.882355   -79.544365    45.44578     78.92077   -170.40627  ]\n",
      " [ 155.80414     85.13338    -15.104425   -33.81146     19.024385\n",
      "   -40.61419   -110.886314   105.09941     64.268265   -80.99516  ]\n",
      " [ 253.98456    107.23374    -58.09528     37.93705     -1.3930702\n",
      "    43.81259   -186.39392    140.8704       5.555084   -11.085662 ]\n",
      " [ 129.72557     15.910092  -171.5745     -21.143776    94.43443\n",
      "   -67.02829    -89.960236    46.304832   102.57905    -91.94014  ]]\n",
      "layer_2:  [[ 203.5155    -371.96603    478.6606     167.8703       8.7348175]\n",
      " [  66.560875  -348.0885     316.62924     58.103226   -55.206078 ]\n",
      " [ 140.36221   -150.92746    180.9376      96.78677    -66.661644 ]\n",
      " [ 214.96378    169.77759     85.53273     37.12509   -103.02345  ]\n",
      " [ -27.951176  -216.71486     -4.879196    50.390152  -199.24785  ]]\n",
      "layer_3:  [[  1.6052687]\n",
      " [ 35.392982 ]\n",
      " [-40.324436 ]\n",
      " [-84.33986  ]\n",
      " [ -5.030918 ]]\n",
      "loss:  23868.52\n",
      "layer_1:  [[ 1.64217453e+02 -9.01342926e+01 -1.69406891e+02 -3.41489868e+01\n",
      "   7.64298477e+01  3.22993965e+01 -1.65760071e+02  1.63389893e+02\n",
      "   5.68313599e+01 -4.60780830e+01]\n",
      " [ 1.11565285e+02 -9.98418121e+01 -3.60851440e+02 -5.65144958e+01\n",
      "   1.96046326e+02 -1.01336914e+02 -1.05464363e+02 -4.65791702e-01\n",
      "   1.11962303e+02 -1.90904343e+02]\n",
      " [ 1.19044975e+02 -2.14006226e+02 -3.71673218e+02 -1.41996193e+01\n",
      "   1.97323334e+02 -2.61735592e+01 -1.31868118e+02  1.19238075e+02\n",
      "   1.04572792e+02 -5.99510765e+01]\n",
      " [ 9.55568848e+01 -3.28991241e+02 -6.38896729e+02  3.98228989e+01\n",
      "   3.64038025e+02 -1.38396118e+02 -1.22368942e+02  5.86332169e+01\n",
      "   1.42388947e+02 -1.12140617e+02]\n",
      " [ 7.16534805e+01 -4.46967285e+02 -9.77224060e+02  1.55529083e+02\n",
      "   6.00312317e+02 -3.26096100e+02 -1.36120636e+02 -2.54461765e+01\n",
      "   1.64636917e+02 -1.98328354e+02]]\n",
      "layer_2:  [[ -250.40863    -278.65283      52.384983    340.9652       41.09572  ]\n",
      " [ -555.5158     -652.3444       -4.6867676   204.37044    -304.08643  ]\n",
      " [ -565.0209     -473.56873    -161.25757     377.85617    -110.606186 ]\n",
      " [ -993.90845    -743.9716     -422.31415     390.05084    -472.21844  ]\n",
      " [-1596.0281    -1071.966      -805.80914     339.99725   -1115.3318   ]]\n",
      "layer_3:  [[-50.023857  ]\n",
      " [ 20.782032  ]\n",
      " [-44.539764  ]\n",
      " [ -0.12643278]\n",
      " [ 94.73209   ]]\n",
      "test loss:  133815.6\n",
      "test loss:  52519.285\n",
      "epoch:  2\n",
      "batches:  8\n",
      "loss:  312.48486\n",
      "layer_1:  [[ -1.2920032   -4.4498386   -1.6886909   -0.8446038    2.3104734\n",
      "    0.94161713   1.1586946   -0.28271362   0.6298951    0.97111756]\n",
      " [  1.889686    -1.9523205    3.7057095   -3.9637716   -1.316797\n",
      "    2.7520366   -5.0771594    6.1021295    3.5107691   -1.3661492 ]\n",
      " [ -3.712479   -19.082836   -15.261812    -2.507225    14.054001\n",
      "   -0.8677039    1.7451477   -1.281465     2.543408    -0.16474116]\n",
      " [  7.7359104    9.438585    19.233398   -24.553265    -8.469585\n",
      "   -5.8434057  -10.68874     17.438469     8.730007   -20.897966  ]\n",
      " [  1.7880086  -18.709808    -7.5140476   -9.778524     7.836534\n",
      "    6.6920047   -8.2934265   11.042784     7.0531454   -2.5863488 ]]\n",
      "layer_2:  [[ -6.0791707  -0.5739734  -2.2586262   1.9996755   4.564469 ]\n",
      " [ -1.6339378  -7.0558395   7.321013    7.7532244  12.165577 ]\n",
      " [-43.91073   -17.574675  -10.664562   10.603374    3.7287326]\n",
      " [ -0.6184883 -80.28677    53.340866   51.938328   19.745079 ]\n",
      " [-38.892853  -28.05164     5.9220304  26.877575   29.498133 ]]\n",
      "layer_3:  [[ -0.9122577]\n",
      " [ -6.8385196]\n",
      " [ -5.9598675]\n",
      " [-31.216291 ]\n",
      " [-13.157389 ]]\n",
      "loss:  5060.838\n",
      "layer_1:  [[  -9.841546    -53.4737      -49.747242     -9.4981365    42.807198\n",
      "    -4.269929      0.38661003   -3.8063655     3.3989947    -8.697372  ]\n",
      " [  10.848124     -0.44954967   17.57226     -48.18195      -2.9767866\n",
      "    -4.518939    -19.597677     24.121338     14.46754     -39.661903  ]\n",
      " [  27.978943     -9.534848     11.90306     -30.57034      -6.7909946\n",
      "    14.195267    -43.157154     37.919388      9.132952    -21.385159  ]\n",
      " [   0.8828327   -57.755386    -37.25619     -28.192326     30.56887\n",
      "    12.484598    -18.246765     19.099754     11.161425    -13.660624  ]\n",
      " [ -15.956541   -106.9972     -117.50263     -15.319915     93.64298\n",
      "   -16.053556     -5.726385     -7.4100776     7.0499763   -26.536009  ]]\n",
      "layer_2:  [[-160.30058   -72.23518   -11.578667   29.44529     8.034684]\n",
      " [ -58.35958  -157.33167    77.40425    87.61763    33.109173]\n",
      " [ -54.93405   -83.9789     71.38788    62.482296   40.45947 ]\n",
      " [-159.9354   -100.34755    22.555084   67.37958    60.670635]\n",
      " [-362.23795  -178.81375   -24.443518   58.150227  -12.591293]]\n",
      "layer_3:  [[-36.347084]\n",
      " [-88.22978 ]\n",
      " [-51.844986]\n",
      " [-68.829124]\n",
      " [-75.11509 ]]\n",
      "loss:  44186.547\n",
      "layer_1:  [[  40.96248      6.539299    55.96589    -89.627335   -23.427479\n",
      "   -11.462221   -64.83276     73.00513     25.136765   -79.55157  ]\n",
      " [  15.660355   -28.742746    -2.133759   -88.68486     13.795996\n",
      "    -0.6127014  -38.910355    31.483181    20.379976   -75.39548  ]\n",
      " [  31.04882    -58.11891     -7.2616787  -66.46313     10.312847\n",
      "    30.521141   -69.52214     59.839474    12.80465    -39.66848  ]\n",
      " [  -4.158412  -128.672      -94.540436   -63.235073    75.28621\n",
      "    22.12298    -36.566147    28.102133    12.698405   -39.270172 ]\n",
      " [ -35.64973   -206.11536   -221.87158    -42.575058   179.05254\n",
      "   -21.301517   -16.735085   -15.608451     0.228037   -64.91659  ]]\n",
      "layer_2:  [[-115.52913  -337.4839    164.53694   178.18555    13.815495]\n",
      " [-209.09737  -299.5623    126.27345   131.55914    49.576748]\n",
      " [-218.21088  -187.20833   115.56618   112.38883    93.35215 ]\n",
      " [-417.81702  -235.02892    70.20188   109.24452   110.059746]\n",
      " [-775.10284  -364.38422    21.546572   75.7657     10.988905]]\n",
      "layer_3:  [[-214.59482]\n",
      " [-202.4468 ]\n",
      " [-159.29695]\n",
      " [-202.2873 ]\n",
      " [-238.26257]]\n",
      "loss:  66328.33\n",
      "layer_1:  [[  68.184296    41.144897   130.05109   -190.33647    -46.82678\n",
      "   -46.108032  -122.4529     127.51559     40.690098  -178.43217  ]\n",
      " [  49.84087    -28.140087    52.591003  -151.18076    -11.656353\n",
      "     0.6042166 -107.05809     90.47829     32.70257   -119.72595  ]\n",
      " [ 103.02097    -37.223515    46.434483  -135.43658    -30.173279\n",
      "    50.03923   -177.93791    118.65662     10.631972   -95.28955  ]\n",
      " [  19.594566   -82.61989    -44.614307  -149.96545     50.537903\n",
      "     8.79394    -77.198586    29.51337     23.899466  -128.54639  ]\n",
      " [  32.442154  -138.19719    -47.69039   -124.72101     47.599136\n",
      "    56.34275   -114.930626    76.86963     14.880154   -68.63851  ]]\n",
      "layer_2:  [[-273.91882  -717.359     285.62976   270.45468   -83.775314]\n",
      " [-315.60605  -468.50348   239.31493   156.69179    15.269348]\n",
      " [-336.77515  -323.36636   290.08313    83.27136    37.179718]\n",
      " [-500.3164   -447.37933   224.6265     80.76778    43.866096]\n",
      " [-518.9439   -272.69693   214.089      60.70883   135.99368 ]]\n",
      "layer_3:  [[-367.83182]\n",
      " [-258.79172]\n",
      " [-157.36797]\n",
      " [-235.28806]\n",
      " [-190.85081]]\n",
      "loss:  5665.455\n",
      "layer_1:  [[-2.42652779e+01 -2.50160324e+02 -1.65798065e+02 -1.34395706e+02\n",
      "   1.57257614e+02  5.40405693e+01 -8.91328659e+01  7.43865538e+00\n",
      "   2.03052521e-01 -7.50298767e+01]\n",
      " [-8.43346863e+01 -3.72224976e+02 -3.46157501e+02 -1.09386932e+02\n",
      "   3.19088440e+02  2.65780449e+00 -5.50944252e+01 -6.37607803e+01\n",
      "  -3.29788589e+01 -1.16396530e+02]\n",
      " [ 9.13654861e+01  3.05373840e+01  1.55508896e+02 -2.72518127e+02\n",
      "  -3.76232033e+01 -3.99921227e+01 -1.97095459e+02  1.23570755e+02\n",
      "   5.69568481e+01 -2.30924850e+02]\n",
      " [ 1.30997314e+02 -1.21502233e+00  1.44797806e+02 -2.38958588e+02\n",
      "  -5.82584343e+01 -2.43973923e+00 -2.51985397e+02  1.53814072e+02\n",
      "   4.72959023e+01 -1.85215759e+02]\n",
      " [ 6.25512123e+01 -8.58496857e+01  3.58733292e+01 -2.32901245e+02\n",
      "   2.02637100e+01  2.30532703e+01 -1.84232239e+02  8.47076721e+01\n",
      "   4.04438400e+01 -1.65637665e+02]]\n",
      "layer_2:  [[ -905.3398    -148.23547    276.46054   -196.75978     92.21797 ]\n",
      " [-1489.2927    -277.04456    286.92313   -317.42993     -5.949504]\n",
      " [ -491.05972   -709.1905     418.82233     17.383392  -237.40823 ]\n",
      " [ -477.86765   -535.45703    471.08823    -54.32025   -222.49011 ]\n",
      " [ -637.57294   -432.34464    407.32425   -104.11459    -76.80014 ]]\n",
      "layer_3:  [[ 68.28239  ]\n",
      " [126.32609  ]\n",
      " [ -4.7824864]\n",
      " [ 89.97697  ]\n",
      " [ 57.95276  ]]\n",
      "loss:  101083.71\n",
      "layer_1:  [[ 1.14784721e+02 -1.23310402e+02  5.39011421e+01 -2.29795273e+02\n",
      "   1.71503067e-01  1.01035713e+02 -3.01213654e+02  1.08710258e+02\n",
      "   1.33102484e+01 -1.11423737e+02]\n",
      " [ 1.55874500e+01 -1.75088303e+02 -9.13571243e+01 -2.47574020e+02\n",
      "   1.21922653e+02  4.13852119e+01 -1.68172974e+02 -1.88640823e+01\n",
      "   1.69829540e+01 -1.86960892e+02]\n",
      " [ 2.40959473e+01 -2.64885132e+02 -9.04594727e+01 -2.26289200e+02\n",
      "   1.18608879e+02  1.11292618e+02 -2.15462769e+02  5.23450737e+01\n",
      "   6.52397919e+00 -9.91748657e+01]\n",
      " [-5.38867226e+01 -4.14021332e+02 -2.81207214e+02 -2.23262512e+02\n",
      "   2.78189911e+02  9.34638977e+01 -1.52089844e+02 -2.08944626e+01\n",
      "  -1.95747719e+01 -1.27529152e+02]\n",
      " [-1.44970123e+02 -5.84694214e+02 -5.26518188e+02 -1.91223404e+02\n",
      "   5.05843842e+02  2.85873642e+01 -1.02668442e+02 -1.19915184e+02\n",
      "  -7.42326813e+01 -1.90229706e+02]]\n",
      "layer_2:  [[ -719.9675       6.616028   518.375     -456.59613   -116.03725 ]\n",
      " [-1003.37976   -255.41351    475.1251    -405.08228    -94.320114]\n",
      " [-1035.5095      19.83075    489.81662   -448.41632     57.109604]\n",
      " [-1581.7474     -70.18324    522.13367   -513.4031     111.97381 ]\n",
      " [-2419.375     -243.70071    567.6794    -663.9389      27.497871]]\n",
      "layer_3:  [[391.51566]\n",
      " [291.83408]\n",
      " [288.67847]\n",
      " [277.9832 ]\n",
      " [356.54324]]\n",
      "loss:  49685.38\n",
      "layer_1:  [[ 180.01593     84.02284    306.45227   -404.7259     -98.523705\n",
      "   -41.81873   -385.13425    193.9759      88.33139   -305.93042  ]\n",
      " [ 111.83298     15.96727    182.05905   -406.4714     -19.930271\n",
      "     6.6209984 -306.305      100.96675     68.566246  -293.78506  ]\n",
      " [ 141.59305    -44.103294   171.31252   -374.85806    -42.92195\n",
      "    67.293015  -365.4553     157.28564     51.78718   -223.89366  ]\n",
      " [ 247.99051    -41.738045   172.67303   -382.88226    -89.66134\n",
      "   173.91289   -512.2113     200.65059      2.3916912 -192.63315  ]\n",
      " [  55.25107   -163.43889      8.498873  -378.43604     71.06791\n",
      "   101.31421   -277.274       70.03489     31.67048   -223.99495  ]]\n",
      "layer_2:  [[ -736.9267     -669.4259      574.5609     -441.56912    -623.3384   ]\n",
      " [ -847.7067     -555.0933      495.22415    -402.50967    -345.29962  ]\n",
      " [ -853.34406    -320.47748     540.1307     -453.80048    -253.45677  ]\n",
      " [ -915.5196       -3.5754395   680.6574     -676.5387     -219.84174  ]\n",
      " [-1134.7971     -298.88043     486.4342     -386.4725       28.167374 ]]\n",
      "layer_3:  [[356.03604]\n",
      " [169.42819]\n",
      " [170.19609]\n",
      " [269.12875]\n",
      " [-56.12931]]\n",
      "loss:  2258931.8\n",
      "layer_1:  [[  72.95694  -208.37749    11.739849 -404.4259     43.137733  259.97073\n",
      "  -342.72366   164.4719    -15.947983 -144.1417  ]\n",
      " [ -46.537792 -273.5607   -199.55122  -422.97952   215.87585   171.6444\n",
      "  -185.47801   -19.239502  -28.765057 -269.86343 ]\n",
      " [ -48.093964 -397.44876  -196.02304  -410.75305   209.95816   274.66876\n",
      "  -226.0688     92.734695  -42.629955 -149.60236 ]\n",
      " [-169.17857  -598.58624  -460.23477  -416.33426   431.33615   261.47986\n",
      "  -124.68973     2.432979  -99.49513  -199.67113 ]\n",
      " [-316.55246  -826.8433   -789.52234  -391.32538   734.6188    201.06151\n",
      "   -29.350761 -115.41975  -196.46344  -288.24106 ]]\n",
      "layer_2:  [[-1201.0973     -14.920349   248.64815   -200.57236    583.3694  ]\n",
      " [-1654.9634    -449.17752    226.19241    -62.647583   627.78265 ]\n",
      " [-1676.0857    -125.718445   193.30501    -10.796753   925.535   ]\n",
      " [-2444.4844    -371.88065    164.21674    156.45337   1200.714   ]\n",
      " [-3564.6274    -740.447      122.00903    296.74582   1389.8606  ]]\n",
      "layer_3:  [[ -740.4655 ]\n",
      " [-1006.29987]\n",
      " [-1133.1388 ]\n",
      " [-1610.6132 ]\n",
      " [-2195.1477 ]]\n",
      "test loss:  15502.763\n",
      "test loss:  129646.17\n",
      "epoch:  3\n",
      "batches:  8\n",
      "loss:  175.13783\n",
      "layer_1:  [[-2.3807323e+00 -5.3207817e+00 -3.2817125e-02 -2.4898844e+00\n",
      "   2.2367573e+00  2.5381658e+00  1.9437184e+00 -1.6018016e+00\n",
      "  -6.4029574e-01  1.8046646e+00]\n",
      " [ 1.9516014e+00 -8.3741868e-01  1.0197732e+01 -1.1388455e+01\n",
      "  -4.3978162e+00  6.0208039e+00 -9.6793766e+00  4.3019590e+00\n",
      "   3.4638417e+00 -7.4761415e-01]\n",
      " [-9.3241730e+00 -2.3291777e+01 -8.1243610e+00 -1.0718474e+01\n",
      "   1.3873384e+01  7.1166563e+00  5.9669847e+00 -6.5673103e+00\n",
      "  -4.1172638e+00  2.4699025e+00]\n",
      " [ 1.0203061e+01  1.3925249e+01  3.2332882e+01 -3.7783203e+01\n",
      "  -1.4287042e+01 -1.5231347e+00 -2.5449522e+01  1.4863117e+01\n",
      "   1.1434365e+01 -1.9119301e+01]\n",
      " [-2.2554469e+00 -1.9969166e+01  8.5117521e+00 -2.8444651e+01\n",
      "   2.2680316e+00  1.8494141e+01 -1.3490746e+01  4.6158900e+00\n",
      "   1.9390585e+00 -5.2459186e-01]]\n",
      "layer_2:  [[  2.3639705  12.503645    1.1537852   4.4393506  11.23205  ]\n",
      " [ -0.8822231  14.995784    2.147251   -8.29652    10.261083 ]\n",
      " [-21.827301   32.100395   11.107677   16.733583   34.738525 ]\n",
      " [-26.867903  -42.91461     9.957624  -11.611069  -21.288937 ]\n",
      " [-23.979393   46.311794   14.694571    0.4097577  47.461258 ]]\n",
      "layer_3:  [[ -1.6158412]\n",
      " [ -7.8003106]\n",
      " [-13.941002 ]\n",
      " [  3.75642  ]\n",
      " [-19.24889  ]]\n",
      "loss:  309.27374\n",
      "layer_1:  [[ -18.781982   -53.93302    -25.58841    -22.219782    34.76826\n",
      "     8.782478    17.873886   -22.801052   -13.210368     5.507802 ]\n",
      " [  12.895395    13.885745    46.804085   -66.65317    -16.8748\n",
      "     5.357099   -33.32036      8.513609    13.559861   -25.327805 ]\n",
      " [  30.037022     9.587746    48.431076   -60.006794   -28.193888\n",
      "    27.689804   -57.289127    22.088469     7.653015    -7.7775555]\n",
      " [  -5.10094    -48.607857     2.3116074  -52.709213    12.348689\n",
      "    30.229406   -10.508007    -5.854594    -3.8606548    5.1436663]\n",
      " [ -35.541214  -107.83357    -66.19508    -43.239506    76.26222\n",
      "    12.545199    33.099167   -47.227905   -29.549538     2.563654 ]]\n",
      "layer_2:  [[-24.02246    92.250946   58.501774   41.034058   35.689266 ]\n",
      " [-18.056211  -16.576168   15.660951   -1.9856949  -3.8343697]\n",
      " [-11.227531   60.810265   23.186981  -27.547802   24.967781 ]\n",
      " [ -5.6575775 127.26512    60.207287   29.686676   70.289856 ]\n",
      " [-91.66152   155.75412   130.63185    78.03393    47.106293 ]]\n",
      "layer_3:  [[  2.3833792]\n",
      " [  2.281568 ]\n",
      " [-28.96598  ]\n",
      " [-13.704407 ]\n",
      " [ 12.042597 ]]\n",
      "loss:  859.1331\n",
      "layer_1:  [[  48.14367     51.971325   108.98968   -113.86439    -54.49723\n",
      "     3.4402952  -83.66133     39.062916    27.141827   -40.967358 ]\n",
      " [  21.711084    12.514645    60.22168   -104.56147    -21.454136\n",
      "    10.164318   -36.75772    -13.995499    11.856676   -29.492197 ]\n",
      " [  37.463825    -8.254978    64.33501    -92.817085   -35.862198\n",
      "    43.767563   -61.553802    13.582602     2.5640736    7.5286756]\n",
      " [  -3.470612   -86.17863     -9.146252   -80.773       24.794847\n",
      "    35.216255     6.423685   -35.75074    -16.774548    20.211441 ]\n",
      " [ -44.801098  -166.13019   -110.134125   -56.089256   118.00862\n",
      "    -4.129732    76.56316   -106.538185   -57.7629      17.309776 ]]\n",
      "layer_2:  [[-45.78658    -76.949036   -12.557022     0.33397675 -18.815308  ]\n",
      " [ 33.797894    42.140076    49.62188     25.95602      4.5660877 ]\n",
      " [ 57.57115    169.34465     50.57897     18.396877    71.525055  ]\n",
      " [ 82.62251    263.95593    156.75166     85.228905    67.77391   ]\n",
      " [ 10.755727   325.9494     302.56503    141.50122    -35.884247  ]]\n",
      "layer_3:  [[ 30.375858]\n",
      " [  6.552541]\n",
      " [-45.348324]\n",
      " [-14.235619]\n",
      " [ 31.805382]]\n",
      "loss:  6676.945\n",
      "layer_1:  [[  79.31953    127.401535   207.06917   -205.90446    -88.20566\n",
      "   -19.681995  -146.92915     67.10007     47.550823   -98.98683  ]\n",
      " [  66.32517     64.11213    148.6157    -164.08423    -77.41963\n",
      "    10.274393   -92.85711     20.763699    27.249043   -34.440285 ]\n",
      " [ 122.053444    81.99199    163.29613   -164.17207   -119.83349\n",
      "    60.592995  -150.17221     45.167725     3.6646452    1.1797876]\n",
      " [  40.76886     13.054764    74.149445  -147.04988    -31.183285\n",
      "     5.47134    -27.1688     -63.067        3.5064316  -25.289307 ]\n",
      " [  55.010693   -29.786022    82.34416   -131.38051    -49.594448\n",
      "    52.85313    -50.35383    -16.126276    -8.554403    37.5467   ]]\n",
      "layer_2:  [[-163.49208  -263.72437  -113.18469    26.60498   -58.739105]\n",
      " [  39.333992  -15.149483  -20.914516   58.721146   35.915504]\n",
      " [  79.015305  134.88094   -26.84523    38.98858   114.77898 ]\n",
      " [ 160.10405   132.6303    119.24638    83.22162     4.084196]\n",
      " [ 209.26178   320.09796   114.48777   102.67021   122.70629 ]]\n",
      "layer_3:  [[161.12718  ]\n",
      " [ 53.40004  ]\n",
      " [-35.227882 ]\n",
      " [  1.3115506]\n",
      " [-66.11869  ]]\n",
      "loss:  23158.814\n",
      "layer_1:  [[  22.23638   -105.167496     1.7099895  -90.66385     11.161996\n",
      "     4.649194    69.34604   -117.878296   -41.58676     75.78074  ]\n",
      " [ -24.084305  -199.33113   -126.50594    -34.762436   128.39677\n",
      "   -73.65819    184.70438   -237.33704   -103.41886     83.960884 ]\n",
      " [ 111.18815    180.58347    279.10635   -276.03653   -125.58903\n",
      "   -17.247786  -167.29315     34.45312     55.505264   -99.17169  ]\n",
      " [ 152.67441    169.3633     282.1994    -257.3336    -172.18747\n",
      "    13.219559  -191.97284     70.51707     40.594624   -49.65831  ]\n",
      " [ 100.0012      84.843      196.65872   -220.17958   -113.130165\n",
      "     9.08745    -88.93827    -27.02578     22.21788    -12.860898 ]]\n",
      "layer_2:  [[ 406.7525    446.2213    306.0972    208.03015    57.145363]\n",
      " [ 450.04535   564.5897    586.1832    271.13843  -179.67877 ]\n",
      " [ -70.47393  -231.34512  -198.256      53.53839    33.981472]\n",
      " [ -20.274483 -163.15796  -193.03816   100.68379   131.0678  ]\n",
      " [ 207.6717     73.52058   -18.267067  135.08331   121.35038 ]]\n",
      "layer_3:  [[ -98.41661 ]\n",
      " [-131.44463 ]\n",
      " [ 194.96407 ]\n",
      " [ 220.30382 ]\n",
      " [  71.408714]]\n",
      "loss:  13352.419\n",
      "layer_1:  [[ 166.0468     112.13878    242.50836   -199.48186   -183.74458\n",
      "    60.961918  -132.69531     -6.498083    -7.663433    77.32676  ]\n",
      " [  87.61294     51.55132    120.33137   -169.09114    -72.070625\n",
      "   -30.021349    20.345839  -178.33774    -14.032061    26.633852 ]\n",
      " [ 103.08628    -20.523853   134.46252   -152.27321   -100.89921\n",
      "    26.251453     9.280849  -103.97149    -29.934452   119.17679  ]\n",
      " [  56.520218  -136.14954      1.2684336 -100.57617      1.7000341\n",
      "   -41.389587   159.16435   -232.48917    -79.44209    144.11714  ]\n",
      " [   2.2328835 -250.50488   -164.09627    -11.388424   150.0564\n",
      "  -164.81232    328.0565    -403.47162   -166.57468    159.05637  ]]\n",
      "layer_2:  [[ 324.1599    281.63254  -158.03859    53.648018  368.59317 ]\n",
      " [ 471.63556   235.02397   148.76413   149.93085   103.77635 ]\n",
      " [ 557.23425   452.84637   147.54816   238.36469   285.7165  ]\n",
      " [ 740.1763    568.07324   511.10297   444.91248    96.970604]\n",
      " [ 843.4219    646.9469    935.8359    663.80206  -248.27281 ]]\n",
      "layer_3:  [[-151.05475 ]\n",
      " [-100.23802 ]\n",
      " [-104.248825]\n",
      " [   6.954379]\n",
      " [ 141.45334 ]]\n",
      "loss:  13215.963\n",
      "layer_1:  [[ 194.5509      348.24496     478.1752     -377.23676    -226.72408\n",
      "    -0.99142647 -297.42856      64.88443      88.27468     -82.31213   ]\n",
      " [ 165.57634     293.41183     393.99258    -332.37515    -189.1805\n",
      "   -14.697854   -182.04384     -69.58119      64.35266     -44.503864  ]\n",
      " [ 207.5756      253.52515     399.53522    -308.79718    -251.88403\n",
      "    13.389236   -190.30121      -5.86528      39.1987       29.726324  ]\n",
      " [ 328.149       314.20157     439.35727    -325.1739     -359.0119\n",
      "    87.37573    -272.15704      34.923733    -17.83877      91.05284   ]\n",
      " [ 166.64531     156.36002     288.94473    -254.89908    -189.1674\n",
      "   -20.370798    -46.258347   -148.18295       9.994009     69.181335  ]]\n",
      "layer_2:  [[-192.16557  -249.31424  -730.7035   -308.66412   371.5339  ]\n",
      " [ 133.74678   -69.975204 -494.1242   -185.75464   355.2245  ]\n",
      " [ 193.95088   -33.84825  -491.0417    -41.586056  514.3015  ]\n",
      " [ 271.49173    90.85976  -581.34534    12.928408  735.08527 ]\n",
      " [ 521.87476   140.84807  -152.48691   141.06703   432.07886 ]]\n",
      "layer_3:  [[-122.24452 ]\n",
      " [-211.41037 ]\n",
      " [  15.2627  ]\n",
      " [  49.872566]\n",
      " [   9.865263]]\n",
      "loss:  474908.56\n",
      "layer_1:  [[ 247.70279   154.66763   346.9077   -228.35335  -291.4795     15.813927\n",
      "   -49.374245  -93.88435   -33.26435   186.75769 ]\n",
      " [ 173.8315    114.03511   190.77219  -171.60005  -155.02371  -122.463356\n",
      "   139.64162  -350.80984   -50.149944  114.937935]\n",
      " [ 190.16614     6.203251  212.73619  -155.53305  -195.17943   -54.87662\n",
      "   145.93306  -243.99425   -69.7671    242.51826 ]\n",
      " [ 146.07489  -121.02193    53.263367  -64.51112   -74.406845 -172.02875\n",
      "   358.987    -437.61343  -141.59013   286.68314 ]\n",
      " [  91.724205 -242.8226   -138.11945    78.39179    96.60637  -359.10107\n",
      "   601.0135   -686.8095   -259.14822   326.39624 ]]\n",
      "layer_2:  [[ 600.1334      81.33212   -295.5492     449.97665    848.6902   ]\n",
      " [ 828.5195      -4.7336426  165.9914     498.63794    423.07568  ]\n",
      " [ 957.39526    246.0904     170.7532     678.6649     689.1095   ]\n",
      " [1269.3191     340.77954    694.15466    931.34216    413.66446  ]\n",
      " [1511.019      387.39597   1285.1155    1188.6128     -33.300293 ]]\n",
      "layer_3:  [[717.62775]\n",
      " [574.01624]\n",
      " [722.911  ]\n",
      " [800.9999 ]\n",
      " [877.79224]]\n",
      "test loss:  166900.3\n",
      "test loss:  1611434.4\n",
      "epoch:  4\n",
      "batches:  8\n",
      "loss:  4812.6313\n",
      "layer_1:  [[ -1.4092009   -3.0215116    3.060499    -0.34983823   0.1055696\n",
      "    0.833226     3.532001    -4.548217    -0.77341896   4.913991  ]\n",
      " [  3.776782     8.130247    18.237803    -7.826355   -10.1673355\n",
      "    3.1175313   -6.290723    -2.5548449    3.8364081    8.223755  ]\n",
      " [ -3.783845   -11.450709     6.4233074   -0.19313955   3.1932418\n",
      "   -1.5137475   14.709289   -20.61903     -4.494884    17.168179  ]\n",
      " [ 11.137789    32.300148    45.36125    -32.45278    -20.830881\n",
      "    0.8573642  -24.429642     2.7233598   13.086477    -1.2425869 ]\n",
      " [  5.800378     4.0491676   32.83745    -14.1148205  -15.946999\n",
      "    6.2148433   -0.36347008 -17.378149     2.4828305   25.12907   ]]\n",
      "layer_2:  [[ 23.135143   22.370564    0.841979    2.696547    8.422863 ]\n",
      " [ 18.017897   18.14915   -35.14444    -8.114256   43.06759  ]\n",
      " [ 80.68156    77.72722    14.507179   10.534782   15.669767 ]\n",
      " [-18.897469  -20.83218   -91.04953   -26.035152   68.84945  ]\n",
      " [ 89.08921    85.13353   -41.28856    -1.1175642  82.25284  ]]\n",
      "layer_3:  [[ -27.3247  ]\n",
      " [ -18.32377 ]\n",
      " [-107.29301 ]\n",
      " [  35.852833]\n",
      " [ -95.19612 ]]\n",
      "loss:  291333.9\n",
      "layer_1:  [[  -9.913496   -22.236504    12.586243     7.2448974   13.914871\n",
      "    -5.6565285   31.63953    -63.871643   -10.615064    46.9259   ]\n",
      " [  15.536737    50.280136    78.42016    -47.71649    -31.198011\n",
      "     5.050666   -30.37167    -24.16462     17.80888     15.013302 ]\n",
      " [  31.931086    54.668304    88.34258    -40.01627    -49.03662\n",
      "    23.26845    -48.17421    -16.864405    12.108763    40.45485  ]\n",
      " [   5.50286      1.1581268   56.112602   -15.4167385  -17.226301\n",
      "    12.748897     6.8283653  -61.78663      1.292162    64.878006 ]\n",
      " [ -15.413788   -38.214474    16.161263    20.883701    30.669895\n",
      "   -19.216076    64.124306  -136.08093    -23.33434     92.03921  ]]\n",
      "layer_2:  [[ 217.44868   248.80351     9.96862   -35.97874    12.320891]\n",
      " [  49.146957   62.661774 -160.52151   -69.53604   141.50513 ]\n",
      " [  61.40444   124.996185 -199.75131   -81.93387   218.47409 ]\n",
      " [ 244.49774   290.30414   -86.363335  -64.600426  140.83311 ]\n",
      " [ 419.72174   479.01135    25.152122  -82.52774     3.415596]]\n",
      "layer_3:  [[-483.8129 ]\n",
      " [ -97.83973]\n",
      " [-136.9429 ]\n",
      " [-495.5851 ]\n",
      " [-963.36694]]\n",
      "loss:  1375422.1\n",
      "layer_1:  [[  40.06157    114.74851    155.95113    -86.46782    -65.75575\n",
      "    18.956602   -90.5628     -12.827703    37.561497    25.482944 ]\n",
      " [  21.28244     82.95357    124.00911    -57.813164   -41.587826\n",
      "    12.340893   -39.54326    -85.29837     25.747267    50.56276  ]\n",
      " [  33.5948      72.635796   138.33192    -43.18504    -60.10161\n",
      "    44.04599    -60.60725    -67.66721     18.54803     97.45707  ]\n",
      " [   0.9195888   10.626978    90.42306     -2.4772298  -10.425642\n",
      "    22.244198    14.157559  -148.19115      4.5219536  134.89688  ]\n",
      " [ -33.70387    -34.37486     32.307453    63.923553    66.7945\n",
      "   -28.594528    92.1478    -270.8327     -28.348803   178.93051  ]]\n",
      "layer_2:  [[-5.5301910e+01  4.6126019e+01 -3.7499640e+02 -1.6332770e+02\n",
      "   3.3080951e+02]\n",
      " [ 1.8673856e+02  2.5506924e+02 -3.0173413e+02 -1.9567035e+02\n",
      "   2.4939609e+02]\n",
      " [ 2.1846588e+02  3.9045230e+02 -3.5078021e+02 -2.0923993e+02\n",
      "   3.7213330e+02]\n",
      " [ 4.9152893e+02  6.5326825e+02 -2.2101938e+02 -2.4013159e+02\n",
      "   2.3133800e+02]\n",
      " [ 7.5966748e+02  9.7283380e+02 -1.1082912e+02 -3.3770093e+02\n",
      "   2.1286011e-02]]\n",
      "layer_3:  [[   99.74005]\n",
      " [ -465.37366]\n",
      " [ -565.7073 ]\n",
      " [-1262.9344 ]\n",
      " [-2165.5552 ]]\n",
      "loss:  266502.44\n",
      "layer_1:  [[  61.392723   230.00299    263.8115    -159.43422    -89.807686\n",
      "    18.009378  -171.48451      6.0036087   71.966736   -11.041011 ]\n",
      " [  48.46498    182.85608    231.11214    -93.83055    -88.24578\n",
      "    33.850426  -116.93132    -66.74922     63.73289     69.81911  ]\n",
      " [  92.27649    228.58379    265.41345    -84.595024  -131.72787\n",
      "    91.02564   -179.18161    -62.447166    48.84725    126.49681  ]\n",
      " [  28.6423     154.899      184.83954    -42.774895   -51.677837\n",
      "    16.863604   -51.845013  -180.61218     54.090202   102.8075   ]\n",
      " [  35.761204   126.28374    205.28214    -20.664434   -70.76324\n",
      "    65.28185    -75.13636   -146.13747     48.98851    176.03476  ]]\n",
      "layer_2:  [[-330.01654  -160.16797  -661.6593   -258.36133   531.62683 ]\n",
      " [ -21.493439  141.36478  -667.4643   -317.135     596.67847 ]\n",
      " [ -17.381042  310.42596  -840.4395   -385.62915   826.3279  ]\n",
      " [ 267.71777   389.58356  -618.0796   -406.30878   487.70688 ]\n",
      " [ 309.89917   588.49243  -680.05347  -410.2417    679.6809  ]]\n",
      "layer_3:  [[ 833.864  ]\n",
      " [ 229.29492]\n",
      " [ 220.20233]\n",
      " [-481.19763]\n",
      " [-554.0829 ]]\n",
      "loss:  828750.06\n",
      "layer_1:  [[ -12.898436   112.96307    152.27502     86.37021      5.7448053\n",
      "    28.274418     7.106945  -264.61212     76.53515    225.0384   ]\n",
      " [ -64.85278     92.16411     78.076645   214.79605    117.38875\n",
      "   -50.331814   104.18907   -435.59915     61.275967   281.0986   ]\n",
      " [  74.420586   329.05246    366.90845   -191.05315   -116.21854\n",
      "    40.554207  -220.59654    -71.93913    103.9048      33.647804 ]\n",
      " [ 100.69781    344.4098     388.1458    -159.78479   -159.06172\n",
      "    80.34884   -256.391      -51.686874   103.811104    96.30642  ]\n",
      " [  56.923462   285.93173    327.38324    -82.13718   -109.91741\n",
      "    56.978737  -154.79955   -165.55768    110.88433    142.26276  ]]\n",
      "layer_2:  [[  371.62842    629.3031    -817.26245   -553.839      705.2832  ]\n",
      " [  542.7443     842.8731    -871.39197   -737.2784     493.70465 ]\n",
      " [ -282.98914     20.909698 -1021.78516   -498.74207    849.8738  ]\n",
      " [ -306.85678     92.434784 -1166.9878    -535.699     1094.1895  ]\n",
      " [   -2.377449   288.34573  -1131.4327    -598.3964    1015.0116  ]]\n",
      "layer_3:  [[ -429.59216]\n",
      " [-1092.6019 ]\n",
      " [  996.0224 ]\n",
      " [ 1198.0507 ]\n",
      " [  593.9558 ]]\n",
      "loss:  339744.72\n",
      "layer_1:  [[  82.61646    326.9119     371.59985    -52.12213   -143.27744\n",
      "   150.14844   -246.31818   -163.17418    104.651276   237.14111  ]\n",
      " [  22.037006   309.82587    270.22382     32.669525   -46.219707\n",
      "    33.67496    -92.52567   -331.12833    138.60349    189.52077  ]\n",
      " [  23.765419   265.73798    304.63187     69.26034    -68.59559\n",
      "    98.378296  -116.76036   -270.0039     145.40823    290.1455   ]\n",
      " [ -29.442736   256.3082     229.54968    219.12863     31.113554\n",
      "    26.189983     4.1600485 -426.4814     179.16656    347.3802   ]\n",
      " [ -99.74083    286.84366    141.4233     435.25656    180.49821\n",
      "   -92.101074   127.66341   -642.8916     199.47064    414.59244  ]]\n",
      "layer_2:  [[   65.115875    745.3311    -1261.4469     -857.04395    1317.7249   ]\n",
      " [  133.53433     459.3236    -1283.4963     -892.84674    1050.3679   ]\n",
      " [  140.15927     642.9868    -1378.6691     -865.95935    1362.7378   ]\n",
      " [  136.56113     528.3185    -1617.751      -972.1504     1392.9248   ]\n",
      " [   -7.8361816   355.6515    -2009.3252    -1167.6459     1369.9551   ]]\n",
      "layer_3:  [[428.80185]\n",
      " [316.66766]\n",
      " [498.62317]\n",
      " [631.1173 ]\n",
      " [896.4683 ]]\n",
      "loss:  2931532.5\n",
      "layer_1:  [[  85.225845  404.99747   494.73663  -375.78384  -134.90817   162.70538\n",
      "  -431.64426  -131.68106    65.774025  100.31277 ]\n",
      " [  62.987453  411.47403   450.6349   -266.88687  -109.05679   128.84285\n",
      "  -328.07578  -261.98978    99.14462   132.47304 ]\n",
      " [  82.11      403.35236   483.69522  -222.97513  -157.24413   182.86952\n",
      "  -368.2582   -225.15666    98.244484  218.62572 ]\n",
      " [ 163.32782   502.49683   554.51013  -221.30022  -236.4876    308.3378\n",
      "  -503.07224  -240.08963    60.773705  314.68567 ]\n",
      " [  43.374702  391.61844   423.2517    -85.66371  -103.32893   131.99374\n",
      "  -245.58745  -367.794     146.15527   259.75696 ]]\n",
      "layer_2:  [[  424.3521   1625.7454   -615.578   -1260.6486    599.87024]\n",
      " [  521.31946  1462.5205   -860.4149  -1329.4762    738.6387 ]\n",
      " [  534.1019   1612.345    -980.82245 -1365.181    1032.6144 ]\n",
      " [  701.0974   2165.4448  -1181.3094  -1634.2986   1371.6761 ]\n",
      " [  568.68884  1381.8264  -1235.2024  -1415.2626   1167.1494 ]]\n",
      "layer_3:  [[-1796.7704]\n",
      " [-1698.8474]\n",
      " [-1590.6486]\n",
      " [-2007.7721]\n",
      " [-1367.9747]]\n",
      "loss:  1977912.6\n",
      "layer_1:  [[ 4.19948807e+01  4.38180664e+02  4.48908813e+02 -4.69051361e-01\n",
      "  -1.25465614e+02  2.02873428e+02 -3.19543457e+02 -2.97786011e+02\n",
      "   1.78551361e+02  3.54881592e+02]\n",
      " [-1.51172066e+01  4.77304810e+02  3.33062683e+02  1.31965424e+02\n",
      "  -7.26661777e+00  4.50273438e+01 -1.47669922e+02 -5.37567566e+02\n",
      "   2.34212692e+02  2.82367767e+02]\n",
      " [-1.61521225e+01  4.04328369e+02  3.83776337e+02  1.75809570e+02\n",
      "  -3.71791267e+01  1.29489151e+02 -1.73548798e+02 -4.46093506e+02\n",
      "   2.48565720e+02  4.12212463e+02]\n",
      " [-7.88459854e+01  4.29186523e+02  3.05455292e+02  3.97584106e+02\n",
      "   9.03585510e+01  2.86140137e+01 -3.77180977e+01 -6.54149109e+02\n",
      "   3.11339661e+02  4.73601166e+02]\n",
      " [-1.67599640e+02  5.02128784e+02  2.10896576e+02  7.03322754e+02\n",
      "   2.80366272e+02 -1.25660126e+02  9.87648163e+01 -9.31744934e+02\n",
      "   3.61333252e+02  5.44036804e+02]]\n",
      "layer_2:  [[  -42.47635   1055.0352   -1590.4779   -1165.0103    1906.2009  ]\n",
      " [  -48.603973   565.99646  -1752.3304   -1352.4016    1597.946   ]\n",
      " [  -50.4552     765.5915   -1842.3942   -1263.3635    2027.3331  ]\n",
      " [ -176.5198     441.65564  -2231.2222   -1473.6299    2119.199   ]\n",
      " [ -481.727       23.795166 -2793.4624   -1790.1355    2144.7295  ]]\n",
      "layer_3:  [[1064.8374]\n",
      " [ 944.4939]\n",
      " [1299.7471]\n",
      " [1625.5627]\n",
      " [2097.6804]]\n",
      "test loss:  512869.6\n",
      "test loss:  308999.88\n",
      "epoch:  5\n",
      "batches:  8\n",
      "loss:  1220.7894\n",
      "layer_1:  [[ -2.105323     0.1526202    3.6837938    2.3722992    0.40842518\n",
      "    1.1635674    1.9224675   -4.6217675    1.543836     4.044909  ]\n",
      " [ -1.6117624   13.404924    16.23437     -3.5210288   -5.6824164\n",
      "    6.3796897  -11.023773    -5.6871943    7.225006     8.650275  ]\n",
      " [ -7.8452187    3.101779    10.2503395   11.8204155    5.58161\n",
      "    1.7378013    5.703612   -23.186394     6.7123423   14.96456   ]\n",
      " [ -2.3239038   35.035515    33.770878   -30.205832    -7.4476833\n",
      "    8.329058   -30.347824    -5.2936916   12.988362     6.2115874 ]\n",
      " [ -6.6700315   23.108437    33.15781      1.7424412   -6.266397\n",
      "   14.677813   -15.650305   -25.766167    16.591055    25.315796  ]]\n",
      "layer_2:  [[ 12.139459   10.515082  -13.450678   -7.232564   14.394498 ]\n",
      " [ -4.1334295  23.702335  -40.101967  -28.226843   59.312836 ]\n",
      " [ 38.06365    40.60122   -48.678757  -37.97187    41.84255  ]\n",
      " [-31.572193   52.607666  -61.219574  -52.26466    80.25566  ]\n",
      " [ 24.308447   76.24873   -95.13543   -71.01782   129.41743  ]]\n",
      "layer_3:  [[ -3.538937]\n",
      " [ 31.905607]\n",
      " [-28.027798]\n",
      " [ 59.97882 ]\n",
      " [ 33.72853 ]]\n",
      "loss:  33320.664\n",
      "layer_1:  [[ -18.776337      9.182106     14.762235     30.923523     18.964336\n",
      "     3.2814069     9.074211    -70.599525     13.461738     28.942932  ]\n",
      " [ -11.312495     62.719578     52.80223     -35.474495     -5.863424\n",
      "    14.412622    -42.020813    -34.184452     26.027956     14.059496  ]\n",
      " [  -0.6579938    75.141685     62.703457    -21.691338    -20.548967\n",
      "    36.11553     -67.084       -29.300634     26.952534     31.291067  ]\n",
      " [ -17.087194     38.590843     47.718464     14.923405     -0.29995632\n",
      "    26.664143    -23.22453     -74.38411      30.299665     44.613503  ]\n",
      " [ -35.53182      28.518625     22.152138     70.35         42.888626\n",
      "     2.7518988    13.368544   -154.12263      28.387714     55.92221   ]]\n",
      "layer_2:  [[ 127.30391   131.28082   -63.732147 -117.05594    29.00272 ]\n",
      " [ -19.366306   91.39808  -110.47812  -102.3743    138.23901 ]\n",
      " [ -55.497265  114.11023  -141.74829  -113.877426  239.10822 ]\n",
      " [ 103.656876  176.88434  -124.44969  -150.77982   173.52785 ]\n",
      " [ 240.35313   258.65173  -125.17539  -264.063      37.384388]]\n",
      "layer_3:  [[-161.22067 ]\n",
      " [  42.704712]\n",
      " [ 127.8105  ]\n",
      " [ -70.78924 ]\n",
      " [-337.00955 ]]\n",
      "loss:  147234.39\n",
      "layer_1:  [[ -19.727097   121.43666     82.56477    -71.349       -7.2837725\n",
      "    28.32766    -98.74052    -23.424759    45.25805     13.223897 ]\n",
      " [ -24.97192    103.8215      68.26627    -34.75419      1.0126553\n",
      "    24.507826   -60.671555   -97.82403     43.81652     19.636757 ]\n",
      " [ -15.976881   100.87344     82.44327    -13.338498   -17.665894\n",
      "    58.186848   -90.17963    -80.85834     44.222897    49.76924  ]\n",
      " [ -33.923187    58.69535     55.5735      36.87681     15.380219\n",
      "    45.471737   -37.415123  -168.18771     44.69783     62.563988 ]\n",
      " [ -61.208878    43.59338     11.636578   117.26082     83.04768\n",
      "    11.408699     8.607542  -303.53348     33.2829      69.41387  ]]\n",
      "layer_2:  [[-179.11435    72.37941  -137.97679  -107.28985   238.66751 ]\n",
      " [  47.9849    180.63931  -128.34084  -200.89174   158.2622  ]\n",
      " [   9.766998  222.14755  -152.0841   -179.05127   303.9816  ]\n",
      " [ 287.52277   379.62003   -93.21983  -287.77197   139.9601  ]\n",
      " [ 569.3897    582.0718    -26.748245 -499.41592  -166.37668 ]]\n",
      "layer_3:  [[ 206.3353  ]\n",
      " [ -63.389114]\n",
      " [  53.79417 ]\n",
      " [-293.27518 ]\n",
      " [-769.04596 ]]\n",
      "loss:  63774.98\n",
      "layer_1:  [[-5.72989578e+01  2.04885178e+02  1.03765190e+02 -1.50420273e+02\n",
      "   3.39162750e+01  2.58218536e+01 -1.62335922e+02 -1.07118702e+01\n",
      "   6.54757690e+01 -1.09311657e+01]\n",
      " [-4.36353340e+01  1.74707199e+02  1.00733391e+02 -7.72716751e+01\n",
      "   1.34172726e+00  4.53649139e+01 -1.30914658e+02 -8.55809937e+01\n",
      "   6.90333939e+01  1.72061596e+01]\n",
      " [-1.42992706e+01  2.24046906e+02  1.15062752e+02 -6.01331177e+01\n",
      "  -3.24475517e+01  1.02368866e+02 -2.00027832e+02 -8.11878967e+01\n",
      "   6.15609131e+01  4.15310364e+01]\n",
      " [-4.55142975e+01  1.56997955e+02  7.26690369e+01 -2.67874050e+01\n",
      "   1.77289219e+01  4.13918152e+01 -9.03464203e+01 -2.11990280e+02\n",
      "   6.29574089e+01  1.64673367e+01]\n",
      " [-3.75362701e+01  1.31839478e+02  9.36129761e+01  1.53390884e-01\n",
      "  -6.64742756e+00  9.13456573e+01 -1.24573364e+02 -1.78533829e+02\n",
      "   6.31396217e+01  6.40480423e+01]]\n",
      "layer_2:  [[-416.99112    43.080536 -126.20361   -62.42108   219.67596 ]\n",
      " [-140.32845   153.84795  -129.66418  -182.85237   270.9631  ]\n",
      " [-262.0429    188.58626  -197.37683  -167.38037   496.64508 ]\n",
      " [ 213.2471    363.94675   -65.40628  -356.5869     81.96805 ]\n",
      " [ 175.99289   434.7247    -82.87548  -285.2182    291.56732 ]]\n",
      "layer_3:  [[ 367.11835]\n",
      " [ 134.3025 ]\n",
      " [ 322.99716]\n",
      " [-245.75432]\n",
      " [ -80.01819]]\n",
      "loss:  297168.88\n",
      "layer_1:  [[ -62.585514    63.007984    27.234715    53.691223    49.8251\n",
      "    83.72135    -72.31894   -336.4265      40.108967    44.66587  ]\n",
      " [ -97.201126    27.81881    -44.658855   149.4167     146.29448\n",
      "    48.43615    -26.154785  -554.1013       1.5636743   32.29261  ]\n",
      " [ -94.34777    292.69955    117.50168   -171.19124     55.105194\n",
      "    35.4505    -201.02968    -83.28456     99.09122    -24.180008 ]\n",
      " [ -70.518684   307.53464    128.08261   -137.70732      9.870943\n",
      "    73.618256  -243.40399    -62.196423   102.22523     -2.9872525]\n",
      " [ -75.54434    240.53793    105.63325    -79.25033     18.457462\n",
      "    72.767365  -176.32312   -199.99826     95.01244      9.218386 ]]\n",
      "layer_2:  [[ 708.1398    805.42535   203.72096  -431.2564   -192.63586 ]\n",
      " [1281.5116   1287.3108    451.46994  -766.1322   -824.2749  ]\n",
      " [-409.54993    78.17688   -92.832    -153.90001   203.27588 ]\n",
      " [-516.12885    22.077412 -133.48074  -125.11106   418.48724 ]\n",
      " [   6.993283  330.15436   -28.121403 -305.93686   199.53859 ]]\n",
      "layer_3:  [[ -453.49557 ]\n",
      " [-1007.05505 ]\n",
      " [  283.58344 ]\n",
      " [  419.95557 ]\n",
      " [   15.980625]]\n",
      "loss:  162452.42\n",
      "layer_1:  [[ -62.151398  262.68237    95.04894   -50.277203   -8.192402  144.79184\n",
      "  -253.28418  -182.50803    81.880035   21.482899]\n",
      " [ -84.554924  204.22328    28.839231  -21.08811    59.241577   74.33755\n",
      "  -141.86833  -401.38678    67.549644  -31.499807]\n",
      " [ -75.835655  148.02692    57.904625    7.878563   26.469498  146.12213\n",
      "  -183.50322  -348.4005     66.229904   34.329376]\n",
      " [ -95.94806    71.57646   -13.434548   80.60358   100.54375   137.8276\n",
      "  -130.02295  -578.2991     31.001945   22.854738]\n",
      " [-139.95642    18.19931  -118.89521   200.22156   233.58978    98.6022\n",
      "   -86.14801  -887.2093    -34.56356    -8.708958]]\n",
      "layer_2:  [[ -130.62595    332.17386     41.233765  -158.52888    364.93112 ]\n",
      " [  577.4032     730.387      297.1668    -500.43512   -316.64862 ]\n",
      " [  531.8772     829.0357     281.81958   -341.88947    -20.205406]\n",
      " [ 1248.1045    1432.945      602.2401    -622.9804    -630.5498  ]\n",
      " [ 2076.3872    2197.236     1044.8562   -1067.3623   -1588.1538  ]]\n",
      "layer_3:  [[ 275.6255   ]\n",
      " [-218.05183  ]\n",
      " [  -1.3988132]\n",
      " [-322.95895  ]\n",
      " [-755.51373  ]]\n",
      "loss:  157192.23\n",
      "layer_1:  [[-184.33392    439.83676     99.22059   -235.91222    117.35723\n",
      "    35.995884  -296.78232    -29.542133   154.97093    -76.37807  ]\n",
      " [-161.58333    395.55566     77.46635   -185.2681     104.48595\n",
      "    50.594105  -250.51305   -209.79245    136.31003    -78.84043  ]\n",
      " [-141.12073    384.8596      92.5572    -141.29413     50.936584\n",
      "   104.39868   -301.5022    -171.79439    135.37706    -45.758556 ]\n",
      " [ -92.6342     491.99265     95.618164  -117.78635     -3.4228172\n",
      "   210.00513   -427.42416   -168.32623    114.165825   -31.51985  ]\n",
      " [-130.71674    300.68976     49.565983   -84.778305    66.07394\n",
      "   120.46553   -253.10773   -385.2995     110.087746   -50.445072 ]]\n",
      "layer_2:  [[-8.6392981e+02 -2.2417747e+02  8.4987610e+01 -1.3005585e+01\n",
      "   1.4002512e+02]\n",
      " [-2.9130530e+02  1.7390717e+02  2.2576718e+02 -2.2910382e+02\n",
      "  -8.2299698e+01]\n",
      " [-4.1297903e+02  1.2193659e+02  1.8534885e+02 -1.2937062e+02\n",
      "   1.9254340e+02]\n",
      " [-7.1321179e+02  6.8671577e+01  7.2562012e+01 -5.7067108e-01\n",
      "   5.8244458e+02]\n",
      " [ 2.8657556e+02  6.8536078e+02  4.5535449e+02 -3.4813312e+02\n",
      "  -1.9780991e+02]]\n",
      "layer_3:  [[398.62677]\n",
      " [280.9211 ]\n",
      " [374.19073]\n",
      " [562.5075 ]\n",
      " [342.01514]]\n",
      "loss:  2355940.0\n",
      "layer_1:  [[ -116.15924     294.1879       50.086422    -49.564075     18.723736\n",
      "    213.32904    -331.76138    -352.72742      90.03514     -35.63662  ]\n",
      " [ -130.13994     240.39778     -60.478333    -26.171043    121.094284\n",
      "    138.13023    -241.61049    -679.03705      50.283855   -128.16599  ]\n",
      " [ -118.76129     145.47366     -19.043858      1.6286888    77.08764\n",
      "    236.69792    -291.1064     -603.5131       46.453438    -39.550446 ]\n",
      " [ -135.22255      31.109442   -133.30138      73.674126    187.48148\n",
      "    247.72896    -271.48038    -933.78613     -27.223114    -77.86947  ]\n",
      " [ -181.85822     -65.26636    -292.47437     197.31934     369.39572\n",
      "    226.61317    -268.61423   -1359.3872     -142.90305    -146.25914  ]]\n",
      "layer_2:  [[  165.85852    632.553      440.2395    -105.48148     58.626434]\n",
      " [ 1067.5942    1371.4126     976.787     -548.98804   -982.7259  ]\n",
      " [ 1010.50165   1496.8895     933.626     -275.87625   -565.16156 ]\n",
      " [ 1947.3322    2537.9775    1587.2197    -547.54425  -1517.4376  ]\n",
      " [ 3045.5022    3837.9102    2470.026     -989.321    -2932.2112  ]]\n",
      "layer_3:  [[ 650.12616]\n",
      " [ 924.6109 ]\n",
      " [1146.9204 ]\n",
      " [1792.7092 ]\n",
      " [2618.2576 ]]\n",
      "test loss:  194431.0\n",
      "test loss:  63338.676\n",
      "epoch:  6\n",
      "batches:  8\n",
      "loss:  296.44666\n",
      "layer_1:  [[-1.83064473e+00 -1.41359746e+00  1.58122897e+00  8.86696696e-01\n",
      "   1.01943672e-01  1.72682452e+00  1.21311259e+00 -5.42963982e+00\n",
      "   2.00603306e-02  4.26053345e-01]\n",
      " [-6.33781290e+00  1.29279480e+01  6.63076782e+00 -1.82262802e+00\n",
      "  -3.21315002e+00  3.52053928e+00 -6.90004826e+00 -4.37186003e+00\n",
      "   7.54071999e+00 -1.86913967e+00]\n",
      " [-6.62800837e+00 -5.72841644e+00  1.14279604e+00  2.99495220e+00\n",
      "   4.57008553e+00  6.24238110e+00  1.12075233e+00 -2.92114639e+01\n",
      "  -2.16708994e+00 -1.51827443e+00]\n",
      " [-1.95070190e+01  3.77208710e+01  1.07324753e+01 -2.11300259e+01\n",
      "   4.23188639e+00 -1.48533905e+00 -1.51338959e+01  8.05694103e-01\n",
      "   1.80474949e+01 -6.59834480e+00]\n",
      " [-1.35596514e+01  1.51810303e+01  1.09130249e+01 -2.27959967e+00\n",
      "  -2.89058805e+00  1.38935118e+01 -1.30099125e+01 -2.88117180e+01\n",
      "   1.00275679e+01 -3.95769858e+00]]\n",
      "layer_2:  [[ 23.013828   14.844166    4.6597915  -4.3890867  -4.403948 ]\n",
      " [ -3.0633812 -19.361565   -4.12154    -9.910427   19.983498 ]\n",
      " [ 98.459145   76.87394    32.40301   -23.037374  -47.908604 ]\n",
      " [-50.30075   -62.956165  -23.644188  -11.838952   20.602432 ]\n",
      " [ 64.67233    29.81715    16.58275   -21.82626     5.1618905]]\n",
      "layer_3:  [[ -0.48789334]\n",
      " [-15.500652  ]\n",
      " [  1.1493366 ]\n",
      " [-28.821545  ]\n",
      " [-10.025385  ]]\n",
      "loss:  20470.566\n",
      "layer_1:  [[ -11.865902     -9.167492     -0.46496105    7.542345      9.872694\n",
      "    13.1001835     2.213067    -85.467026     -9.892947    -15.001498  ]\n",
      " [ -32.882282     63.87454      19.500872    -27.665956      2.2289443\n",
      "     0.70212746  -16.45863     -28.837547     27.54891     -21.51324   ]\n",
      " [ -23.629662     77.59591      24.591114    -12.211875    -16.053043\n",
      "    19.12318     -36.940113    -22.824593     29.103626    -22.937908  ]\n",
      " [ -20.482233     22.198818     16.898518     -1.8380623    -7.1203012\n",
      "    28.148155    -16.745697    -85.568306      9.585584    -19.840813  ]\n",
      " [ -20.652426    -12.764385     -9.822157     17.717115     23.756626\n",
      "    26.086962     -2.7931995  -188.89096     -24.0611      -38.228252  ]]\n",
      "layer_2:  [[ 271.41553   167.61618    57.256752  -83.90454  -151.64305 ]\n",
      " [  25.167961  -91.35271   -44.159595  -63.866734   -4.3056  ]\n",
      " [ -37.31442  -152.5534    -65.648056  -42.363033   89.254715]\n",
      " [ 230.32733    80.92624    17.017128  -74.54257   -49.041637]\n",
      " [ 561.41376   351.5518    126.89832  -191.68292  -352.654   ]]\n",
      "layer_3:  [[ -99.12516 ]\n",
      " [-121.15564 ]\n",
      " [-107.973045]\n",
      " [-123.35972 ]\n",
      " [-211.23985 ]]\n",
      "loss:  200467.64\n",
      "layer_1:  [[-6.14768829e+01  1.36356537e+02  3.58303986e+01 -4.67967529e+01\n",
      "  -2.67573071e+00 -6.21328545e+00 -3.24491425e+01 -7.42946339e+00\n",
      "   5.73038292e+01 -4.85623093e+01]\n",
      " [-4.52261963e+01  1.04716736e+02  2.97893925e+01 -3.36032143e+01\n",
      "  -7.84513092e+00  4.84947491e+00 -1.56141806e+01 -9.53301392e+01\n",
      "   3.58401947e+01 -5.69529495e+01]\n",
      " [-3.45036850e+01  1.02474258e+02  4.32038422e+01 -1.22671070e+01\n",
      "  -3.46859322e+01  3.58074570e+01 -4.02879372e+01 -7.90845718e+01\n",
      "   3.47957649e+01 -4.83216591e+01]\n",
      " [-2.36185570e+01  3.75799141e+01  2.60325394e+01 -6.39066696e-02\n",
      "  -1.99029655e+01  4.61328812e+01 -1.59876509e+01 -1.89147949e+02\n",
      "   2.19523239e+00 -5.88901367e+01]\n",
      " [-2.03402920e+01 -5.13457870e+00 -1.32907104e+01  3.05133934e+01\n",
      "   2.35642624e+01  3.81617661e+01  4.68345451e+00 -3.53329071e+02\n",
      "  -5.28980560e+01 -9.88042908e+01]]\n",
      "layer_2:  [[ -92.19505  -343.51877  -135.94412   -90.183365   49.878586]\n",
      " [ 198.28197  -146.32124   -97.73912  -173.88791   -71.24656 ]\n",
      " [ 128.03728  -191.41664  -121.89972  -103.329865   85.43877 ]\n",
      " [ 530.4021    122.07724   -27.209248 -189.50279  -157.23453 ]\n",
      " [1047.432     477.45563    88.21312  -405.93787  -638.0128  ]]\n",
      "layer_3:  [[-205.78574]\n",
      " [-350.60397]\n",
      " [-272.73035]\n",
      " [-428.17096]\n",
      " [-747.0973 ]]\n",
      "loss:  289351.75\n",
      "layer_1:  [[-128.97914    236.56981     43.277138  -103.4201      36.1766\n",
      "   -35.30162    -36.887524    17.84363     91.30419    -83.34869  ]\n",
      " [ -80.85971    196.9198      58.918346   -55.53778    -25.732014\n",
      "    -3.1053257  -27.199722   -69.13347     73.72415    -94.38147  ]\n",
      " [ -56.265182   254.57507     70.46258    -31.428768   -74.436714\n",
      "    41.441864   -72.13289    -60.713795    69.709885  -109.96629  ]\n",
      " [ -51.046745   164.15619     43.512917   -38.52959    -32.014153\n",
      "     9.865783    -9.01409   -211.8685      38.922657  -123.93819  ]\n",
      " [ -37.416924   139.57338     69.05337    -13.626545   -69.53963\n",
      "    57.39782    -37.780922  -181.06308     35.319714  -100.55154  ]]\n",
      "layer_2:  [[-176.11607  -619.26306  -270.03946  -145.94672   -45.074913]\n",
      " [  74.92555  -493.34766  -238.26222  -206.11613    28.173958]\n",
      " [ -90.26717  -698.4419   -372.28854  -137.62978   277.93402 ]\n",
      " [ 496.91998  -249.81898  -211.37296  -359.6066   -187.7886  ]\n",
      " [ 410.28705  -264.25943  -234.51334  -214.31671    61.239006]]\n",
      "layer_3:  [[-332.19806]\n",
      " [-459.27628]\n",
      " [-390.7052 ]\n",
      " [-773.0824 ]\n",
      " [-590.22705]]\n",
      "loss:  1131622.1\n",
      "layer_1:  [[   0.9614496   72.12019     54.54196     -2.1851158  -71.56345\n",
      "    58.765556     9.865862  -347.88684    -21.741447  -153.63495  ]\n",
      " [  18.674484    24.19802      8.333462    36.4756     -24.358044\n",
      "    41.062454    44.32608   -586.52246   -108.995026  -231.42746  ]\n",
      " [-149.59012    338.65707     71.233475  -122.89334      8.116213\n",
      "   -47.75698    -23.984331   -46.69214    121.15401   -151.52692  ]\n",
      " [-121.66783    361.1755      89.846924   -86.79365    -57.73443\n",
      "   -18.147121   -49.461033   -23.38413    125.420815  -167.50177  ]\n",
      " [ -83.14902    278.89465     89.60038    -66.5977     -72.53407\n",
      "     2.263836   -17.838802  -177.95401     86.48088   -174.39223  ]]\n",
      "layer_2:  [[  958.24774     10.279669  -235.2892    -369.0605    -252.22466 ]\n",
      " [ 1702.6631     395.11237   -204.62987   -717.32574   -917.0664  ]\n",
      " [  -48.902832  -892.6811    -406.30624   -313.21805    -67.283295]\n",
      " [ -217.22713  -1080.3214    -452.8298    -225.809      195.6     ]\n",
      " [  313.43234   -702.89026   -391.16986   -366.73926     10.628128]]\n",
      "layer_3:  [[-1048.767  ]\n",
      " [-1819.9978 ]\n",
      " [ -600.88055]\n",
      " [ -435.2794 ]\n",
      " [ -799.6329 ]]\n",
      "loss:  2029142.4\n",
      "layer_1:  [[ -23.93378    321.70355    126.206726   -35.32617   -163.99347\n",
      "    53.628937   -56.951607  -142.25586     75.870255  -197.47165  ]\n",
      " [   1.6486225  248.25652     77.44725    -51.99138   -111.81342\n",
      "     6.8089685   13.408955  -372.14462     26.557018  -253.91356  ]\n",
      " [  18.363815   195.66231    120.998215   -26.217442  -162.99107\n",
      "    74.307785   -16.35076   -322.5423      19.87912   -214.155    ]\n",
      " [  65.32832    112.899185    88.68032    -11.586827  -144.77133\n",
      "    79.89215     25.522713  -564.9462     -63.550674  -284.48447  ]\n",
      " [ 103.77661     56.722076    31.441307    36.562737   -91.02821\n",
      "    51.73471     71.07833   -886.0593    -188.36871   -404.52286  ]]\n",
      "layer_2:  [[  -28.06224   -973.01263   -519.34875   -142.81119    482.30536 ]\n",
      " [  720.78357   -510.95178   -421.67834   -511.81232   -177.91412 ]\n",
      " [  611.64417   -487.6297    -443.35147   -260.36447    195.79663 ]\n",
      " [ 1353.7866     -61.977818  -444.43994   -499.6178    -302.0464  ]\n",
      " [ 2298.2944     387.34943   -469.53333   -956.949    -1160.3955  ]]\n",
      "layer_3:  [[ -319.96875]\n",
      " [-1077.6136 ]\n",
      " [ -770.5426 ]\n",
      " [-1443.03   ]\n",
      " [-2475.0942 ]]\n",
      "loss:  56204.227\n",
      "layer_1:  [[-142.40515   515.8373    113.78061  -174.62596   -59.049343  -91.874374\n",
      "   -45.883553   57.69134   189.00511  -252.11792 ]\n",
      " [ -68.58533   467.48782   122.53836  -158.80824  -103.30761   -66.581116\n",
      "   -21.282555 -122.19616   144.06882  -280.55634 ]\n",
      " [ -40.6172    467.2336    157.57983  -113.78942  -184.71841   -22.99681\n",
      "   -47.821556  -82.33173   142.40257  -286.47342 ]\n",
      " [  15.809708  595.64355   182.47025   -78.4283   -289.6178     55.735245\n",
      "  -115.22426   -67.32344   127.413345 -336.9984  ]\n",
      " [  37.6264    379.57635   155.70169  -105.782    -215.28494     4.241356\n",
      "   -18.60831  -294.0022     79.90017  -328.6534  ]]\n",
      "layer_2:  [[ -787.64307  -1597.2622    -465.67242   -124.81198    310.8161  ]\n",
      " [ -359.36438  -1297.0032    -479.59814   -245.80319    254.3044  ]\n",
      " [ -561.12506  -1448.6212    -500.54572    -57.690643   619.1571  ]\n",
      " [ -958.43567  -1904.8453    -786.9562     155.05661   1166.395   ]\n",
      " [  -20.793137 -1047.4413    -499.6719    -160.0733     450.29312 ]]\n",
      "layer_3:  [[ 116.47355]\n",
      " [-153.44254]\n",
      " [ 100.33872]\n",
      " [ 430.55832]\n",
      " [-227.3386 ]]\n",
      "loss:  140239.45\n",
      "layer_1:  [[  138.85811     392.86758     223.31766     -81.5603     -346.2302\n",
      "     76.69776     -74.45199    -216.76506      58.070587   -347.49194  ]\n",
      " [  221.54202     337.32086     156.57298    -119.49658    -303.50787\n",
      "     11.7815895   -10.093742   -518.5748      -22.749065   -468.06543  ]\n",
      " [  239.65977     249.92334     222.0528      -96.31772    -366.59506\n",
      "    103.59514     -39.163963   -444.98065     -33.15808    -406.35114  ]\n",
      " [  356.65213     150.73401     195.11111    -103.51163    -373.31775\n",
      "    108.414856     -3.0100574  -738.65906    -163.4465     -531.5128   ]\n",
      " [  478.13358      78.29913     145.37067     -76.42704    -348.80875\n",
      "     72.96913      35.518696  -1113.4954     -347.4323     -719.4373   ]]\n",
      "layer_2:  [[ -746.79846  -1255.3359    -518.1446     422.16144   1274.9425  ]\n",
      " [ -116.35919   -799.50525   -472.4258      92.40561    575.97217 ]\n",
      " [ -229.52014   -729.8193    -510.74832    457.99487   1085.3184  ]\n",
      " [  320.68677   -266.26865   -577.2786     412.16754    697.51587 ]\n",
      " [ 1004.18726    214.73438   -674.4614     185.96124    -35.603912]]\n",
      "layer_3:  [[ 571.63586 ]\n",
      " [  66.572365]\n",
      " [ 354.4127  ]\n",
      " [  47.03603 ]\n",
      " [-462.58923 ]]\n",
      "test loss:  542892.06\n",
      "test loss:  968519.6\n",
      "epoch:  7\n",
      "batches:  8\n",
      "loss:  996.7818\n",
      "layer_1:  [[  2.0847006   -0.96372163   3.5826874   -0.48863536  -3.6099925\n",
      "    0.97041637   1.9437996   -3.2429714   -0.7659657   -2.2188506 ]\n",
      " [  0.9523624   14.871215     9.876849    -1.9424876  -12.405865\n",
      "   -0.20378458  -1.2128491    0.41428405   7.540083    -9.0015545 ]\n",
      " [ 11.563261    -3.5544784   10.87309     -3.9874332  -12.684508\n",
      "    3.0745032    5.10639    -20.092613    -6.5194645  -14.066113  ]\n",
      " [ -7.1057463   41.005997    12.498588   -18.566944   -11.272148\n",
      "   -9.446529    -3.6743705   10.099459    19.85816    -15.987054  ]\n",
      " [ 11.638077    20.105444    23.86093     -7.4779024  -30.682634\n",
      "    5.4588795   -0.31539893 -14.932515     6.661812   -25.111189  ]]\n",
      "layer_2:  [[   3.0566273     3.5269356     1.0174499     7.906034     12.968451  ]\n",
      " [ -38.876404    -50.58204       0.28149223   11.481138     50.411934  ]\n",
      " [  10.499117     15.168095     -4.1174226    32.240242     35.651875  ]\n",
      " [ -99.28267    -123.43317     -19.250439      8.26388      61.244923  ]\n",
      " [ -58.91416     -71.689095     -9.9178705    55.13519     119.70371   ]]\n",
      "layer_3:  [[ 5.0220366]\n",
      " [23.925436 ]\n",
      " [ 9.871219 ]\n",
      " [43.322838 ]\n",
      " [55.519688 ]]\n",
      "loss:  11082.776\n",
      "layer_1:  [[  44.355034    -6.2234154   26.080536   -15.107256   -37.95945\n",
      "     6.984332     6.904783   -56.183228   -23.233217   -47.490948 ]\n",
      " [   9.248559    68.49089     31.076447   -31.156782   -38.18638\n",
      "   -12.343021    -2.5581794   -4.5678606   26.262215   -45.33396  ]\n",
      " [  17.34211     85.69714     40.53545    -14.147052   -62.154694\n",
      "     1.644187   -12.072876     2.8831365   28.643564   -56.033707 ]\n",
      " [  50.019684    28.390203    49.021317   -22.906998   -71.805855\n",
      "    14.594168    -1.4005748  -47.93821     -2.880516   -64.672646 ]\n",
      " [ 100.15766     -6.2096405   47.79563    -31.664389   -79.075264\n",
      "    13.320829     8.271967  -127.269035   -53.613964  -108.48343  ]]\n",
      "layer_2:  [[ -56.735207    31.296097   -12.205641   134.47401    126.41201  ]\n",
      " [-200.45547   -195.63739    -13.493477    63.70574    167.17902  ]\n",
      " [-280.08917   -278.7227     -15.304749   109.305084   278.9063   ]\n",
      " [-185.64432    -98.05065    -15.7916565  199.59048    294.56305  ]\n",
      " [-142.9197      51.114136   -39.879845   280.26782    250.59753  ]]\n",
      "layer_3:  [[ 61.35343 ]\n",
      " [ 84.39313 ]\n",
      " [122.08645 ]\n",
      " [127.428375]\n",
      " [135.09258 ]]\n",
      "loss:  36654.84\n",
      "layer_1:  [[   6.561447   142.71877     49.257263   -47.185047   -65.37741\n",
      "   -28.907797   -10.304584    32.212715    59.220398   -81.549385 ]\n",
      " [  59.50173    108.39661     61.310043   -54.483574   -94.73996\n",
      "   -13.758238    -5.66696    -38.88401     25.74354   -105.72116  ]\n",
      " [  65.280624   109.678406    79.78931    -32.089535  -124.21949\n",
      "    12.521944   -16.837053   -22.044895    24.895922  -105.78224  ]\n",
      " [ 132.84995     41.251453    90.04273    -53.408188  -146.97366\n",
      "    28.031319    -7.1085176 -103.97144    -27.44508   -140.03616  ]\n",
      " [ 222.21411     -3.8311386   89.84151    -69.91373   -165.32747\n",
      "    22.94304      1.3129082 -222.40729   -109.42588   -219.1929   ]]\n",
      "layer_2:  [[-475.13876   -438.45425     18.22165    121.5984     312.40744  ]\n",
      " [-462.27072   -292.04358     10.950478   239.41475    407.75073  ]\n",
      " [-529.851     -352.35013      6.0058517  328.4996     570.40063  ]\n",
      " [-514.9394    -128.29779     -7.6033096  520.7704     644.61914  ]\n",
      " [-569.55347     74.48111    -40.579178   707.70886    633.3583   ]]\n",
      "layer_3:  [[130.3162 ]\n",
      " [152.35725]\n",
      " [186.37407]\n",
      " [222.08461]\n",
      " [262.44836]]\n",
      "loss:  27488.518\n",
      "layer_1:  [[ -10.431878   239.52008     53.776546  -102.00227    -56.764854\n",
      "   -65.93503    -22.510822    79.41326     96.30562   -115.282715 ]\n",
      " [  65.34339    199.90196     94.10136    -75.494026  -142.43295\n",
      "   -31.889477   -14.444978    10.278983    66.472755  -152.45534  ]\n",
      " [  84.25241    263.6801     110.45998    -46.95886   -197.2938\n",
      "     2.6343155  -35.421352    22.57545     65.28823   -180.61043  ]\n",
      " [ 162.6154     162.11629    108.36317    -93.98019   -193.68419\n",
      "   -13.3405285  -15.603344   -96.98643     11.974781  -210.07426  ]\n",
      " [ 164.79848    141.09253    139.20128    -68.45596   -228.737\n",
      "    29.206902   -27.140831   -66.80853      7.965645  -194.08263  ]]\n",
      "layer_2:  [[ -782.73157    -708.2993       -7.6447906   141.40974     351.4397   ]\n",
      " [ -871.7373     -579.8798       87.646774    374.3289      668.6311   ]\n",
      " [-1092.8909     -794.3391       40.347107    515.11414     954.9762   ]\n",
      " [-1001.091      -405.53253      80.68437     624.7434      864.71423  ]\n",
      " [-1043.1737     -432.60394      64.4197      768.29236    1093.4211   ]]\n",
      "layer_3:  [[126.43192]\n",
      " [141.98062]\n",
      " [194.96458]\n",
      " [175.24477]\n",
      " [203.05263]]\n",
      "loss:  618.37213\n",
      "layer_1:  [[ 309.4673      57.920853   162.5711    -116.06567   -289.62988\n",
      "    44.943264   -24.224049  -168.84659    -78.78102   -275.10596  ]\n",
      " [ 467.05893     -0.6343918  170.76773   -151.30392   -334.573\n",
      "    36.046955   -23.30045   -323.10388   -204.65771   -406.19235  ]\n",
      " [  64.40376    336.28076    105.619446  -142.41821   -150.01215\n",
      "   -84.981186   -25.72985     62.107113   117.48678   -210.44847  ]\n",
      " [  72.437485   364.91565    127.58626   -102.12473   -210.65498\n",
      "   -63.15264    -26.429026    84.18013    124.35054   -234.86536  ]\n",
      " [ 184.3393     273.6768     160.1088    -123.181335  -270.29904\n",
      "   -31.697933   -26.103647   -32.25482     62.4811    -269.99496  ]]\n",
      "layer_2:  [[-1444.8225   -129.31454   136.06725  1311.6129   1440.0806 ]\n",
      " [-1813.0511    140.75516   121.09494  1770.4019   1600.1571 ]\n",
      " [-1362.8445   -925.1251    133.86133   430.72186   789.0037 ]\n",
      " [-1512.0045  -1062.0708    207.39581   556.66064  1024.8799 ]\n",
      " [-1606.958    -737.9534    233.44817   880.0055   1297.6143 ]]\n",
      "layer_3:  [[-24.550259 ]\n",
      " [-33.12946  ]\n",
      " [ 22.645716 ]\n",
      " [ 18.018753 ]\n",
      " [ -6.8807893]]\n",
      "loss:  253364.3\n",
      "layer_1:  [[ 218.073      318.17975    195.7284     -88.622734  -346.38748\n",
      "    14.425869   -46.88964      2.8786507   55.05697   -289.93564  ]\n",
      " [ 371.9767     228.86078    187.7942    -165.70856   -371.35782\n",
      "   -12.909167   -39.321854  -163.5491     -25.977749  -382.74966  ]\n",
      " [ 366.22336    179.4054     234.31569   -139.03679   -410.1835\n",
      "    49.27394    -47.5854    -116.66742    -32.971405  -345.0114   ]\n",
      " [ 559.2586      82.41162    258.6949    -202.51468   -485.23413\n",
      "    68.00461    -50.456745  -270.45023   -157.00659   -468.7708   ]\n",
      " [ 790.59033      9.510216   273.81577   -255.51405   -557.0073\n",
      "    53.636513   -56.66649   -471.58026   -333.42255   -659.65155  ]]\n",
      "layer_2:  [[-1984.0593   -881.48975   299.50888  1193.6296   1758.834  ]\n",
      " [-2252.5088   -482.15817   365.36035  1534.5538   1827.1777 ]\n",
      " [-2235.0278   -466.10992   325.48807  1732.501    2123.8206 ]\n",
      " [-2742.8406   -117.82568   329.33432  2414.5393   2530.2117 ]\n",
      " [-3427.5688    234.40857   337.28247  3152.9683   2849.3855 ]]\n",
      "layer_3:  [[-319.6755 ]\n",
      " [-411.09824]\n",
      " [-431.34406]\n",
      " [-551.6183 ]\n",
      " [-688.17847]]\n",
      "loss:  67305.555\n",
      "layer_1:  [[  74.48565    516.1941     140.11787   -183.98534   -221.45909\n",
      "  -142.08855    -30.417397   176.06668    193.26802   -303.14117  ]\n",
      " [ 229.70134    458.95984    182.39752   -207.69273   -318.55383\n",
      "  -105.70328    -41.331062    36.844128   126.77552   -364.54126  ]\n",
      " [ 223.33472    464.5851     216.90503   -157.61884   -382.5054\n",
      "   -70.054596   -38.96394     71.83199    127.86124   -370.76645  ]\n",
      " [ 251.53474    601.2982     239.72275   -112.593346  -477.70407\n",
      "    -7.3357162  -65.84291     88.69305    119.29652   -423.36682  ]\n",
      " [ 411.66132    365.2464     259.2515    -206.15273   -483.30722\n",
      "   -29.06351    -52.77835    -79.99431     34.699593  -455.0909   ]]\n",
      "layer_2:  [[-2191.955   -1427.7445    368.2099    665.9707   1166.3429 ]\n",
      " [-2544.542   -1143.9219    398.71323  1133.4974   1651.5864 ]\n",
      " [-2622.211   -1236.0061    465.1461   1294.7605   1917.6045 ]\n",
      " [-3033.7803  -1622.2156    366.35538  1586.9785   2445.7148 ]\n",
      " [-3086.6719   -877.19684   471.55234  1943.9119   2460.2212 ]]\n",
      "layer_3:  [[-151.03119]\n",
      " [-233.15932]\n",
      " [-261.30167]\n",
      " [-228.88177]\n",
      " [-349.97095]]\n",
      "loss:  39022.402\n",
      "layer_1:  [[  431.57553     387.54248     308.16254    -168.08908    -563.87366\n",
      "     40.378723    -77.0067      -26.891186     17.048203   -450.0383   ]\n",
      " [  683.15204     315.13965     294.85294    -279.57153    -627.78375\n",
      "     -2.2167788   -87.04886    -246.25894    -102.85129    -624.79266  ]\n",
      " [  665.9615      231.20248     359.88675    -254.29243    -669.2655\n",
      "     84.05794     -90.481514   -179.21347    -113.36961    -559.53156  ]\n",
      " [  954.4076      117.28691     398.0204     -352.12518    -787.3097\n",
      "    105.88898    -107.764046   -369.57513    -292.25882    -748.5971   ]\n",
      " [ 1291.5638       27.959145    428.0245     -438.7689     -904.24286\n",
      "     89.177414   -130.59969    -610.5171     -536.41174   -1016.22687  ]]\n",
      "layer_2:  [[-3415.317    -1020.20276    275.92746   2345.1614    2993.3389  ]\n",
      " [-4201.335     -621.7555     284.81903   3025.4785    3287.922   ]\n",
      " [-4094.2375    -556.07434    208.43747   3280.586     3658.861   ]\n",
      " [-5102.396     -149.41956     97.67059   4408.797     4377.0034  ]\n",
      " [-6367.4707     270.64856    -27.027283  5648.2666    5025.3184  ]]\n",
      "layer_3:  [[204.22142]\n",
      " [161.54475]\n",
      " [204.60715]\n",
      " [237.70538]\n",
      " [263.5201 ]]\n",
      "test loss:  94136.4\n",
      "test loss:  187797.86\n",
      "epoch:  8\n",
      "batches:  8\n",
      "loss:  124.07717\n",
      "layer_1:  [[  5.7531323   -1.2823089    4.7514787   -2.0369947   -6.1398106\n",
      "    0.96481395   0.94595504  -0.7471323   -1.4877483   -3.4039392 ]\n",
      " [  6.7260537   14.75816     10.903479    -3.0860004  -16.769941\n",
      "   -1.4708335   -0.9791302    4.612713     7.156961   -10.41055   ]\n",
      " [ 28.845081    -4.829443    16.60194    -11.53768    -24.538013\n",
      "    3.2390702    0.96265864  -8.815317   -10.281345   -19.946827  ]\n",
      " [  6.552387    40.66856     12.977394   -19.293724   -20.941864\n",
      "  -12.193899    -4.383749    17.959023    19.937674   -17.264107  ]\n",
      " [ 33.610085    19.16119     29.952251   -14.654238   -46.545864\n",
      "    3.445903    -2.546517    -0.30703604   3.2473545  -32.017475  ]]\n",
      "layer_2:  [[ -30.007978     5.3777757    5.783539    31.508791    34.374603 ]\n",
      " [ -89.66399    -39.57837     24.796158    46.140762    79.407074 ]\n",
      " [-147.15372     20.80962     11.705885   146.8913     140.36256  ]\n",
      " [-187.23648   -106.76895     24.679558    55.760296   111.37883  ]\n",
      " [-258.23694    -51.310604    39.724266   197.2948     245.42276  ]]\n",
      "layer_3:  [[ 1.8804588]\n",
      " [ 6.8964777]\n",
      " [ 2.9026947]\n",
      " [19.45658  ]\n",
      " [20.27817  ]]\n",
      "loss:  2469.4053\n",
      "layer_1:  [[  87.26813     -9.136654    40.323174   -34.3895     -67.3002\n",
      "     7.5805335   -3.027502   -28.34436    -33.23129    -61.976707 ]\n",
      " [  40.249393    67.6063      35.59897    -37.22998    -60.426693\n",
      "   -16.104797    -6.572606    14.293423    23.604164   -51.129986 ]\n",
      " [  39.792385    86.4297      43.982822   -18.518353   -80.16244\n",
      "    -4.621315    -8.409328    21.028006    26.940105   -60.974625 ]\n",
      " [  99.80171     26.008072    63.901855   -41.95344   -107.091866\n",
      "    12.185552    -8.893686   -15.025801   -12.692345   -80.556175 ]\n",
      " [ 192.75397    -12.234558    78.81691    -73.55855   -142.32669\n",
      "    14.812963   -12.473153   -67.8099     -75.611885  -140.11963  ]]\n",
      "layer_2:  [[ -461.69336      50.459488     13.0795555   432.1681      393.1174   ]\n",
      " [ -451.97446    -158.03203      59.544956    226.87595     317.82147  ]\n",
      " [ -501.03827    -225.366        79.258286    264.94482     403.24878  ]\n",
      " [ -657.5075      -56.224365     51.893127    543.0012      595.188    ]\n",
      " [-1021.30804      90.94595       9.190437    929.1659      831.2339   ]]\n",
      "layer_3:  [[30.365025]\n",
      " [41.815636]\n",
      " [52.69766 ]\n",
      " [59.154114]\n",
      " [75.64057 ]]\n",
      "loss:  14640.09\n",
      "layer_1:  [[  37.811977  143.72314    49.59555   -50.29779   -89.41422   -36.895935\n",
      "    -9.109924   55.34048    58.205097  -83.135765]\n",
      " [ 119.173065  106.49703    73.17284   -71.35928  -137.4193    -18.340405\n",
      "   -15.848177   -1.162713   17.148335 -119.051575]\n",
      " [ 113.42793   109.514725   90.52393   -47.245235 -160.65271     5.160343\n",
      "   -17.550549   14.179232   17.075802 -117.86076 ]\n",
      " [ 223.86276    36.750763  117.85099   -91.11021  -210.8504     25.473946\n",
      "   -23.073774  -43.54894   -47.657776 -168.69029 ]\n",
      " [ 374.04858   -13.379932  140.2188   -139.93018  -268.88898    25.494923\n",
      "   -32.866802 -124.22354  -147.01439  -269.82608 ]]\n",
      "layer_2:  [[ -729.405     -365.76016    123.94627    278.73157    448.9416  ]\n",
      " [-1001.6121    -219.46742     99.69873    611.3606     734.1834  ]\n",
      " [-1012.43677   -267.87396    107.883125   676.9175     855.32043 ]\n",
      " [-1404.4131     -48.41678     56.786224  1173.8838    1207.3169  ]\n",
      " [-2046.0118     161.64603    -11.120377  1801.5374    1593.1434  ]]\n",
      "layer_3:  [[ 81.78269]\n",
      " [ 96.61358]\n",
      " [117.29329]\n",
      " [141.13203]\n",
      " [172.4638 ]]\n",
      "loss:  27162.23\n",
      "layer_1:  [[  32.55571    240.90034     48.570667  -103.471405   -88.398636\n",
      "   -76.422485   -24.81306    110.17944     96.01094   -110.23832  ]\n",
      " [ 122.44899    200.45093    100.01155    -88.45045   -185.3514\n",
      "   -41.276604   -18.732351    51.93238     59.641045  -159.21245  ]\n",
      " [ 125.13876    267.4723     112.85019    -56.33162   -232.14792\n",
      "   -11.847515   -25.413359    63.554718    60.36812   -183.7912   ]\n",
      " [ 262.72018    158.67076    130.85742   -127.83162   -264.81985\n",
      "   -18.625828   -35.15142    -31.895493    -6.3667603 -233.6032   ]\n",
      " [ 250.3787     139.44148    160.60747   -100.56885   -291.55396\n",
      "    20.782005   -35.225166    -4.110571    -9.792434  -216.12502  ]]\n",
      "layer_2:  [[-1076.6588   -623.158      90.5972    293.32523   504.6194 ]\n",
      " [-1395.5769   -461.8929    193.89319   725.50244   960.8004 ]\n",
      " [-1561.0295   -648.31793   171.09534   851.46423  1196.3806 ]\n",
      " [-1961.1865   -276.64386   152.72131  1302.3231   1439.9065 ]\n",
      " [-1918.1855   -296.0835    143.93204  1404.7      1611.0349 ]]\n",
      "layer_3:  [[154.32481]\n",
      " [129.38232]\n",
      " [210.13954]\n",
      " [152.33679]\n",
      " [191.04121]]\n",
      "loss:  7417.1426\n",
      "layer_1:  [[ 437.53067    51.386925  200.59737  -172.79195  -378.44165    41.483543\n",
      "   -49.248383  -81.579285 -110.76297  -311.83252 ]\n",
      " [ 666.6016    -13.113201  234.85437  -245.9165   -469.82043    37.664894\n",
      "   -69.36213  -191.36214  -257.3536   -467.9466  ]\n",
      " [ 133.01799   337.44495   104.07177  -152.72084  -200.16856   -97.69702\n",
      "   -34.074345  111.64833   111.66943  -207.63478 ]\n",
      " [ 121.60847   369.29614   123.25126  -109.1938   -250.52838   -80.04378\n",
      "   -20.659548  131.04083   120.18888  -229.01216 ]\n",
      " [ 277.8603    273.06274   174.27493  -151.3655   -338.80524   -43.19097\n",
      "   -38.253586   35.519917   46.378235 -283.41138 ]]\n",
      "layer_2:  [[-2746.751       33.74115    108.37689   2256.444     2216.7715  ]\n",
      " [-3822.2937     338.5677      25.436737  3238.5815    2831.447   ]\n",
      " [-1928.2383    -771.922      227.33722    769.5978    1084.389   ]\n",
      " [-1990.7863    -872.5736     331.6858     862.34094   1249.7134  ]\n",
      " [-2518.4707    -542.23694    309.03546   1504.4515    1799.8752  ]]\n",
      "layer_3:  [[ 50.511444]\n",
      " [ 15.013117]\n",
      " [121.62341 ]\n",
      " [129.76587 ]\n",
      " [ 84.88643 ]]\n",
      "loss:  73460.93\n",
      "layer_1:  [[ 280.63504    320.39453    202.02939   -110.08209   -394.55307\n",
      "    -2.5910301  -42.478973    61.918686    41.78293   -292.32275  ]\n",
      " [ 505.0007     223.53323    217.3987    -216.31061   -463.88077\n",
      "   -20.641247   -67.40411    -73.28636    -55.181156  -409.1922   ]\n",
      " [ 483.3449     175.61636    262.93066   -187.884     -493.23694\n",
      "    37.97229    -63.15534    -29.571907   -61.73178   -370.13065  ]\n",
      " [ 745.48596     72.23228    313.4218    -287.23413   -612.8991\n",
      "    61.969376   -87.978806  -142.82355   -205.76279   -519.3208   ]\n",
      " [1064.4851      -7.9685287  360.13416   -386.6966    -741.5114\n",
      "    53.107628  -119.83353   -289.20685   -407.81702   -740.2547   ]]\n",
      "layer_2:  [[-2694.3276   -645.9678    342.857    1684.17     2097.4458 ]\n",
      " [-3589.6753   -233.07074   329.50735  2458.7942   2562.3071 ]\n",
      " [-3466.4812   -214.46997   297.69244  2600.848    2790.782  ]\n",
      " [-4660.1934    174.79877   213.89758  3782.6272   3622.6887 ]\n",
      " [-6221.249     591.05615   122.96991  5160.8667   4485.0176 ]]\n",
      "layer_3:  [[ -36.64662]\n",
      " [-204.61452]\n",
      " [-164.37846]\n",
      " [-284.19537]\n",
      " [-448.49527]]\n",
      "loss:  95614.09\n",
      "layer_1:  [[ 124.743256  520.22626   125.16632  -187.25201  -257.77     -161.93167\n",
      "   -27.769691  225.30972   190.32544  -279.09515 ]\n",
      " [ 319.3857    459.13577   183.61295  -228.9455   -381.62842  -120.45363\n",
      "   -55.570522  104.42265   113.596405 -358.714   ]\n",
      " [ 292.96234   467.59344   215.54814  -175.83583  -434.5512    -89.04711\n",
      "   -38.829548  136.11644   116.09861  -362.0605  ]\n",
      " [ 297.58444   608.6065    231.4934   -125.41518  -516.97437   -34.148544\n",
      "   -46.027412  152.11266   110.25162  -406.30035 ]\n",
      " [ 533.92676   363.12015   279.01303  -249.99312  -570.1583    -41.956432\n",
      "   -72.97002    11.218489    8.44063  -468.97354 ]]\n",
      "layer_2:  [[-2593.8364  -1204.595     426.389     866.6201   1321.735  ]\n",
      " [-3356.909    -912.3276    382.51486  1638.7522   2057.8752 ]\n",
      " [-3328.1777   -976.1612    478.01886  1750.9014   2244.6633 ]\n",
      " [-3644.247   -1319.7966    391.7433   2001.9503   2684.838  ]\n",
      " [-4322.6035   -609.20544   382.10748  2791.638    3118.259  ]]\n",
      "layer_3:  [[231.89108]\n",
      " [252.57777]\n",
      " [260.88068]\n",
      " [467.1377 ]\n",
      " [306.2078 ]]\n",
      "loss:  46894.31\n",
      "layer_1:  [[ 5.32929077e+02  3.83018494e+02  3.25668976e+02 -1.99837494e+02\n",
      "  -6.30357300e+02  1.18668594e+01 -6.93652344e+01  5.23422661e+01\n",
      "   4.05822754e-01 -4.55366791e+02]\n",
      " [ 8.58942932e+02  3.04898010e+02  3.36153503e+02 -3.45764771e+02\n",
      "  -7.44236572e+02 -1.67727146e+01 -1.18168015e+02 -1.29476486e+02\n",
      "  -1.38992966e+02 -6.56478943e+02]\n",
      " [ 8.25734253e+02  2.21834686e+02  4.00784668e+02 -3.18380798e+02\n",
      "  -7.75923218e+02  6.48810425e+01 -1.08330048e+02 -6.62113571e+01\n",
      "  -1.48587585e+02 -5.90874695e+02]\n",
      " [ 1.18727551e+03  1.02529343e+02  4.66219208e+02 -4.56852844e+02\n",
      "  -9.43325439e+02  9.57400818e+01 -1.52845718e+02 -2.10820602e+02\n",
      "  -3.50353882e+02 -8.08203247e+02]\n",
      " [ 1.61522644e+03  6.98118591e+00  5.28737671e+02 -5.94281006e+02\n",
      "  -1.12093372e+03  8.86593475e+01 -2.06410934e+02 -3.92560699e+02\n",
      "  -6.22733398e+02 -1.10810022e+03]]\n",
      "layer_2:  [[-4430.4956    -638.7501     444.06915   2980.5776    3461.3677  ]\n",
      " [-5938.287     -241.17834    311.33545   4179.778     4212.956   ]\n",
      " [-5714.3945    -179.03906    244.5473    4368.7153    4509.9336  ]\n",
      " [-7484.5845     248.84265    -10.469482  6063.1035    5704.423   ]\n",
      " [-9691.12       710.8733    -294.47888   8000.2925    6940.982   ]]\n",
      "layer_3:  [[168.3745 ]\n",
      " [128.17207]\n",
      " [210.16055]\n",
      " [285.0131 ]\n",
      " [335.42667]]\n",
      "test loss:  962030.4\n",
      "test loss:  2426274.5\n",
      "epoch:  9\n",
      "batches:  8\n",
      "loss:  2590.2449\n",
      "layer_1:  [[ 7.2655172e+00 -1.4704494e+00  5.1568718e+00 -2.6311111e+00\n",
      "  -7.0983992e+00  7.8264552e-01  5.7560599e-01  3.2502306e-01\n",
      "  -1.7478201e+00 -3.7152214e+00]\n",
      " [ 9.1013460e+00  1.4364325e+01  1.1008535e+01 -3.1730204e+00\n",
      "  -1.7836752e+01 -3.0077600e+00  2.3793504e-03  6.2988358e+00\n",
      "   7.3263335e+00 -9.7899551e+00]\n",
      " [ 3.5952408e+01 -5.5432673e+00  1.8660532e+01 -1.4540066e+01\n",
      "  -2.9112362e+01  2.6411686e+00 -6.4284945e-01 -3.9522285e+00\n",
      "  -1.1740139e+01 -2.1644382e+01]\n",
      " [ 1.2496269e+01  3.9778057e+01  1.2539585e+01 -1.8802002e+01\n",
      "  -2.3249947e+01 -1.5418226e+01 -2.5763443e+00  2.1134026e+01\n",
      "   2.0632359e+01 -1.4834676e+01]\n",
      " [ 4.2571400e+01  1.8097073e+01  3.1721142e+01 -1.6912352e+01\n",
      "  -5.1713272e+01  5.0450015e-01 -2.0084529e+00  5.7838712e+00\n",
      "   2.4219604e+00 -3.2478416e+01]]\n",
      "layer_2:  [[ -44.541527    9.577892    8.780165   40.11178    41.20047 ]\n",
      " [-106.50084   -21.833605   44.066765   50.68987    81.96443 ]\n",
      " [-217.55682    38.386772   21.718163  190.64484   175.39552 ]\n",
      " [-214.21321   -71.96281    63.151604   54.75343   114.310394]\n",
      " [-338.0454    -10.411175   77.36908   237.9057    275.90628 ]]\n",
      "layer_3:  [[-11.59449 ]\n",
      " [-27.05397 ]\n",
      " [-56.266815]\n",
      " [-40.668026]\n",
      " [-78.32918 ]]\n",
      "loss:  143338.06\n",
      "layer_1:  [[ 1.05573547e+02 -1.10904713e+01  4.57793694e+01 -4.16986542e+01\n",
      "  -7.87973862e+01  5.27229500e+00 -6.37847900e+00 -1.63378487e+01\n",
      "  -3.66186142e+01 -6.63216400e+01]\n",
      " [ 5.54933128e+01  6.52063446e+01  3.71365814e+01 -3.77822189e+01\n",
      "  -6.71546402e+01 -2.31371269e+01 -3.19166708e+00  2.19922924e+01\n",
      "   2.44289951e+01 -4.86759987e+01]\n",
      " [ 5.17975960e+01  8.39900284e+01  4.50505714e+01 -1.75381718e+01\n",
      "  -8.42261810e+01 -1.46657677e+01  5.11579514e-02  2.80913467e+01\n",
      "   2.89934273e+01 -5.71024323e+01]\n",
      " [ 1.21967949e+02  2.30681038e+01  6.94511642e+01 -4.78907661e+01\n",
      "  -1.19681015e+02  5.37416935e+00 -8.28837204e+00 -1.22086310e+00\n",
      "  -1.47718697e+01 -8.31903381e+01]\n",
      " [ 2.32195572e+02 -1.62178173e+01  9.07524872e+01 -8.95904617e+01\n",
      "  -1.67203995e+02  1.02012634e+01 -1.95706978e+01 -4.21412201e+01\n",
      "  -8.32237396e+01 -1.49782135e+02]]\n",
      "layer_2:  [[ -644.9633    113.61041    71.59127   535.8196    473.29395]\n",
      " [ -549.2274    -66.43364   171.37924   248.11205   338.65176]\n",
      " [ -575.07837  -113.62506   219.48718   266.31174   400.39914]\n",
      " [ -859.7378     53.24768   171.84254   638.22064   665.8689 ]\n",
      " [-1419.948     226.4095    132.91116  1157.2019   1007.14154]]\n",
      "layer_3:  [[-282.09903]\n",
      " [-231.83458]\n",
      " [-255.71564]\n",
      " [-378.79785]\n",
      " [-598.2862 ]]\n",
      "loss:  871635.4\n",
      "layer_1:  [[  59.332333  138.7237     50.832546  -46.512096  -94.535446  -53.86354\n",
      "     5.491476   64.09233    63.09778   -74.24161 ]\n",
      " [ 150.08476   101.31135    78.416824  -74.138626 -151.38333   -31.899303\n",
      "    -9.990909   14.271226   18.053213 -116.743484]\n",
      " [ 140.26228   103.9425     95.47854   -48.088276 -171.1646    -12.465624\n",
      "    -5.400452   28.517292   19.600052 -114.24716 ]\n",
      " [ 265.85733    30.663929  128.99422  -102.38089  -234.12608    12.16477\n",
      "   -22.196707  -18.221952  -51.27769  -173.97716 ]\n",
      " [ 439.75876   -20.697527  159.90717  -165.35811  -309.2991     15.306543\n",
      "   -43.1502    -82.00524  -158.387    -284.60956 ]]\n",
      "layer_2:  [[ -810.92175   -178.02512    376.36365    236.0305     425.0378  ]\n",
      " [-1221.8446     -18.464111   356.84076    664.6107     777.4187  ]\n",
      " [-1198.4084     -49.166534   391.61017    701.63477    868.1002  ]\n",
      " [-1788.2432     186.07422    335.07977   1334.5444    1321.4207  ]\n",
      " [-2706.8743     447.8494     296.44098   2140.7104    1843.5885  ]]\n",
      "layer_3:  [[ -451.0526]\n",
      " [ -679.3815]\n",
      " [ -692.8086]\n",
      " [-1005.251 ]\n",
      " [-1468.8024]]\n",
      "loss:  647045.6\n",
      "layer_1:  [[  65.697945    231.57321      47.892586    -95.05763     -90.70887\n",
      "  -104.03189       0.61813164  121.06249     105.32836     -88.05639   ]\n",
      " [ 156.47711     191.6431      102.24902     -85.07465    -192.79825\n",
      "   -66.82362       1.1451931    67.4664       65.88466    -143.37215   ]\n",
      " [ 155.27658     257.15125     114.055954    -48.65692    -233.46213\n",
      "   -46.34738       7.0800605    77.84273      70.427086   -162.41013   ]\n",
      " [ 309.68182     149.86836     138.02467    -134.24544    -284.58856\n",
      "   -38.925247    -27.575706     -5.886612     -6.187775   -225.7936    ]\n",
      " [ 291.6064      129.94264     167.2904     -104.61181    -306.8533\n",
      "    -4.4552956   -20.466082     20.228098     -7.634463   -207.03876   ]]\n",
      "layer_2:  [[-1121.2316    -341.9233     451.87735    153.39883    432.19882 ]\n",
      " [-1526.9932    -161.48431    560.51044    653.39233    914.781   ]\n",
      " [-1642.3081    -284.6663     626.26373    718.2441    1091.9645  ]\n",
      " [-2269.7666      37.279877   496.2691    1352.0969    1477.1959  ]\n",
      " [-2178.8008      28.958313   504.99774   1417.5044    1610.4104  ]]\n",
      "layer_3:  [[-469.9597 ]\n",
      " [-712.383  ]\n",
      " [-754.2555 ]\n",
      " [-986.43335]\n",
      " [-961.31134]]\n",
      "loss:  129424.42\n",
      "layer_1:  [[ 476.2256      43.55359    203.95876   -189.07341   -396.45706\n",
      "    31.27943    -53.226334   -47.69613   -118.30974   -298.52585  ]\n",
      " [ 726.62067    -20.848042   245.12808   -279.3166    -505.40027\n",
      "    36.1558     -89.27509   -138.26926   -275.491     -462.37933  ]\n",
      " [ 163.51181    324.7396      94.226326  -144.27812   -191.09146\n",
      "  -129.71286     -6.4771233  128.2004     121.228806  -158.70515  ]\n",
      " [ 144.40178    355.84784    111.712875   -97.25      -234.66708\n",
      "  -118.39833     17.081457   145.51476    132.65619   -174.90907  ]\n",
      " [ 311.67377    260.99377    168.45222   -150.62321   -338.75864\n",
      "   -71.45571    -18.654915    59.594128    51.00802   -243.95663  ]]\n",
      "layer_2:  [[-2964.8516    214.46576    71.91516  2301.7837   2240.976  ]\n",
      " [-4247.8506    495.84644  -174.58383  3441.8984   2977.3828 ]\n",
      " [-1867.259    -417.9026    539.7677    509.50775   918.7908 ]\n",
      " [-1865.8193   -470.6226    705.7095    546.4138   1026.1085 ]\n",
      " [-2549.0342   -201.70338   541.17     1327.992    1672.2135 ]]\n",
      "layer_3:  [[-266.98663]\n",
      " [-320.02768]\n",
      " [-357.7454 ]\n",
      " [-399.7095 ]\n",
      " [-405.88965]]\n",
      "loss:  2007168.6\n",
      "layer_1:  [[ 2.7832269e+02  3.0889069e+02  1.7862160e+02 -1.0554343e+02\n",
      "  -3.6701877e+02 -2.6546925e+01 -1.8572960e+01  7.9303314e+01\n",
      "   4.6336838e+01 -2.2193210e+02]\n",
      " [ 5.2450629e+02  2.1510579e+02  2.0259773e+02 -2.2909612e+02\n",
      "  -4.6066324e+02 -2.6036366e+01 -7.1680283e+01 -4.0961082e+01\n",
      "  -6.3124992e+01 -3.5725854e+02]\n",
      " [ 4.9547205e+02  1.6625996e+02  2.4707306e+02 -1.9795032e+02\n",
      "  -4.8575836e+02  2.7789051e+01 -5.9818474e+01  3.5282326e-01\n",
      "  -6.7645248e+01 -3.1851340e+02]\n",
      " [ 7.7507739e+02  6.4470535e+01  3.0420270e+02 -3.1494022e+02\n",
      "  -6.2442664e+02  6.5757645e+01 -1.0465170e+02 -9.5508629e+01\n",
      "  -2.2383466e+02 -4.7946936e+02]\n",
      " [ 1.1157384e+03 -1.4379417e+01  3.5736960e+02 -4.3641354e+02\n",
      "  -7.7287561e+02  7.2533836e+01 -1.5788335e+02 -2.1875766e+02\n",
      "  -4.4068732e+02 -7.0737146e+02]]\n",
      "layer_2:  [[-2365.6235   -421.244     236.95471  1324.1104   1820.7393 ]\n",
      " [-3440.9895   -155.59796  -109.72443  2290.1626   2446.2542 ]\n",
      " [-3271.3745   -127.90704  -113.90027  2400.206    2636.669  ]\n",
      " [-4591.7847    129.78857  -562.90674  3733.5632   3591.0183 ]\n",
      " [-6293.4585    410.40332 -1089.4009   5277.551    4592.853  ]]\n",
      "layer_3:  [[ 618.3615 ]\n",
      " [ 994.07294]\n",
      " [1053.9705 ]\n",
      " [1616.5497 ]\n",
      " [2241.9507 ]]\n",
      "loss:  100158.43\n",
      "layer_1:  [[  94.54338    501.17157     70.64937   -164.8089    -176.01672\n",
      "  -202.9665      12.612865   233.67023    208.08308   -128.85591  ]\n",
      " [ 301.07147    440.2768     134.56686   -212.14629   -315.90387\n",
      "  -157.72221    -28.775417   119.853226   128.31163   -224.55067  ]\n",
      " [ 266.83328    446.51294    163.8285    -153.64912   -360.48712\n",
      "  -136.122        1.4192562  148.36147    135.6333    -221.44878  ]\n",
      " [ 257.0189     582.77374    169.39883    -93.78807   -420.90323\n",
      "   -97.77215     16.534214   160.63367    137.8409    -238.90152  ]\n",
      " [ 523.16144    341.6518     233.79286   -234.91806   -514.345\n",
      "   -85.81396    -48.113956    33.526333    24.532385  -345.2744   ]]\n",
      "layer_2:  [[-1850.5089    -733.19293    583.6398      11.675842   692.23364 ]\n",
      " [-2662.3528    -449.61044    465.6163     762.39825   1385.1975  ]\n",
      " [-2555.3042    -461.45898    630.9022     799.80896   1503.9463  ]\n",
      " [-2676.7224    -689.56866    646.42346    848.03937   1775.3322  ]\n",
      " [-3604.7769    -101.81906    437.495     1803.582     2321.8306  ]]\n",
      "layer_3:  [[-227.80646]\n",
      " [-302.0687 ]\n",
      " [-324.21005]\n",
      " [-258.33795]\n",
      " [-407.11697]]\n",
      "loss:  4523700.5\n",
      "layer_1:  [[ 4.72352509e+02  3.55710693e+02  2.49371307e+02 -1.62704620e+02\n",
      "  -5.23875122e+02 -5.33952713e+01 -2.68909302e+01  6.07942276e+01\n",
      "   3.49470215e+01 -2.78550537e+02]\n",
      " [ 8.26717346e+02  2.75813141e+02  2.69240234e+02 -3.14050720e+02\n",
      "  -6.60297485e+02 -8.25422363e+01 -9.68554611e+01 -1.07356186e+02\n",
      "  -1.04919685e+02 -4.93185333e+02]\n",
      " [ 7.88662842e+02  1.89483688e+02  3.32948181e+02 -2.80125366e+02\n",
      "  -6.87727356e+02 -1.29956665e+01 -7.30190659e+01 -4.81088791e+01\n",
      "  -1.08365845e+02 -4.30632538e+02]\n",
      " [ 1.17263391e+03  6.26661148e+01  4.02332703e+02 -4.15053131e+02\n",
      "  -8.65450745e+02 -1.53211689e+00 -1.21528038e+02 -1.80231003e+02\n",
      "  -3.01093719e+02 -6.49044250e+02]\n",
      " [ 1.62386633e+03 -4.28906174e+01  4.65327515e+02 -5.46889160e+02\n",
      "  -1.04751172e+03 -3.48311501e+01 -1.76879150e+02 -3.46059875e+02\n",
      "  -5.61113525e+02 -9.37225525e+02]]\n",
      "layer_2:  [[-3.1604277e+03 -1.9008789e+00  6.0778882e+02  1.3879662e+03\n",
      "   2.1503220e+03]\n",
      " [-4.6797329e+03  5.0043347e+02  5.1211267e+02  2.3551553e+03\n",
      "   2.6920552e+03]\n",
      " [-4.4170684e+03  5.8520941e+02  5.2394690e+02  2.5049177e+03\n",
      "   2.9492756e+03]\n",
      " [-6.0703320e+03  1.2086152e+03  4.1109247e+02  3.7849360e+03\n",
      "   3.7803945e+03]\n",
      " [-8.0933066e+03  1.9299280e+03  3.0005951e+02  5.1833652e+03\n",
      "   4.5544204e+03]]\n",
      "layer_3:  [[-1047.5707]\n",
      " [-1633.0933]\n",
      " [-1612.1077]\n",
      " [-2286.566 ]\n",
      " [-3094.3203]]\n",
      "test loss:  67032.2\n",
      "test loss:  293309.66\n",
      "Done.\n",
      "name:  mat_mul_3/W1:0\n",
      "variables:  [[-0.21636051  0.5812129   0.6369818  -0.789755   -0.9252739  -0.8309218\n",
      "   0.18514    -1.924299   -0.08573781 -0.12483706]\n",
      " [-0.28714573  0.5600898   2.0680373   0.96889365 -2.0193365   2.846943\n",
      "   1.1805813  -0.7679076  -0.6864922   0.37550095]\n",
      " [-2.687642    0.11513594  1.5035889   0.9227811  -0.5300615   0.9283141\n",
      "  -0.79353523 -0.88243794  1.7442007   0.2577051 ]\n",
      " [-1.805246   -0.00947326 -0.44034514  0.7812431  -1.0725431   0.2686181\n",
      "  -0.2435588  -0.34462914 -1.9025707  -0.21290855]\n",
      " [ 0.64133173  1.1232405  -1.1907492   0.28939933 -0.01511524 -0.6896469\n",
      "  -0.0710603  -0.3216633  -0.34590647 -0.37944713]\n",
      " [ 2.049623   -1.0477298   0.1160311   0.5042311  -0.7444317   1.5322515\n",
      "   0.05527114  2.1483865  -0.9411583   0.28919736]\n",
      " [ 1.6343974   1.0638515   0.6831998  -0.60001653  0.4893547  -1.6470666\n",
      "   1.3178552  -0.4602672  -0.70334363 -0.36069483]\n",
      " [-0.36795142  0.4220319   0.40752837  1.3182241  -0.87429506  0.6082293\n",
      "  -1.6443725  -0.36129507  1.0740466  -1.5464864 ]\n",
      " [-0.47324234 -0.67870516  2.1850483   1.5197606   0.11940492  0.08126224\n",
      "   1.5826895   0.10743052 -0.16212323  0.19613452]\n",
      " [ 0.20657934  0.1803389   0.28227782 -1.5601982  -0.5006256  -1.8677009\n",
      "  -0.5195778  -0.15834995  0.82844204  1.0771893 ]\n",
      " [ 0.9826048   0.78899115  0.6593971  -0.24195273 -1.0266191  -1.4039744\n",
      "  -0.6764923  -0.03615617  0.19300571  0.08363426]\n",
      " [-0.3650449   1.1003778  -0.52398133 -0.34895223 -0.5441155   0.3908801\n",
      "  -0.25612617  1.5258756   1.8301504  -1.3148503 ]\n",
      " [ 0.9179576  -2.1745987   0.19487919 -0.13750681  1.2982079   0.41771108\n",
      "  -0.74857575  0.66731215  0.3027285   0.7513917 ]\n",
      " [ 0.18359825 -0.154466    0.41907623 -1.0355995   0.8221428   1.0938464\n",
      "   0.08909245  1.1670061   1.5690111  -0.7076236 ]\n",
      " [ 1.2822065   0.00603225 -0.83766294 -0.7559708   0.15185325 -0.19944367\n",
      "   1.5663387  -0.42013264  0.67978853  0.21788329]\n",
      " [ 1.2477249   0.17171293 -1.258647   -0.21779694 -0.45627236 -1.9410135\n",
      "  -0.21347758 -2.751473   -0.9167987   2.1600003 ]\n",
      " [-1.0475227  -0.37147623 -0.10460892  1.0016023   1.2545907   0.16007854\n",
      "   2.3103957  -2.0790584  -0.618915   -0.05342279]\n",
      " [ 0.05715553 -0.68435997  0.7199258  -0.33223704 -0.884678    0.06263648\n",
      "  -0.7468187  -1.0218128   0.7717602  -0.4759843 ]\n",
      " [-0.8924622  -0.07442635  1.0103874   0.22487254 -0.24852157  0.63003975\n",
      "   1.4788207   1.4239115  -1.1345072  -0.20163874]]\n",
      "name:  mat_mul_3/W2:0\n",
      "variables:  [[ 0.16329293  0.19343962  0.5007826   0.1414047   1.2769473 ]\n",
      " [-1.5907366   1.2812122   2.279821    0.5383031  -0.1876914 ]\n",
      " [ 1.5356714  -0.34278944  1.4589008  -1.8484579   0.36728173]\n",
      " [ 0.69651055 -0.22972067  0.33259574  0.04472168 -1.7413759 ]\n",
      " [ 0.37727654  0.9591864   1.4239014  -1.0143431   0.99495465]\n",
      " [-1.2998369   0.26113486 -1.1905745   0.92246    -0.16570844]\n",
      " [ 0.8057889   1.5789275   0.02479477  1.0597537   0.32499272]\n",
      " [-0.8931677  -1.8814372   0.43814486 -0.98617035 -0.37956062]\n",
      " [ 0.01543283 -1.082646   -0.48768273 -0.77891487  0.0551509 ]\n",
      " [ 0.62548566 -0.31998208  0.33461073  1.9870856  -0.127759  ]]\n",
      "name:  mat_mul_3/W3:0\n",
      "variables:  [[ 0.8290259 ]\n",
      " [ 1.048073  ]\n",
      " [ 0.19703797]\n",
      " [-1.8933823 ]\n",
      " [-0.8172439 ]]\n",
      "name:  mat_mul_3/b3:0\n",
      "variables:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# Game plan:\n",
    "# Method one: extend input paramters\n",
    "# Method two: multiply variables together to obtain non-linearity\n",
    "\n",
    "def load_input_extend(train_path):\n",
    "    train = open(train_path, 'r')\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    counter = 1\n",
    "    for line in train:\n",
    "        if counter > 50:\n",
    "            break\n",
    "        data = eval(line)\n",
    "        counter += 1\n",
    "        if data[1] > 0:\n",
    "            p1 = data[0][0]\n",
    "            p2 = data[0][1]\n",
    "            p3 = data[0][2]\n",
    "            p = p1**2\n",
    "            data[0].append(p)\n",
    "            p = p2**2\n",
    "            data[0].append(p)\n",
    "            p = p3**2\n",
    "            data[0].append(p)\n",
    "            p = p1*p2\n",
    "            data[0].append(p)\n",
    "            p = p1*p3\n",
    "            data[0].append(p)\n",
    "            p = p2*p3\n",
    "            data[0].append(p)\n",
    "            p = p1**3\n",
    "            data[0].append(p)\n",
    "            p = p2**3\n",
    "            data[0].append(p)\n",
    "            p = p3**3\n",
    "            data[0].append(p)\n",
    "            p = (p1**2)*p2\n",
    "            data[0].append(p)\n",
    "            p = (p1**2)*p3\n",
    "            data[0].append(p)\n",
    "            p = p1*(p2**2)\n",
    "            data[0].append(p)\n",
    "            p = p1*(p3**2)\n",
    "            data[0].append(p)\n",
    "            p = (p2**2)*p3\n",
    "            data[0].append(p)\n",
    "            p = (p3**2)*p2\n",
    "            data[0].append(p)\n",
    "            p = p1*p2*p3 \n",
    "            data[0].append(p)\n",
    "            train_x.append(data[0])\n",
    "            inv_vol = 1/data[1]\n",
    "            train_y.append(inv_vol)\n",
    "    print ('Number of data: ', len(train_x))\n",
    "    \n",
    "    return train_x, train_y\n",
    "\n",
    "def mat_mul_3(param, batch_size, num_param):\n",
    "    # TODO: make this more general\n",
    "    with tf.variable_scope(\"mat_mul_3\", reuse=True):\n",
    "        input_parameter = tf.placeholder(name=\"input_parameter\", dtype=tf.float32, shape=(batch_size, num_param))\n",
    "        input_volume = tf.placeholder(name=\"input_volume\", dtype=tf.float32, shape=(batch_size))\n",
    "        \n",
    "        # Layer 1:\n",
    "        # Input: (batch_size, num_param)\n",
    "        # W1: (num_param, 10), b: (batch_size, 10)\n",
    "        # Output: (batch_size, 10)\n",
    "        W1 = tf.Variable(tf.random_normal([num_param, 10], dtype=tf.float32), name=\"W1\")\n",
    "        y = tf.matmul(input_parameter, W1)\n",
    "        layer_1 = y\n",
    "        \n",
    "        # Layer 2:\n",
    "        # Input: (batch_size, 10)\n",
    "        # W1: (10, 5), b: (batch_size, 5)\n",
    "        # Output: (batch_size, 5)\n",
    "        W2 = tf.Variable(tf.random_normal([10, 5], dtype=tf.float32), name=\"W2\")\n",
    "        y = tf.matmul(y, W2)\n",
    "        layer_2 = y\n",
    "        \n",
    "        # Layer 3:\n",
    "        # Input: (batch_size, 5)\n",
    "        # W1: (5, 1), b: (batch_size, 1)\n",
    "        # Output: (batch_size, 1)\n",
    "        W3 = tf.Variable(tf.random_normal([5, 1], dtype=tf.float32), name=\"W3\")\n",
    "        b3 = tf.Variable(tf.zeros([batch_size, 1], dtype=tf.float32), name=\"b3\")\n",
    "        y = tf.add(tf.matmul(y, W3), b3)\n",
    "        layer_3 = y\n",
    "        \n",
    "        #cost_op = tf.reduce_mean(tf.pow(tf.pow(y,-1)-tf.multiply(tf.pow(input_volume,-1), y), 2))\n",
    "        cost_op = tf.reduce_mean(tf.pow(y-input_volume, 2))\n",
    "        #cost_op = tf.reduce_mean(tf.multiply(tf.pow(y-input_volume, 2), tf.pow(y, -3/4)))\n",
    "\n",
    "    return y, cost_op, input_parameter, input_volume, layer_1, layer_2, layer_3\n",
    "\n",
    "def train(data, vol):\n",
    "    num_data = len(data)\n",
    "    split_idx = int(num_data*0.8)\n",
    "    \n",
    "    input_data = data[0:split_idx]\n",
    "    input_vol = vol[0:split_idx]\n",
    "    \n",
    "    test_data = data[split_idx:]\n",
    "    test_vol = vol[split_idx:]\n",
    "    \n",
    "    num_train = split_idx\n",
    "    num_test = num_data - split_idx\n",
    "    num_param = len(input_data[0])\n",
    "    num_test = len(test_data)\n",
    "    print('number of train: ', num_train)\n",
    "    print('number of test: ', num_test)\n",
    "    print('number of parameters: ', num_param)\n",
    "    print('number of test: ', num_test)\n",
    "    \n",
    "    num_batches = int(num_train/batch_size)\n",
    "    print('batch_size: ', batch_size)\n",
    "    print('number of batches: ', num_batches)\n",
    "    test_batches = int(num_test/batch_size)\n",
    "    input_param = np.array(input_data)\n",
    "    input_vol = np.array(input_vol)\n",
    "    test_param = np.array(test_data)\n",
    "    test_vol = np.array(test_vol)\n",
    "    \n",
    "    #y = fl_linear(input_param)\n",
    "    y, cost_op, input_parameter, input_volume, layer_1, layer_2, layer_3 = mat_mul_3(input_param, batch_size, num_param)\n",
    "    \n",
    "    learning_rate = tf.Variable(0.5, trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(cost_op)\n",
    "    \n",
    "    sess = tf.Session() # Create TensorFlow session\n",
    "    saver = tf.train.Saver() # Save Tensorflow graph\n",
    "    print (\"Beginning Training\")\n",
    "    with sess.as_default():\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        sess.run(tf.assign(learning_rate, alpha))\n",
    "        for epoch in range(1, max_epochs):\n",
    "            print('epoch: ', epoch)\n",
    "            train_one_epoch(sess, cost_op, train_op, num_batches, input_parameter, input_param, input_volume, input_vol, y, layer_1, layer_2, layer_3)\n",
    "            eval_one_epoch(sess, cost_op, test_batches, input_parameter, test_param, input_volume, test_vol)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                path = \"log/model_3.ckpt\"\n",
    "                save_path = saver.save(sess, path)\n",
    "                print(\"Saved.\")\n",
    "            \n",
    "        print('Done.')\n",
    "        \n",
    "        collection = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='mat_mul_3')\n",
    "        for x in collection:\n",
    "            if x.name == 'mat_mul_3/W1:0' or x.name == 'mat_mul_3/W2:0'or x.name == 'mat_mul_3/W3:0' or x.name == 'mat_mul_3/b3:0':\n",
    "                print(\"name: \", x.name)\n",
    "                print(\"variables: \", sess.run(x))\n",
    "                \n",
    "def train_one_epoch(sess, cost_op, train_op, num_batches, input_parameter, input_param, input_volume, input_vol, y, layer_1, layer_2, layer_3):\n",
    "    print('batches: ', num_batches)\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx+1) * batch_size\n",
    "\n",
    "        batch_cost,_, batch_layer_1, batch_layer_2, batch_layer_3 = sess.run([cost_op, train_op, layer_1, layer_2, layer_3], feed_dict={input_parameter: input_param[start_idx:end_idx], input_volume: input_vol[start_idx:end_idx]})\n",
    "        print(\"loss: \", batch_cost)\n",
    "        print(\"layer_1: \", batch_layer_1)\n",
    "        print(\"layer_2: \", batch_layer_2)\n",
    "        print(\"layer_3: \", batch_layer_3)\n",
    "\n",
    "def eval_one_epoch(sess, cost_op, test_batches, input_parameter, test_param, input_volume, test_vol):\n",
    "    for batch_idx in range(test_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx+1) * batch_size\n",
    "        cost = sess.run(cost_op, feed_dict={input_parameter: test_param[start_idx:end_idx], input_volume: test_vol[start_idx:end_idx]})\n",
    "        print(\"test loss: \", cost)\n",
    "\n",
    "# Parameters:\n",
    "last_cost = 0\n",
    "alpha = 0.4\n",
    "batch_size = 5\n",
    "max_epochs = 10        \n",
    "\n",
    "train_path = '/home/carnd/CYML/output/train/cylinder/lift_1_to_50.txt'\n",
    "train_data, train_vol = load_input_extend(train_path)\n",
    "print ('sample data: ', train_data[0])\n",
    "train(train_data, train_vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invert:  0.5925925926130731\n",
      "invert:  1.6874999999416787\n"
     ]
    }
   ],
   "source": [
    "print ('invert: ', train_y[0]**(-1))\n",
    "print ('invert: ', (train_y[0]**(-1))**(-1))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
